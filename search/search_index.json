{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to StringSight","text":"<p>Extract, cluster, and analyze behavioral properties from Large Language Models</p> <p>StringSight helps you understand how different generative models behave by automatically extracting behavioral properties from their responses, grouping similar behaviors together, and quantifying how important these behaviors are.</p>"},{"location":"#what-is-stringsight","title":"What is StringSight?","text":"<p>StringSight is a comprehensive analysis framework for evaluating and comparing Large Language Model (LLM) responses. Instead of just measuring accuracy or overall quality scores, StringSight:</p> <ol> <li> <p>Extracts behavioral properties - Uses LLMs to identify specific behavioral traits in model responses (e.g., \"provides step-by-step reasoning\", \"uses technical jargon\", \"includes creative examples\")</p> </li> <li> <p>Clusters similar behaviors - Groups related properties together to identify common patterns (e.g., \"Reasoning Transparency\", \"Communication Style\")</p> </li> <li> <p>Quantifies importance - Calculates statistical metrics to show which models excel at which behaviors and by how much</p> </li> <li> <p>Provides insights - Surfaces the why behind model performance differences, not just the what</p> </li> </ol>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>\ud83d\udd0d Automatic property extraction - LLM-powered analysis identifies behavioral patterns without manual coding</li> <li>\ud83d\udcca Clustering - Groups similar behaviors into meaningful clusters</li> <li>\ud83d\udcc8 Statistical analysis - Computes significance testing, confidence intervals, and quality scores</li> <li>Multiple analysis modes - Single-model analysis or side-by-side comparisons (Arena-style)</li> <li>\ud83c\udff7\ufe0f Fixed-taxonomy labeling - LLM-as-judge with predefined behavioral axes</li> <li>\ud83d\udcb0 Cost tracking - Built-in monitoring of LLM API costs</li> <li>\ud83d\udcf1 Interactive visualizations - React frontend for exploring results</li> <li>\ud83d\udd27 Flexible pipeline - Modular architecture supports custom extractors, clusterers, and metrics</li> </ul>"},{"location":"#quick-example","title":"Quick Example","text":"<pre><code>import pandas as pd\nfrom stringsight import explain\n\n# Your data with model responses\ndf = pd.DataFrame({\n    \"prompt\": [\"What is machine learning?\", \"Explain quantum computing\"],\n    \"model\": [\"gpt-4\", \"gpt-4\"],\n    \"model_response\": [\"Machine learning involves...\", \"Quantum computing uses...\"],\n    \"score\": [{\"accuracy\": 1, \"helpfulness\": 4.2}, {\"accuracy\": 0, \"helpfulness\": 3.8}]\n})\n\n# Extract and cluster behavioral properties\nclustered_df, model_stats = explain(\n    df,\n    method=\"single_model\",\n    output_dir=\"results/\"\n)\n\n# View results using the React frontend or other visualization tools\n</code></pre>"},{"location":"#use-cases","title":"Use Cases","text":""},{"location":"#model-evaluation-comparison","title":"\ud83c\udfc6 Model Evaluation &amp; Comparison","text":"<p>Compare multiple models to understand their behavioral strengths and weaknesses. Identify which models excel at specific tasks (reasoning, creativity, factual accuracy, etc.).</p>"},{"location":"#research-analysis","title":"\ud83d\udd2c Research &amp; Analysis","text":"<p>Analyze how model behavior changes across:</p> <ul> <li>Different prompting strategies</li> <li>Model versions/checkpoints</li> <li>Fine-tuning approaches</li> <li>Temperature settings</li> </ul>"},{"location":"#task-specific-evaluation","title":"Task-Specific Evaluation","text":"<p>Focus on behaviors relevant to your domain:</p> <ul> <li>Call center responses (empathy, clarity, policy adherence)</li> <li>Code generation (correctness, efficiency, edge case handling)</li> <li>Creative writing (originality, coherence, style)</li> </ul>"},{"location":"#how-it-works","title":"How It Works","text":"<p>StringSight uses a 4-stage pipeline:</p> <pre><code>Data Input \u2192 Property Extraction \u2192 Post-processing \u2192 Clustering \u2192 Metrics &amp; Analysis\n</code></pre> <ol> <li>Property Extraction - An LLM analyzes each response and extracts behavioral properties</li> <li>Post-processing - Parse and validate extracted properties into structured data</li> <li>Clustering - Group similar properties using embeddings and HDBSCAN</li> <li>Metrics &amp; Analysis - Calculate per-model statistics, quality scores, and significance tests</li> </ol>"},{"location":"#installation","title":"Installation","text":"<pre><code># Create conda environment\nconda create -n stringsight python=3.11\nconda activate stringsight\n\n# Install StringSight\npip install -e \".[full]\"\n\n# Set API key\nexport OPENAI_API_KEY=\"your-api-key-here\"\n</code></pre> <p>See the Installation Guide for detailed setup instructions.</p>"},{"location":"#next-steps","title":"Next Steps","text":"<ul> <li>Quick Start - Get up and running in 5 minutes</li> <li> <p>User Guide - Learn how to use StringSight effectively</p> </li> <li> <p>Advanced Usage - Custom pipelines and performance tuning</p> </li> </ul>"},{"location":"#support","title":"Support","text":"<ul> <li>Documentation: You're reading it!</li> <li>Issues: GitHub Issues</li> <li>Source Code: GitHub Repository</li> </ul>"},{"location":"#license","title":"License","text":"<p>StringSight is released under the MIT License.</p>"},{"location":"CACHING/","title":"Caching Configuration","text":"<p>StringSight uses disk-based caching to speed up repeated operations and reduce API costs.</p>"},{"location":"CACHING/#default-behavior","title":"Default Behavior","text":"<p>Caching is ENABLED by default for both: - LLM Completions: Cluster summaries, label generation, property extraction - Embeddings: Text embeddings for clustering</p> <p>Cache is stored in <code>.cache/stringsight/</code> with a 50GB size limit.</p>"},{"location":"CACHING/#environment-variables","title":"Environment Variables","text":""},{"location":"CACHING/#disable-all-caching","title":"Disable All Caching","text":"<p><pre><code>export STRINGSIGHT_DISABLE_CACHE=1\n</code></pre> Disables both LLM completions and embeddings caching.</p>"},{"location":"CACHING/#disable-only-embedding-caching","title":"Disable Only Embedding Caching","text":"<p><pre><code>export STRINGSIGHT_DISABLE_EMBEDDING_CACHE=1\n</code></pre> LLM completions will still be cached, but embeddings will be recomputed each time.</p> <p>Use case: When you want to ensure fresh embeddings (e.g., testing different embedding models) while still benefiting from LLM completion caching.</p>"},{"location":"CACHING/#customize-cache-location","title":"Customize Cache Location","text":"<p><pre><code>export STRINGSIGHT_CACHE_DIR=\"/path/to/cache\"\n</code></pre> Default: <code>.cache/stringsight/</code></p>"},{"location":"CACHING/#customize-cache-size","title":"Customize Cache Size","text":"<p><pre><code>export STRINGSIGHT_CACHE_MAX_SIZE=\"100GB\"\n</code></pre> Default: <code>50GB</code></p> <p>Supported units: <code>B</code>, <code>KB</code>, <code>MB</code>, <code>GB</code>, <code>TB</code></p>"},{"location":"CACHING/#examples","title":"Examples","text":""},{"location":"CACHING/#example-1-default-all-caching-enabled","title":"Example 1: Default (All Caching Enabled)","text":"<pre><code># No environment variables needed\npython your_script.py\n</code></pre> <p>Logs will show: <pre><code>INFO: LLM caching enabled (embeddings=enabled)\nINFO: Caching enabled for clustering (cache_dir=.cache/stringsight, embeddings=enabled)\n</code></pre></p>"},{"location":"CACHING/#example-2-disable-embedding-cache-only","title":"Example 2: Disable Embedding Cache Only","text":"<pre><code>export STRINGSIGHT_DISABLE_EMBEDDING_CACHE=1\npython your_script.py\n</code></pre> <p>Logs will show: <pre><code>INFO: LLM caching enabled (embeddings=disabled)\nINFO: Caching enabled for clustering (cache_dir=.cache/stringsight, embeddings=disabled)\n</code></pre></p> <p>Result:  - \u2705 LLM completions (cluster labels, summaries) are cached - \u274c Embeddings are computed fresh each time</p>"},{"location":"CACHING/#example-3-disable-all-caching","title":"Example 3: Disable All Caching","text":"<pre><code>export STRINGSIGHT_DISABLE_CACHE=1\npython your_script.py\n</code></pre> <p>Logs will show: <pre><code>INFO: LLM caching disabled (STRINGSIGHT_DISABLE_CACHE=1)\nINFO: Caching disabled for clustering (set STRINGSIGHT_DISABLE_CACHE=0 to enable)\n</code></pre></p> <p>Result: Everything is computed fresh each time.</p>"},{"location":"CACHING/#example-4-custom-cache-configuration","title":"Example 4: Custom Cache Configuration","text":"<pre><code>export STRINGSIGHT_CACHE_DIR=\"/mnt/fast-ssd/cache\"\nexport STRINGSIGHT_CACHE_MAX_SIZE=\"200GB\"\nexport STRINGSIGHT_DISABLE_EMBEDDING_CACHE=1\npython your_script.py\n</code></pre> <p>Result:  - LLM completions cached in <code>/mnt/fast-ssd/cache/</code> (up to 200GB) - Embeddings not cached</p>"},{"location":"CACHING/#cache-performance","title":"Cache Performance","text":"<p>With caching enabled: - 10-100x faster for repeated clustering runs on same data - Significant cost savings by avoiding duplicate LLM API calls - Automatic invalidation ensures fresh data when inputs change</p>"},{"location":"CACHING/#cache-management","title":"Cache Management","text":""},{"location":"CACHING/#view-cache-size","title":"View Cache Size","text":"<pre><code>du -sh .cache/stringsight/\n</code></pre>"},{"location":"CACHING/#clear-cache","title":"Clear Cache","text":"<pre><code>rm -rf .cache/stringsight/\n</code></pre>"},{"location":"CACHING/#clear-only-embeddings-cache","title":"Clear Only Embeddings Cache","text":"<pre><code>rm -rf .cache/stringsight/embeddings/\n</code></pre>"},{"location":"CACHING/#clear-only-completions-cache","title":"Clear Only Completions Cache","text":"<pre><code>rm -rf .cache/stringsight/completions/\n</code></pre>"},{"location":"CACHING/#when-to-disable-embedding-cache","title":"When to Disable Embedding Cache","text":"<p>Consider disabling embedding cache when: - Testing different embedding models - Debugging embedding-related issues - Working with constantly changing text data - Benchmarking embedding performance</p>"},{"location":"CACHING/#technical-details","title":"Technical Details","text":"<ul> <li>Cache Type: DiskCache (thread-safe, persistent across runs)</li> <li>Cache Keys: SHA-256 hashes of input parameters</li> <li>Namespacing: Completions and embeddings stored separately</li> <li>Model-aware: Embeddings are namespaced by model to prevent dimension mismatches</li> <li>Eviction: LRU (Least Recently Used) when size limit is reached</li> </ul>"},{"location":"CONVERSATION_EXTRACTION/","title":"Data Extraction","text":""},{"location":"CONVERSATION_EXTRACTION/#overview","title":"Overview","text":"<p>The pipeline now automatically saves conversations, properties, and clusters as separate JSONL files in the results directory. This makes it easy to load and work with specific data components without loading the entire dataset.</p>"},{"location":"CONVERSATION_EXTRACTION/#files-created","title":"Files Created","text":"<p>When running the pipeline with an <code>output_dir</code>, these files are created:</p> <ol> <li><code>full_dataset.json</code> - Complete dataset including conversations, properties, clusters, and model stats</li> <li><code>conversation.jsonl</code> - Just the conversations in JSONL format (one conversation per line)</li> <li><code>properties.jsonl</code> - Just the properties in JSONL format (one property per line)</li> <li><code>clusters.jsonl</code> - Just the clusters in JSONL format (one cluster per line)</li> </ol>"},{"location":"CONVERSATION_EXTRACTION/#conversation-format","title":"Conversation Format","text":"<p>The <code>conversation.jsonl</code> file uses the input format expected by the pipeline, making it easy to reuse extracted conversations as new inputs.</p>"},{"location":"CONVERSATION_EXTRACTION/#single-model-format","title":"Single Model Format","text":"<p>Each line contains a single conversation:</p> <pre><code>{\n  \"question_id\": \"0\",\n  \"prompt\": \"User's input prompt\",\n  \"model\": \"model_name\",\n  \"model_response\": [\n    {\"role\": \"user\", \"content\": \"...\"},\n    {\"role\": \"assistant\", \"content\": \"...\"}\n  ],\n  \"score\": {\"Helpfulness\": 5.0, \"Conciseness\": 4.0}\n}\n</code></pre>"},{"location":"CONVERSATION_EXTRACTION/#side-by-side-format","title":"Side-by-Side Format","text":"<p>For side-by-side comparisons:</p> <pre><code>{\n  \"question_id\": \"0\",\n  \"prompt\": \"User's input prompt\",\n  \"model_a\": \"gpt-4\",\n  \"model_b\": \"claude-3\",\n  \"model_a_response\": [\n    {\"role\": \"user\", \"content\": \"...\"},\n    {\"role\": \"assistant\", \"content\": \"...\"}\n  ],\n  \"model_b_response\": [\n    {\"role\": \"user\", \"content\": \"...\"},\n    {\"role\": \"assistant\", \"content\": \"...\"}\n  ],\n  \"score_a\": {\"Helpfulness\": 5.0},\n  \"score_b\": {\"Helpfulness\": 4.0},\n  \"winner\": \"model_a\"\n}\n</code></pre> <p>Key points: - Uses <code>score</code> (not <code>scores</code>) for single model - Uses <code>score_a</code> and <code>score_b</code> (not <code>scores</code> list) for side-by-side - Uses <code>model_response</code> (not <code>responses</code>) for single model - Uses <code>model_a_response</code> and <code>model_b_response</code> for side-by-side - Metadata fields like <code>winner</code> are at the top level</p>"},{"location":"CONVERSATION_EXTRACTION/#properties-format","title":"Properties Format","text":"<p>Each line in <code>properties.jsonl</code> contains a single property:</p> <pre><code>{\n  \"id\": \"67891533-db42-45e0-bde2-fe7e1840b4a2\",\n  \"question_id\": \"352\",\n  \"model\": \"xai/grok-3-mini-beta\",\n  \"property_description\": \"Fulfills the user's length constraint\",\n  \"category\": \"User Experience\",\n  \"reason\": \"...\",\n  \"evidence\": \"...\",\n  \"behavior_type\": \"Positive\",\n  \"raw_response\": null,\n  \"contains_errors\": false,\n  \"unexpected_behavior\": false,\n  \"meta\": {}\n}\n</code></pre>"},{"location":"CONVERSATION_EXTRACTION/#clusters-format","title":"Clusters Format","text":"<p>Each line in <code>clusters.jsonl</code> contains a single cluster:</p> <pre><code>{\n  \"id\": \"1\",\n  \"label\": \"Strictly follows user instructions\",\n  \"size\": 120,\n  \"property_descriptions\": [\"...\", \"...\"],\n  \"property_ids\": [\"...\", \"...\"],\n  \"question_ids\": [\"...\", \"...\"],\n  \"meta\": {}\n}\n</code></pre>"},{"location":"CONVERSATION_EXTRACTION/#extracting-data-from-existing-results","title":"Extracting Data from Existing Results","text":"<p>If you have existing results that don't have the separate JSONL files, use the extraction script:</p> <pre><code>python scripts/extract_conversations.py &lt;results_directory&gt;\n</code></pre> <p>Example: <pre><code>python scripts/extract_conversations.py results/koala\n</code></pre></p> <p>This will: 1. Read <code>full_dataset.json</code> from the specified directory 2. Extract the conversations, properties, and clusters fields 3. Save to separate JSONL files in the same directory:    - <code>conversation.jsonl</code>    - <code>properties.jsonl</code> (if properties exist)    - <code>clusters.jsonl</code> (if clusters exist)</p>"},{"location":"CONVERSATION_EXTRACTION/#loading-data","title":"Loading Data","text":""},{"location":"CONVERSATION_EXTRACTION/#python-api","title":"Python API","text":"<pre><code>import json\nimport pandas as pd\n\n# Load conversations\nconversations = []\nwith open(\"results/my_experiment/conversation.jsonl\", \"r\") as f:\n    for line in f:\n        conversations.append(json.loads(line))\n\n# Or use pandas for any JSONL file\nconversations_df = pd.read_json(\"results/my_experiment/conversation.jsonl\", lines=True)\nproperties_df = pd.read_json(\"results/my_experiment/properties.jsonl\", lines=True)\nclusters_df = pd.read_json(\"results/my_experiment/clusters.jsonl\", lines=True)\n</code></pre>"},{"location":"CONVERSATION_EXTRACTION/#frontend","title":"Frontend","text":"<p>The frontend can now load data components independently for faster initial page loads:</p> <pre><code>// Load conversations\nfetch('/path/to/conversation.jsonl')\n  .then(response =&gt; response.text())\n  .then(text =&gt; {\n    const conversations = text.trim().split('\\n').map(line =&gt; JSON.parse(line));\n    // Use conversations...\n  });\n\n// Load properties\nfetch('/path/to/properties.jsonl')\n  .then(response =&gt; response.text())\n  .then(text =&gt; {\n    const properties = text.trim().split('\\n').map(line =&gt; JSON.parse(line));\n    // Use properties...\n  });\n\n// Load clusters\nfetch('/path/to/clusters.jsonl')\n  .then(response =&gt; response.text())\n  .then(text =&gt; {\n    const clusters = text.trim().split('\\n').map(line =&gt; JSON.parse(line));\n    // Use clusters...\n  });\n</code></pre>"},{"location":"CONVERSATION_EXTRACTION/#benefits","title":"Benefits","text":"<ol> <li>Faster loading - Load only the data components you need</li> <li>Streaming - JSONL format allows streaming line-by-line for large datasets</li> <li>Smaller files - Individual components are much smaller than the full dataset</li> <li>Standard format - JSONL is widely supported and easy to process</li> <li>Reusable - <code>conversation.jsonl</code> uses input format, making it easy to reuse as pipeline input</li> <li>Independent - Properties and clusters can be loaded without conversations for analysis</li> </ol>"},{"location":"frontend-results-files/","title":"Frontend results files","text":""},{"location":"frontend-results-files/#ui-results-files-required-inputs-and-structures","title":"UI Results Files: Required Inputs and Structures","text":""},{"location":"frontend-results-files/#scope","title":"Scope","text":"<ul> <li>Only the files the UI actually reads to load results</li> <li>Metrics are assumed to use the df.jsonl variants (not the legacy .json files)</li> </ul>"},{"location":"frontend-results-files/#load-paths","title":"Load Paths","text":"<ul> <li>Server/API path: UI calls backend endpoint to read files from a results directory</li> <li>Folder upload path: UI reads files directly in the browser</li> </ul>"},{"location":"frontend-results-files/#quick-reference-file-purpose","title":"Quick Reference: File Purpose","text":"File Purpose Contains <code>conversation.jsonl</code> User prompts and model responses Question ID, prompt, model(s), response(s), scores <code>properties.jsonl</code> Extracted behavioral properties Property ID, description, category, evidence, behavior type <code>clusters.jsonl</code> Grouped similar properties Cluster ID, label, size, property/question IDs <code>full_dataset.json</code> Complete archive (fallback) All conversations, properties, clusters, model stats <code>model_cluster_scores_df.jsonl</code> Per-model-cluster metrics Model, cluster, size, proportions, quality scores <code>model_scores_df.jsonl</code> Per-model aggregates Model, total size, average quality scores <code>cluster_scores_df.jsonl</code> Per-cluster aggregates Cluster, total size, average proportions, quality <p>Recommended: Load the three JSONL files (<code>conversation.jsonl</code>, <code>properties.jsonl</code>, <code>clusters.jsonl</code>) instead of extracting from <code>full_dataset.json</code> for better performance.</p>"},{"location":"frontend-results-files/#required-files","title":"Required files","text":"<p>1) conversation.jsonl - Used by: Server/API path and Folder upload path - Purpose: Contains all conversations/prompts with model responses and scores for the Data tab - Format: JSONL (one JSON object per line) - Description: Each line represents a single conversation with a user prompt and model response(s). Uses the input format with <code>score</code> (single model) or <code>score_a</code>/<code>score_b</code> (side-by-side). Responses are in OpenAI message format with role/content pairs.</p> <p>Single model structure: <pre><code>{\n  \"question_id\": \"352\",\n  \"prompt\": \"Write a Python function to reverse a string\",\n  \"model\": \"gpt-4\",\n  \"model_response\": [\n    {\"role\": \"user\", \"content\": \"Write a Python function to reverse a string\"},\n    {\"role\": \"assistant\", \"content\": \"Here's a function to reverse a string:\\n\\n```python\\ndef reverse_string(s):\\n    return s[::-1]\\n```\"}\n  ],\n  \"score\": {\"helpfulness\": 5.0, \"conciseness\": 4.5}\n}\n</code></pre></p> <p>Side-by-side structure: <pre><code>{\n  \"question_id\": \"352\",\n  \"prompt\": \"Write a Python function to reverse a string\",\n  \"model_a\": \"gpt-4\",\n  \"model_b\": \"claude-3-opus\",\n  \"model_a_response\": [\n    {\"role\": \"user\", \"content\": \"Write a Python function to reverse a string\"},\n    {\"role\": \"assistant\", \"content\": \"Here's a function:\\n\\n```python\\ndef reverse_string(s):\\n    return s[::-1]\\n```\"}\n  ],\n  \"model_b_response\": [\n    {\"role\": \"user\", \"content\": \"Write a Python function to reverse a string\"},\n    {\"role\": \"assistant\", \"content\": \"Here's how to reverse a string:\\n\\n```python\\ndef reverse_string(text):\\n    return text[::-1]\\n```\"}\n  ],\n  \"score_a\": {\"helpfulness\": 5.0},\n  \"score_b\": {\"helpfulness\": 4.5},\n  \"winner\": \"model_a\"\n}\n</code></pre></p> <p>1b) clustered_results_lightweight.jsonl (DEPRECATED) - Used by: Server/API path (legacy support only) - Purpose: Old format for conversations - Note: This file is deprecated. New pipelines generate <code>conversation.jsonl</code> instead. Use <code>conversation.jsonl</code> for all new implementations.</p> <p>2) properties.jsonl - Used by: Server/API path and Folder upload path - Purpose: Contains extracted behavioral properties from model responses for the Properties and Clusters tabs - Format: JSONL (one JSON object per line) - Description: Each line represents a single extracted property describing a specific behavior exhibited by a model in its response. Properties include categorization, evidence from the response, and behavior type classification. These are standalone objects without cluster joins.</p> <p>Sample row: <pre><code>{\n  \"id\": \"67891533-db42-45e0-bde2-fe7e1840b4a2\",\n  \"question_id\": \"352\",\n  \"model\": \"gpt-4\",\n  \"property_description\": \"Provides code with proper syntax highlighting and formatting\",\n  \"category\": \"Code Quality\",\n  \"reason\": \"The model formatted the code in a clear, readable manner with proper Python syntax\",\n  \"evidence\": \"The response includes properly indented code within markdown code blocks\",\n  \"behavior_type\": \"Positive\",\n  \"raw_response\": null,\n  \"contains_errors\": false,\n  \"unexpected_behavior\": false,\n  \"meta\": {}\n}\n</code></pre></p> <p>2b) parsed_properties.jsonl (DEPRECATED) - Used by: Server/API path (legacy support only) - Purpose: Old format for properties with cluster joins - Note: This file is deprecated. New pipelines generate <code>properties.jsonl</code> instead (standalone properties without cluster joins). Use <code>properties.jsonl</code> for all new implementations.</p> <p>3) clusters.jsonl - Used by: Server/API path and Folder upload path - Purpose: Contains cluster definitions grouping similar properties for the Clusters tab - Format: JSONL (one JSON object per line) - Description: Each line represents a cluster of similar behavioral properties. Clusters group properties by semantic similarity and include lists of property descriptions, property IDs, and question IDs that belong to the cluster. These are standalone cluster definitions that can be joined with properties as needed.</p> <p>Sample row: <pre><code>{\n  \"id\": \"1\",\n  \"label\": \"Provides well-formatted code examples\",\n  \"size\": 45,\n  \"property_descriptions\": [\n    \"Provides code with proper syntax highlighting and formatting\",\n    \"Includes markdown code blocks for code snippets\",\n    \"Uses consistent indentation in code examples\"\n  ],\n  \"property_ids\": [\n    \"67891533-db42-45e0-bde2-fe7e1840b4a2\",\n    \"a1b2c3d4-e5f6-4789-abcd-ef1234567890\",\n    \"12345678-90ab-cdef-1234-567890abcdef\"\n  ],\n  \"question_ids\": [\n    \"352\",\n    \"423\",\n    \"501\"\n  ],\n  \"meta\": {}\n}\n</code></pre></p> <p>4) full_dataset.json - Used by: Folder upload path as fallback; Server/API path (legacy) - Purpose: Complete dataset bundle containing all data (conversations, properties, clusters, model stats) - Format: JSON (single large object) - Description: A single JSON file containing the entire dataset in internal format. This is useful for archiving complete results or as a fallback when individual JSONL files are not available. However, loading individual JSONL files is preferred for better performance.</p> <p>Structure: <pre><code>{\n  \"conversations\": [\n    {\n      \"question_id\": \"352\",\n      \"prompt\": \"Write a Python function to reverse a string\",\n      \"model\": \"gpt-4\",\n      \"responses\": [\n        {\"role\": \"user\", \"content\": \"...\"},\n        {\"role\": \"assistant\", \"content\": \"...\"}\n      ],\n      \"scores\": {\"helpfulness\": 5.0},\n      \"meta\": {}\n    }\n  ],\n  \"properties\": [\n    {\n      \"id\": \"67891533-db42-45e0-bde2-fe7e1840b4a2\",\n      \"question_id\": \"352\",\n      \"model\": \"gpt-4\",\n      \"property_description\": \"Provides well-formatted code\",\n      \"category\": \"Code Quality\",\n      \"behavior_type\": \"Positive\",\n      \"meta\": {}\n    }\n  ],\n  \"clusters\": [\n    {\n      \"id\": \"1\",\n      \"label\": \"Provides well-formatted code examples\",\n      \"size\": 45,\n      \"property_descriptions\": [\"...\", \"...\"],\n      \"property_ids\": [\"...\", \"...\"],\n      \"question_ids\": [\"...\", \"...\"],\n      \"meta\": {}\n    }\n  ],\n  \"model_stats\": {\n    \"gpt-4\": {\"total_properties\": 150, \"avg_quality\": 4.5}\n  },\n  \"all_models\": [\"gpt-4\", \"claude-3-opus\", \"gemini-pro\"]\n}\n</code></pre></p> <p>Note: Prefer loading individual JSONL files (<code>conversation.jsonl</code>, <code>properties.jsonl</code>, <code>clusters.jsonl</code>) for better performance. The <code>full_dataset.json</code> uses internal format (e.g., <code>scores</code> instead of <code>score</code>), while JSONL files use input format.</p>"},{"location":"frontend-results-files/#metrics-files-dfjsonl-variants-load-only-if-metrics-are-computed","title":"Metrics Files (df.jsonl variants; load only if metrics are computed)","text":"<p>5) model_cluster_scores_df.jsonl - Used by: Server/API path and Folder upload path - Purpose: Per-model per-cluster metrics showing how each model performs on each behavioral cluster - Format: JSONL (one JSON object per line) - Description: Each line represents metrics for a specific model-cluster combination. Shows the size (number of properties), proportion of that model's total properties in this cluster, delta compared to baseline, and quality scores for each metric. Used for model comparison charts and cluster breakdowns.</p> <p>Sample row: <pre><code>{\n  \"model\": \"gpt-4\",\n  \"cluster\": \"Provides well-formatted code examples\",\n  \"size\": 25,\n  \"proportion\": 0.35,\n  \"proportion_delta\": 0.05,\n  \"quality_helpfulness\": 0.82,\n  \"quality_helpfulness_delta\": 0.03,\n  \"quality_conciseness\": 0.78,\n  \"quality_conciseness_delta\": 0.01\n}\n</code></pre></p> <p>6) model_scores_df.jsonl - Used by: Server/API path and Folder upload path - Purpose: Aggregated metrics for each model across all clusters - Format: JSONL (one JSON object per line) - Description: Each line represents overall metrics for a single model. Includes total size (number of properties), average quality scores across all metrics, and model-level statistics. Used for high-level model comparison summaries.</p> <p>Sample row: <pre><code>{\n  \"model\": \"gpt-4\",\n  \"size\": 300,\n  \"avg_quality_overall\": 0.81,\n  \"avg_quality_helpfulness\": 0.83,\n  \"avg_quality_conciseness\": 0.79,\n  \"avg_quality_harmlessness\": 0.95\n}\n</code></pre></p> <p>7) cluster_scores_df.jsonl - Used by: Server/API path and Folder upload path - Purpose: Aggregated metrics for each cluster across all models - Format: JSONL (one JSON object per line) - Description: Each line represents overall metrics for a single behavioral cluster. Shows total size across all models, average proportion (how common this cluster is), and average quality scores. Used for cluster-level analysis and identifying common behavioral patterns.</p> <p>Sample row: <pre><code>{\n  \"cluster\": \"Provides well-formatted code examples\",\n  \"size\": 120,\n  \"proportion\": 0.40,\n  \"avg_quality_overall\": 0.79,\n  \"avg_quality_helpfulness\": 0.81,\n  \"avg_quality_conciseness\": 0.77\n}\n</code></pre></p>"},{"location":"frontend-results-files/#notes","title":"Notes","text":"<p>Loading order preferences: - Conversations: <code>conversation.jsonl</code> \u2192 <code>clustered_results_lightweight.jsonl</code> (deprecated) \u2192 <code>full_dataset.json</code> (fallback) - Properties: <code>properties.jsonl</code> \u2192 <code>parsed_properties.jsonl</code> (deprecated) \u2192 <code>full_dataset.json</code> (fallback) - Clusters: <code>clusters.jsonl</code> \u2192 <code>full_dataset.json</code> (fallback)</p> <p>Key points: - The UI should load <code>conversation.jsonl</code>, <code>properties.jsonl</code>, and <code>clusters.jsonl</code> as separate files instead of extracting from <code>full_dataset.json</code> for better performance - <code>conversation.jsonl</code> uses the input format (score/score_a/score_b, model_response/model_a_response/model_b_response) which matches the expected pipeline input format - <code>properties.jsonl</code> contains standalone properties without cluster joins (unlike deprecated <code>parsed_properties.jsonl</code>) - <code>clusters.jsonl</code> contains standalone cluster metadata - The UI enriches cluster <code>meta</code> at runtime by joining properties and calculating per\u2011model aggregates - Metrics: prefer df.jsonl files. Do not load legacy <code>model_cluster_scores.json</code>, <code>cluster_scores.json</code>, or <code>model_scores.json</code> if df.jsonl files are present</p>"},{"location":"frontend-results-files/#file-relationships-and-joins","title":"File Relationships and Joins","text":"<p>The files are designed to be joined on specific keys:</p> <pre><code>conversation.jsonl (question_id, model)\n    \u2193 join on question_id + model\nproperties.jsonl (question_id, model, id)\n    \u2193 join on property_description\nclusters.jsonl (id, property_descriptions[], property_ids[], question_ids[])\n</code></pre> <p>Example join logic: 1. Load all three files independently 2. To display properties for a conversation: join <code>properties.jsonl</code> with <code>conversation.jsonl</code> on <code>question_id</code> and <code>model</code> 3. To display clusters for properties: join <code>clusters.jsonl</code> with <code>properties.jsonl</code> where <code>property.property_description</code> is in <code>cluster.property_descriptions[]</code> 4. To display metrics: load metric files and join on <code>model</code> and/or <code>cluster</code> fields</p> <p>Why separate files? - Performance: Load only what you need - Flexibility: Different views need different data - Streaming: JSONL can be processed line-by-line - Caching: Cache individual components separately - Reusability: <code>conversation.jsonl</code> can be reused as pipeline input</p>"},{"location":"troubleshooting/","title":"Troubleshooting","text":"<p>Common issues and solutions for StringSight.</p>"},{"location":"troubleshooting/#installation-issues","title":"Installation Issues","text":""},{"location":"troubleshooting/#no-module-named-stringsight","title":"\"No module named 'stringsight'\"","text":"<p>Solution: <pre><code>conda activate stringsight\npip install -e \".[full]\"\n</code></pre></p>"},{"location":"troubleshooting/#pytorchcuda-errors","title":"PyTorch/CUDA Errors","text":"<p>Solution: <pre><code>conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia\n</code></pre></p>"},{"location":"troubleshooting/#nodejs-version-issues","title":"Node.js Version Issues","text":"<p>Solution: <pre><code>conda install -c conda-forge nodejs=20\n# Or use nvm\nnvm install 20 &amp;&amp; nvm use 20\n</code></pre></p>"},{"location":"troubleshooting/#runtime-issues","title":"Runtime Issues","text":""},{"location":"troubleshooting/#openai-api-key-not-found","title":"\"OpenAI API key not found\"","text":"<p>Solution: <pre><code>export OPENAI_API_KEY=\"your-api-key-here\"\n# Or create .env file\necho \"OPENAI_API_KEY=your-api-key-here\" &gt; .env\n</code></pre></p>"},{"location":"troubleshooting/#clustering-produced-no-valid-clusters","title":"\"Clustering produced no valid clusters\"","text":"<p>Causes: - Dataset too small - min_cluster_size too large - Properties too similar/dissimilar</p> <p>Solutions: <pre><code># Reduce cluster size threshold\nexplain(df, min_cluster_size=5)\n\n# Use more data (minimum 20-50 conversations recommended)\n\n# Try different embedding model\nexplain(df, embedding_model=\"all-MiniLM-L6-v2\")\n</code></pre></p>"},{"location":"troubleshooting/#out-of-memory-errors","title":"Out of Memory Errors","text":"<p>Solutions: <pre><code># Use local embeddings\nexplain(df, embedding_model=\"all-MiniLM-L6-v2\")\n\n# Disable embeddings in output\nexplain(df, include_embeddings=False)\n\n# Increase cluster size\nexplain(df, min_cluster_size=50)\n\n# Process in batches\nfor chunk in pd.read_csv(\"data.csv\", chunksize=1000):\n    explain(chunk, output_dir=\"results/batch\")\n</code></pre></p>"},{"location":"troubleshooting/#frontend-issues","title":"Frontend Issues","text":""},{"location":"troubleshooting/#port-already-in-use","title":"Port Already in Use","text":"<p>Solution: <pre><code># Kill process\nlsof -ti:8000 | xargs kill -9\nlsof -ti:5173 | xargs kill -9\n\n# Or use different port\npython -m uvicorn stringsight.api:app --port 8001\n</code></pre></p>"},{"location":"troubleshooting/#cors-errors","title":"CORS Errors","text":"<p>Solution: Check <code>stringsight/api.py</code> CORS configuration includes your frontend URL.</p>"},{"location":"troubleshooting/#frontend-wont-start","title":"Frontend Won't Start","text":"<p>Solution: <pre><code>cd frontend/\nrm -rf node_modules package-lock.json\nnpm install\nnpm run dev\n</code></pre></p>"},{"location":"troubleshooting/#data-issues","title":"Data Issues","text":""},{"location":"troubleshooting/#missing-required-column","title":"\"Missing required column\"","text":"<p>Solution: <pre><code># Check columns\nprint(df.columns.tolist())\n\n# Rename if needed\ndf = df.rename(columns={'response': 'model_response'})\n</code></pre></p>"},{"location":"troubleshooting/#invalid-response-format","title":"\"Invalid response format\"","text":"<p>Solution: <pre><code># Convert to strings\ndf['model_response'] = df['model_response'].astype(str)\n</code></pre></p>"},{"location":"troubleshooting/#score-column-not-recognized","title":"Score Column Not Recognized","text":"<p>Solution: <pre><code>import json\n# If scores are strings\ndf['score'] = df['score'].apply(json.loads)\n</code></pre></p>"},{"location":"troubleshooting/#performance-issues","title":"Performance Issues","text":""},{"location":"troubleshooting/#slow-extraction","title":"Slow Extraction","text":"<p>Solutions: <pre><code># Use faster model\nexplain(df, model_name=\"gpt-4o-mini\")\n\n# Increase parallelism\nexplain(df, max_workers=32)\n\n# Enable caching\nexplain(df, extraction_cache_dir=\".cache/extraction\")\n</code></pre></p>"},{"location":"troubleshooting/#slow-clustering","title":"Slow Clustering","text":"<p>Solutions: <pre><code># Use local embeddings\nexplain(df, embedding_model=\"all-MiniLM-L6-v2\")\n\n# Disable dimensionality reduction\nfrom stringsight.clusterers import HDBSCANClusterer\nclusterer = HDBSCANClusterer(disable_dim_reduction=True)\n</code></pre></p>"},{"location":"troubleshooting/#getting-help","title":"Getting Help","text":"<ul> <li>GitHub Issues: Report bugs</li> <li>Documentation: Read the docs</li> <li>Logs: Check console output for error details</li> </ul>"},{"location":"advanced/custom-pipelines/","title":"Custom Pipelines","text":"<p>Learn how to build custom analysis pipelines with StringSight's modular architecture.</p>"},{"location":"advanced/custom-pipelines/#pipeline-architecture","title":"Pipeline Architecture","text":"<p>StringSight uses a 4-stage pipeline where each stage operates on a <code>PropertyDataset</code> object:</p> <pre><code>PropertyDataset \u2192 Extraction \u2192 Post-processing \u2192 Clustering \u2192 Metrics \u2192 PropertyDataset\n</code></pre> <p>Each stage: - Inherits from <code>PipelineStage</code> - Implements a <code>run(dataset: PropertyDataset) -&gt; PropertyDataset</code> method - Can be configured independently - Supports caching to avoid recomputation</p>"},{"location":"advanced/custom-pipelines/#basic-custom-pipeline","title":"Basic Custom Pipeline","text":""},{"location":"advanced/custom-pipelines/#using-pipelinebuilder","title":"Using PipelineBuilder","text":"<pre><code>from stringsight.pipeline import PipelineBuilder\nfrom stringsight.extractors import OpenAIExtractor\nfrom stringsight.postprocess import LLMJsonParser, PropertyValidator\nfrom stringsight.clusterers import HDBSCANClusterer\nfrom stringsight.metrics import SingleModelMetrics\n\n# Build custom pipeline\npipeline = (PipelineBuilder(\"My Custom Pipeline\")\n    .extract_properties(\n        OpenAIExtractor(\n            model=\"gpt-4o-mini\",\n            temperature=0.5\n        )\n    )\n    .parse_properties(LLMJsonParser())\n    .validate_properties(PropertyValidator())\n    .cluster_properties(\n        HDBSCANClusterer(\n            min_cluster_size=5,\n            embedding_model=\"all-MiniLM-L6-v2\"\n        )\n    )\n    .compute_metrics(SingleModelMetrics())\n    .configure(\n        use_wandb=True,\n        wandb_project=\"custom-analysis\"\n    )\n    .build())\n\n# Use with explain()\nfrom stringsight import explain\n\nclustered_df, model_stats = explain(\n    df,\n    custom_pipeline=pipeline\n)\n</code></pre>"},{"location":"advanced/custom-pipelines/#manual-pipeline-construction","title":"Manual Pipeline Construction","text":"<pre><code>from stringsight.pipeline import Pipeline\nfrom stringsight.core import PropertyDataset\n\n# Create dataset from DataFrame\ndataset = PropertyDataset.from_dataframe(df, method=\"single_model\")\n\n# Initialize pipeline\npipeline = Pipeline(\"Manual Pipeline\")\n\n# Add stages\npipeline.add_stage(OpenAIExtractor(model=\"gpt-4.1\"))\npipeline.add_stage(LLMJsonParser())\npipeline.add_stage(PropertyValidator())\npipeline.add_stage(HDBSCANClusterer(min_cluster_size=15))\npipeline.add_stage(SingleModelMetrics())\n\n# Run pipeline\nresult_dataset = pipeline.run(dataset)\n\n# Extract results\nclustered_df = result_dataset.to_dataframe()\nmodel_stats = result_dataset.model_stats\n</code></pre>"},{"location":"advanced/custom-pipelines/#custom-extractors","title":"Custom Extractors","text":"<p>Create custom property extractors by inheriting from <code>PipelineStage</code>:</p> <pre><code>from stringsight.core.stage import PipelineStage\nfrom stringsight.core.data_objects import PropertyDataset, Property\nfrom typing import List\nimport anthropic\n\nclass ClaudeExtractor(PipelineStage):\n    \"\"\"Custom extractor using Anthropic's Claude API.\"\"\"\n\n    def __init__(self, model: str = \"claude-3-opus-20240229\", **kwargs):\n        super().__init__(**kwargs)\n        self.model = model\n        self.client = anthropic.Anthropic()\n\n    def run(self, data: PropertyDataset) -&gt; PropertyDataset:\n        \"\"\"Extract properties using Claude.\"\"\"\n        properties = []\n\n        for conv in data.conversations:\n            # Build prompt\n            prompt = f\"Analyze this response and identify key behavioral properties:\\n\\n{conv.responses}\"\n\n            # Call Claude\n            response = self.client.messages.create(\n                model=self.model,\n                max_tokens=1024,\n                messages=[{\"role\": \"user\", \"content\": prompt}]\n            )\n\n            # Parse response and create Property objects\n            prop = Property(\n                id=f\"{conv.question_id}_prop\",\n                question_id=conv.question_id,\n                model=conv.model,\n                property_description=response.content[0].text,\n                raw_response=response.content[0].text\n            )\n            properties.append(prop)\n\n        data.properties = properties\n        return data\n\n# Use custom extractor\npipeline = (PipelineBuilder(\"Claude Pipeline\")\n    .extract_properties(ClaudeExtractor(model=\"claude-3-sonnet-20240229\"))\n    .cluster_properties(HDBSCANClusterer())\n    .build())\n</code></pre>"},{"location":"advanced/custom-pipelines/#custom-clusterers","title":"Custom Clusterers","text":"<p>Create custom clustering strategies:</p> <pre><code>from stringsight.clusterers.base import BaseClusterer\nfrom sklearn.cluster import DBSCAN\nimport numpy as np\n\nclass DBSCANClusterer(BaseClusterer):\n    \"\"\"Custom clusterer using DBSCAN algorithm.\"\"\"\n\n    def __init__(self, eps: float = 0.5, min_samples: int = 5, **kwargs):\n        super().__init__(**kwargs)\n        self.eps = eps\n        self.min_samples = min_samples\n\n    def run(self, data: PropertyDataset) -&gt; PropertyDataset:\n        \"\"\"Cluster properties using DBSCAN.\"\"\"\n        # Get embeddings\n        embeddings = self.generate_embeddings(data)\n\n        # Apply DBSCAN\n        clusterer = DBSCAN(eps=self.eps, min_samples=self.min_samples)\n        labels = clusterer.fit_predict(embeddings)\n\n        # Create clusters\n        data.clusters = self.create_clusters_from_labels(\n            data,\n            labels,\n            cluster_name_prefix=\"DBSCAN\"\n        )\n\n        return data\n\n# Use custom clusterer\npipeline = (PipelineBuilder(\"DBSCAN Pipeline\")\n    .extract_properties(OpenAIExtractor())\n    .cluster_properties(DBSCANClusterer(eps=0.3, min_samples=10))\n    .build())\n</code></pre>"},{"location":"advanced/custom-pipelines/#custom-metrics","title":"Custom Metrics","text":"<p>Implement custom metric calculations:</p> <pre><code>from stringsight.metrics.base import BaseMetrics\nfrom collections import defaultdict\n\nclass CustomMetrics(BaseMetrics):\n    \"\"\"Custom metrics calculator.\"\"\"\n\n    def run(self, data: PropertyDataset) -&gt; PropertyDataset:\n        \"\"\"Calculate custom metrics.\"\"\"\n        metrics = defaultdict(dict)\n\n        for cluster in data.clusters:\n            for model in data.all_models:\n                # Get properties for this model-cluster combination\n                model_props = [\n                    p for p in cluster.property_ids\n                    if data.get_property(p).model == model\n                ]\n\n                # Calculate custom metrics\n                metrics[model][cluster.label] = {\n                    \"count\": len(model_props),\n                    \"proportion\": len(model_props) / len(cluster.property_ids),\n                    \"custom_score\": self.calculate_custom_score(model_props)\n                }\n\n        data.model_stats = dict(metrics)\n        return data\n\n    def calculate_custom_score(self, properties):\n        \"\"\"Your custom scoring logic.\"\"\"\n        # Example: average property description length\n        return sum(len(p.property_description) for p in properties) / len(properties)\n\n# Use custom metrics\npipeline = (PipelineBuilder(\"Custom Metrics Pipeline\")\n    .extract_properties(OpenAIExtractor())\n    .cluster_properties(HDBSCANClusterer())\n    .compute_metrics(CustomMetrics())\n    .build())\n</code></pre>"},{"location":"advanced/custom-pipelines/#advanced-configurations","title":"Advanced Configurations","text":""},{"location":"advanced/custom-pipelines/#multi-stage-extraction","title":"Multi-Stage Extraction","text":"<p>Combine multiple extraction strategies:</p> <pre><code>class MultiStageExtractor(PipelineStage):\n    \"\"\"Run multiple extractors in sequence.\"\"\"\n\n    def __init__(self, extractors: List[PipelineStage]):\n        super().__init__()\n        self.extractors = extractors\n\n    def run(self, data: PropertyDataset) -&gt; PropertyDataset:\n        \"\"\"Run all extractors and combine results.\"\"\"\n        all_properties = []\n\n        for extractor in self.extractors:\n            # Run extractor\n            result = extractor.run(data)\n            all_properties.extend(result.properties)\n\n        data.properties = all_properties\n        return data\n\n# Use multi-stage extraction\npipeline = (PipelineBuilder(\"Multi-Stage Pipeline\")\n    .extract_properties(\n        MultiStageExtractor([\n            OpenAIExtractor(model=\"gpt-4.1\", temperature=0.3),\n            OpenAIExtractor(model=\"gpt-4o-mini\", temperature=0.7)\n        ])\n    )\n    .cluster_properties(HDBSCANClusterer())\n    .build())\n</code></pre>"},{"location":"advanced/custom-pipelines/#conditional-processing","title":"Conditional Processing","text":"<p>Add conditional logic to pipeline stages:</p> <pre><code>class ConditionalStage(PipelineStage):\n    \"\"\"Apply different processing based on conditions.\"\"\"\n\n    def __init__(self, condition_fn, true_stage, false_stage):\n        super().__init__()\n        self.condition_fn = condition_fn\n        self.true_stage = true_stage\n        self.false_stage = false_stage\n\n    def run(self, data: PropertyDataset) -&gt; PropertyDataset:\n        \"\"\"Run stage based on condition.\"\"\"\n        if self.condition_fn(data):\n            return self.true_stage.run(data)\n        else:\n            return self.false_stage.run(data)\n\n# Example: Use different clusterers based on dataset size\ndef is_large_dataset(data):\n    return len(data.conversations) &gt; 1000\n\npipeline = Pipeline(\"Conditional Pipeline\")\npipeline.add_stage(OpenAIExtractor())\npipeline.add_stage(\n    ConditionalStage(\n        condition_fn=is_large_dataset,\n        true_stage=HDBSCANClusterer(min_cluster_size=50),\n        false_stage=HDBSCANClusterer(min_cluster_size=10)\n    )\n)\n</code></pre>"},{"location":"advanced/custom-pipelines/#caching-checkpoints","title":"Caching &amp; Checkpoints","text":"<p>Enable caching to save intermediate results:</p> <pre><code># Built-in caching\nclustered_df, model_stats = explain(\n    df,\n    extraction_cache_dir=\".cache/extraction\",\n    clustering_cache_dir=\".cache/clustering\",\n    metrics_cache_dir=\".cache/metrics\"\n)\n\n# Manual checkpointing\ndataset = PropertyDataset.from_dataframe(df, method=\"single_model\")\n\n# Run extraction\nextractor = OpenAIExtractor(cache_dir=\".cache/extraction\")\ndataset = extractor.run(dataset)\n\n# Save checkpoint\ndataset.save(\"checkpoint_after_extraction.json\")\n\n# Later: Load checkpoint\ndataset = PropertyDataset.load(\"checkpoint_after_extraction.json\")\n\n# Continue pipeline\nclusterer = HDBSCANClusterer()\ndataset = clusterer.run(dataset)\n</code></pre>"},{"location":"advanced/custom-pipelines/#example-domain-specific-pipeline","title":"Example: Domain-Specific Pipeline","text":"<p>Build a pipeline for analyzing customer support conversations:</p> <pre><code>from stringsight import explain\nfrom stringsight.extractors import FixedAxesLabeler\n\n# Define customer support taxonomy\nSUPPORT_TAXONOMY = {\n    \"empathetic_response\": \"Response shows empathy and emotional intelligence\",\n    \"policy_adherence\": \"Response correctly follows company policies\",\n    \"problem_resolution\": \"Response effectively solves the customer's issue\",\n    \"clear_communication\": \"Response is easy to understand and well-structured\",\n    \"proactive_assistance\": \"Response anticipates and addresses related concerns\"\n}\n\n# Create pipeline\nfrom stringsight.pipeline import PipelineBuilder\n\npipeline = (PipelineBuilder(\"Customer Support Analysis\")\n    .extract_properties(\n        FixedAxesLabeler(\n            taxonomy=SUPPORT_TAXONOMY,\n            model=\"gpt-4.1\"\n        )\n    )\n    .configure(\n        use_wandb=True,\n        wandb_project=\"customer-support-analysis\"\n    )\n    .build())\n\n# Run analysis\nclustered_df, model_stats = explain(\n    support_df,\n    custom_pipeline=pipeline,\n    output_dir=\"results/customer_support\"\n)\n\n# Analyze results by taxonomy category\nfor category, details in model_stats.items():\n    print(f\"\\n{category}:\")\n    print(f\"  Coverage: {details['proportion']:.2%}\")\n    print(f\"  Quality: {details['quality']}\")\n</code></pre>"},{"location":"advanced/custom-pipelines/#testing-custom-stages","title":"Testing Custom Stages","text":"<p>Write tests for custom pipeline stages:</p> <pre><code>import pytest\nfrom stringsight.core.data_objects import PropertyDataset, ConversationRecord\n\ndef test_custom_extractor():\n    \"\"\"Test custom extractor.\"\"\"\n    # Create test data\n    conv = ConversationRecord(\n        question_id=\"test_1\",\n        prompt=\"Test prompt\",\n        model=\"gpt-4\",\n        responses=\"Test response\",\n        scores={},\n        meta={}\n    )\n    dataset = PropertyDataset(\n        conversations=[conv],\n        all_models=[\"gpt-4\"],\n        properties=[],\n        clusters=[],\n        model_stats={}\n    )\n\n    # Run extractor\n    extractor = ClaudeExtractor()\n    result = extractor.run(dataset)\n\n    # Assertions\n    assert len(result.properties) &gt; 0\n    assert result.properties[0].question_id == \"test_1\"\n    assert result.properties[0].property_description is not None\n\ndef test_custom_clusterer():\n    \"\"\"Test custom clusterer.\"\"\"\n    # Setup test data with properties\n    # ... create dataset with properties\n\n    # Run clusterer\n    clusterer = DBSCANClusterer(eps=0.3, min_samples=5)\n    result = clusterer.run(dataset)\n\n    # Assertions\n    assert len(result.clusters) &gt; 0\n    assert all(c.size &gt;= 5 for c in result.clusters)\n\n# Run tests\npytest.main([__file__, \"-v\"])\n</code></pre>"},{"location":"advanced/custom-pipelines/#best-practices","title":"Best Practices","text":"<ol> <li>Inherit from base classes - Use <code>PipelineStage</code>, <code>BaseClusterer</code>, <code>BaseMetrics</code></li> <li>Implement <code>run()</code> method - Main entry point for your stage</li> <li>Return PropertyDataset - Always return the dataset object (modified or unchanged)</li> <li>Add logging - Use <code>self.logger</code> for debugging</li> <li>Support caching - Implement cache_dir parameter for expensive operations</li> <li>Document parameters - Add docstrings explaining all configuration options</li> <li>Test thoroughly - Write unit tests for custom stages</li> <li>Handle errors gracefully - Add try/except with informative error messages</li> </ol>"},{"location":"advanced/custom-pipelines/#next-steps","title":"Next Steps","text":"<ul> <li>Performance Tuning - Optimize your custom pipelines</li> <li>API Reference - Detailed API documentation</li> <li>Contributing - Share your custom stages</li> </ul>"},{"location":"advanced/performance/","title":"Performance Tuning","text":"<p>Optimize StringSight for speed, cost, and quality based on your requirements.</p>"},{"location":"advanced/performance/#quick-wins","title":"Quick Wins","text":""},{"location":"advanced/performance/#use-cheaper-models","title":"Use Cheaper Models","text":"<pre><code>from stringsight import explain\n\n# Cost-effective configuration\nclustered_df, model_stats = explain(\n    df,\n    model_name=\"gpt-4o-mini\",              \n    embedding_model=\"all-MiniLM-L6-v2\",     # Free local model\n    min_cluster_size=15,                     # Smaller clusters = more clusters\n    use_wandb=False                          # Disable W&amp;B logging (default True)\n)\n</code></pre>"},{"location":"advanced/performance/#use-local-embeddings","title":"Use Local Embeddings","text":"<pre><code># Local sentence-transformers (free, no API calls)\nclustered_df, model_stats = explain(\n    df,\n    embedding_model=\"all-MiniLM-L6-v2\",  # Fast, good quality\n    # or\n    embedding_model=\"all-mpnet-base-v2\"   # Higher quality, slower\n)\n</code></pre>"},{"location":"advanced/performance/#sample-large-datasets","title":"Sample Large Datasets","text":"<pre><code># Analyze subset for initial exploration\nfrom stringsight.dataprep import sample_prompts_evenly\n\ndf_sample = sample_prompts_evenly(\n    df,\n    sample_size=1000,  # Sample 1000 prompts\n    method=\"single_model\",\n    random_state=42\n)\n\nclustered_df, model_stats = explain(df_sample)\n</code></pre>"},{"location":"advanced/performance/#model-selection-trade-offs","title":"Model Selection Trade-offs","text":"Model Cost (per 1M tokens) Speed Quality Best For <code>gpt-4.1</code> $3.50 input / $14.00 output Slow Excellent Production <code>gpt-4.1-mini</code> $0.70 / $2.80 Medium Very Good Balanced <code>gpt-4o-mini</code> $0.60 / $1.80 Fast Good Development <code>gpt-4.1-nano</code> $0.20 / $0.80 Very Fast Decent Large-scale"},{"location":"advanced/performance/#embedding-models","title":"Embedding Models","text":"Model Cost Speed Quality <code>text-embedding-3-large</code> $0.13/1M Medium Excellent <code>text-embedding-3-small</code> $0.02/1M Fast Very Good <code>all-MiniLM-L6-v2</code> Free Very Fast Good <code>all-mpnet-base-v2</code> Free Medium Very Good"},{"location":"advanced/performance/#clustering-optimization","title":"Clustering Optimization","text":""},{"location":"advanced/performance/#adjust-cluster-size","title":"Adjust Cluster Size","text":"<pre><code># Larger clusters = faster, fewer clusters\nclustered_df, model_stats = explain(\n    df,\n    min_cluster_size=50  # vs default 30\n)\n</code></pre>"},{"location":"advanced/performance/#disable-dimensionality-reduction","title":"Disable Dimensionality Reduction","text":"<pre><code>from stringsight.clusterers import HDBSCANClusterer\n\nclusterer = HDBSCANClusterer(\n    disable_dim_reduction=True,  # Skip UMAP/PCA\n    min_cluster_size=30\n)\n</code></pre>"},{"location":"advanced/performance/#parallelization","title":"Parallelization","text":""},{"location":"advanced/performance/#increase-workers","title":"Increase Workers","text":"<pre><code># More parallel API calls (if rate limits allow)\nclustered_df, model_stats = explain(\n    df,\n    max_workers=32  # vs default 16\n)\n</code></pre>"},{"location":"advanced/performance/#batch-processing","title":"Batch Processing","text":"<pre><code># Process large datasets in batches\nimport pandas as pd\n\nbatch_size = 1000\nresults = []\n\nfor i in range(0, len(df), batch_size):\n    batch = df[i:i+batch_size]\n    result, _ = explain(batch, output_dir=f\"results/batch_{i}\")\n    results.append(result)\n\n# Combine results\nfinal_df = pd.concat(results, ignore_index=True)\n</code></pre>"},{"location":"advanced/performance/#caching","title":"Caching","text":"<pre><code># Cache expensive operations\nclustered_df, model_stats = explain(\n    df,\n    extraction_cache_dir=\".cache/extraction\",\n    clustering_cache_dir=\".cache/clustering\",\n    metrics_cache_dir=\".cache/metrics\"\n)\n</code></pre>"},{"location":"advanced/performance/#memory-management","title":"Memory Management","text":""},{"location":"advanced/performance/#for-large-datasets","title":"For Large Datasets","text":"<pre><code># Reduce memory usage\nclustered_df, model_stats = explain(\n    df,\n    include_embeddings=False,  # Don't include embeddings in output\n    min_cluster_size=50,        # Fewer clusters\n    use_wandb=False             # Reduce logging overhead (default True)\n)\n</code></pre>"},{"location":"advanced/performance/#chunk-processing","title":"Chunk Processing","text":"<pre><code># Process in chunks to avoid OOM\nfor chunk in pd.read_csv(\"large_file.csv\", chunksize=5000):\n    result, _ = explain(chunk, output_dir=\"results/chunk\")\n</code></pre>"},{"location":"advanced/performance/#cost-estimation","title":"Cost Estimation","text":"<pre><code># Estimate costs before running\nnum_conversations = len(df)\navg_response_length = 500  # tokens\n\n# Extraction cost (input + output)\nextraction_cost = num_conversations * (\n    (avg_response_length / 1_000_000) * 3.50 +  # input\n    (200 / 1_000_000) * 14.00                     # output (estimated)\n)\n\n# Embedding cost\nnum_properties = num_conversations * 1.5  # estimate\nembedding_cost = (num_properties * 50 / 1_000_000) * 0.02\n\nprint(f\"Estimated cost: ${extraction_cost + embedding_cost:.2f}\")\n</code></pre>"},{"location":"advanced/performance/#benchmarks","title":"Benchmarks","text":"<p>Typical performance on common hardware:</p> Dataset Size GPT-4.1 GPT-4o-mini Local Embeddings Total Time 100 convs 2 min 1 min 10 sec ~3 min 1,000 convs 15 min 8 min 30 sec ~16 min 10,000 convs 2.5 hours 1.3 hours 5 min ~2.6 hours <p>Benchmarks on M1 Mac with 32GB RAM, 16 parallel workers</p>"},{"location":"advanced/performance/#next-steps","title":"Next Steps","text":"<ul> <li>Custom Pipelines - Build optimized pipelines</li> <li>Data Formats - Optimize data loading</li> </ul>"},{"location":"api/reference/","title":"API Reference","text":"<p>Complete API documentation for StringSight's main functions and classes.</p>"},{"location":"api/reference/#main-entry-points","title":"Main Entry Points","text":""},{"location":"api/reference/#explain","title":"explain()","text":""},{"location":"api/reference/#stringsight.public.explain","title":"<code>explain(df, method='single_model', system_prompt=None, prompt_builder=None, task_description=None, *, sample_size=None, model_a=None, model_b=None, score_columns=None, prompt_column='prompt', model_column=None, model_response_column=None, question_id_column=None, model_a_column=None, model_b_column=None, model_a_response_column=None, model_b_response_column=None, model_name='gpt-4.1', temperature=0.7, top_p=0.95, max_tokens=16000, max_workers=64, include_scores_in_prompt=False, clusterer='hdbscan', min_cluster_size=5, embedding_model='text-embedding-3-small', prettify_labels=False, assign_outliers=False, summary_model='gpt-4.1', cluster_assignment_model='gpt-4.1-mini', metrics_kwargs=None, use_wandb=True, wandb_project=None, include_embeddings=False, verbose=False, output_dir=None, custom_pipeline=None, extraction_cache_dir=None, clustering_cache_dir=None, metrics_cache_dir=None, **kwargs)</code>","text":"<p>Explain model behavior patterns from conversation data.</p> <p>This is the main entry point for StringSight. It takes a DataFrame of conversations and returns the same data with extracted properties and clusters.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame with conversation data</p> required <code>method</code> <code>str</code> <p>\"side_by_side\" or \"single_model\"</p> <code>'single_model'</code> <code>system_prompt</code> <code>str</code> <p>System prompt for property extraction (if None, will be auto-determined)</p> <code>None</code> <code>prompt_builder</code> <code>Optional[Callable[[Series], str]]</code> <p>Optional custom prompt builder function</p> <code>None</code> <code>task_description</code> <code>Optional[str]</code> <p>Optional description of the task; when provided with method=\"single_model\" and no explicit system_prompt, a task-aware system prompt is constructed from single_model_system_prompt_custom.</p> <code>None</code> <code>sample_size</code> <code>Optional[int]</code> <p>Optional number of rows to sample from the dataset before processing.         If None, uses the entire dataset. For single_model method with balanced         datasets (each prompt answered by all models), automatically samples prompts         evenly across models. Otherwise falls back to row-level sampling.</p> <code>None</code> <code>model_a</code> <code>Optional[str]</code> <p>For side_by_side method with tidy data, specifies first model to select</p> <code>None</code> <code>model_b</code> <code>Optional[str]</code> <p>For side_by_side method with tidy data, specifies second model to select</p> <code>None</code> <code>score_columns</code> <code>Optional[List[str]]</code> <p>Optional list of column names containing score metrics. Instead of         providing scores as a dictionary in a 'score' column, you can specify         separate columns for each metric. For single_model: columns should be         named like 'accuracy', 'helpfulness'. For side_by_side: columns should         be named like 'accuracy_a', 'accuracy_b', 'helpfulness_a', 'helpfulness_b'.         If provided, these columns will be converted to the expected score dict format.</p> <code>None</code> <code>prompt_column</code> <code>str</code> <p>Name of the prompt column in your dataframe (default: \"prompt\")</p> <code>'prompt'</code> <code>model_column</code> <code>Optional[str]</code> <p>Name of the model column for single_model (default: \"model\")</p> <code>None</code> <code>model_response_column</code> <code>Optional[str]</code> <p>Name of the model response column for single_model (default: \"model_response\")</p> <code>None</code> <code>question_id_column</code> <code>Optional[str]</code> <p>Name of the question_id column (default: \"question_id\" if column exists)</p> <code>None</code> <code>model_a_column</code> <code>Optional[str]</code> <p>Name of the model_a column for side_by_side (default: \"model_a\")</p> <code>None</code> <code>model_b_column</code> <code>Optional[str]</code> <p>Name of the model_b column for side_by_side (default: \"model_b\")</p> <code>None</code> <code>model_a_response_column</code> <code>Optional[str]</code> <p>Name of the model_a_response column for side_by_side (default: \"model_a_response\")</p> <code>None</code> <code>model_b_response_column</code> <code>Optional[str]</code> <p>Name of the model_b_response column for side_by_side (default: \"model_b_response\")</p> <code>None</code> <code>model_name</code> <code>str</code> <p>LLM model for property extraction</p> <code>'gpt-4.1'</code> <code>temperature</code> <code>float</code> <p>Temperature for LLM</p> <code>0.7</code> <code>top_p</code> <code>float</code> <p>Top-p for LLM</p> <code>0.95</code> <code>max_tokens</code> <code>int</code> <p>Max tokens for LLM</p> <code>16000</code> <code>max_workers</code> <code>int</code> <p>Max parallel workers for API calls</p> <code>64</code> <code>clusterer</code> <code>Union[str, PipelineStage]</code> <p>Clustering method (\"hdbscan\", \"hdbscan_native\") or PipelineStage</p> <code>'hdbscan'</code> <code>min_cluster_size</code> <code>int | None</code> <p>Minimum cluster size</p> <code>5</code> <code>embedding_model</code> <code>str</code> <p>Embedding model (\"openai\" or sentence-transformer model)</p> <code>'text-embedding-3-small'</code> <code>assign_outliers</code> <code>bool</code> <p>Whether to assign outliers to nearest clusters</p> <code>False</code> <code>summary_model</code> <code>str</code> <p>LLM model for generating cluster summaries (default: \"gpt-4.1\")</p> <code>'gpt-4.1'</code> <code>cluster_assignment_model</code> <code>str</code> <p>LLM model for assigning outliers to clusters (default: \"gpt-4.1-mini\")</p> <code>'gpt-4.1-mini'</code> <code>metrics_kwargs</code> <code>Optional[Dict[str, Any]]</code> <p>Additional metrics configuration</p> <code>None</code> <code>use_wandb</code> <code>bool</code> <p>Whether to log to Weights &amp; Biases</p> <code>True</code> <code>wandb_project</code> <code>Optional[str]</code> <p>W&amp;B project name</p> <code>None</code> <code>include_embeddings</code> <code>bool</code> <p>Whether to include embeddings in output</p> <code>False</code> <code>verbose</code> <code>bool</code> <p>Whether to print progress</p> <code>False</code> <code>output_dir</code> <code>Optional[str]</code> <p>Directory to save results (optional). If provided, saves:        - clustered_results.parquet: DataFrame with all results        - full_dataset.json: Complete PropertyDataset (JSON format)        - full_dataset.parquet: Complete PropertyDataset (parquet format)        - model_stats.json: Model statistics and rankings        - summary.txt: Human-readable summary</p> <code>None</code> <code>custom_pipeline</code> <code>Optional[Pipeline]</code> <p>Custom pipeline to use instead of default</p> <code>None</code> <code>**kwargs</code> <p>Additional configuration options</p> <code>{}</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Tuple of (clustered_df, model_stats)</p> <code>Dict[str, DataFrame]</code> <ul> <li>clustered_df: Original DataFrame with added property and cluster columns</li> </ul> <code>Tuple[DataFrame, Dict[str, DataFrame]]</code> <ul> <li>model_stats: Dictionary containing three DataFrames:</li> <li>\"model_cluster_scores\": Per model-cluster metrics (size, proportion, quality, etc.)</li> <li>\"cluster_scores\": Per cluster aggregated metrics (across all models)</li> <li>\"model_scores\": Per model aggregated metrics (across all clusters)</li> </ul> Notes on input format <ul> <li>For method=\"single_model\": expect columns [question_id, prompt, model, model_response, (optional) score]</li> <li>For method=\"side_by_side\": expect columns [question_id, prompt, model_a, model_b, model_a_response, model_b_response]</li> <li>Alternatively, for method=\"side_by_side\" you may pass tidy single-model-like data   (columns [prompt, model, model_response] and optionally question_id) and specify   <code>model_a</code> and <code>model_b</code> parameters. The function will select these two   models and convert the input to the expected side-by-side schema.</li> </ul> Example <p>import pandas as pd from stringsight import explain</p> Source code in <code>stringsight/public.py</code> <pre><code>def explain(\n    df: pd.DataFrame,\n    method: str = \"single_model\",\n    system_prompt: str = None,\n    prompt_builder: Optional[Callable[[pd.Series], str]] = None,\n    task_description: Optional[str] = None,\n    *,\n    # Data preparation\n    sample_size: Optional[int] = None,\n    model_a: Optional[str] = None,\n    model_b: Optional[str] = None,\n    score_columns: Optional[List[str]] = None,\n    # Column mapping parameters\n    prompt_column: str = \"prompt\",\n    model_column: Optional[str] = None,\n    model_response_column: Optional[str] = None,\n    question_id_column: Optional[str] = None,\n    model_a_column: Optional[str] = None,\n    model_b_column: Optional[str] = None,\n    model_a_response_column: Optional[str] = None,\n    model_b_response_column: Optional[str] = None,\n    # Extraction parameters\n    model_name: str = \"gpt-4.1\",\n    temperature: float = 0.7,\n    top_p: float = 0.95,\n    max_tokens: int = 16000,\n    max_workers: int = 64,\n    include_scores_in_prompt: bool = False,\n    # Clustering parameters  \n    clusterer: Union[str, \"PipelineStage\"] = \"hdbscan\",\n    min_cluster_size: int | None = 5,\n    embedding_model: str = \"text-embedding-3-small\",\n    prettify_labels: bool = False,\n    assign_outliers: bool = False,\n    summary_model: str = \"gpt-4.1\",\n    cluster_assignment_model: str = \"gpt-4.1-mini\",\n    # Metrics parameters\n    metrics_kwargs: Optional[Dict[str, Any]] = None,\n    # Caching &amp; logging\n    use_wandb: bool = True,\n    wandb_project: Optional[str] = None,\n    include_embeddings: bool = False,\n    verbose: bool = False,\n    # Output parameters\n    output_dir: Optional[str] = None,\n    # Pipeline configuration\n    custom_pipeline: Optional[Pipeline] = None,\n    # Cache configuration\n    extraction_cache_dir: Optional[str] = None,\n    clustering_cache_dir: Optional[str] = None,\n    metrics_cache_dir: Optional[str] = None,\n    **kwargs\n) -&gt; Tuple[pd.DataFrame, Dict[str, pd.DataFrame]]:\n    \"\"\"\n    Explain model behavior patterns from conversation data.\n\n    This is the main entry point for StringSight. It takes a DataFrame of conversations\n    and returns the same data with extracted properties and clusters.\n\n    Args:\n        df: DataFrame with conversation data\n        method: \"side_by_side\" or \"single_model\"\n        system_prompt: System prompt for property extraction (if None, will be auto-determined)\n        prompt_builder: Optional custom prompt builder function\n        task_description: Optional description of the task; when provided with\n            method=\"single_model\" and no explicit system_prompt, a task-aware\n            system prompt is constructed from single_model_system_prompt_custom.\n\n        # Data preparation\n        sample_size: Optional number of rows to sample from the dataset before processing.\n                    If None, uses the entire dataset. For single_model method with balanced\n                    datasets (each prompt answered by all models), automatically samples prompts\n                    evenly across models. Otherwise falls back to row-level sampling.\n        model_a: For side_by_side method with tidy data, specifies first model to select\n        model_b: For side_by_side method with tidy data, specifies second model to select\n        score_columns: Optional list of column names containing score metrics. Instead of\n                    providing scores as a dictionary in a 'score' column, you can specify\n                    separate columns for each metric. For single_model: columns should be\n                    named like 'accuracy', 'helpfulness'. For side_by_side: columns should\n                    be named like 'accuracy_a', 'accuracy_b', 'helpfulness_a', 'helpfulness_b'.\n                    If provided, these columns will be converted to the expected score dict format.\n\n        # Column mapping parameters\n        prompt_column: Name of the prompt column in your dataframe (default: \"prompt\")\n        model_column: Name of the model column for single_model (default: \"model\")\n        model_response_column: Name of the model response column for single_model (default: \"model_response\")\n        question_id_column: Name of the question_id column (default: \"question_id\" if column exists)\n        model_a_column: Name of the model_a column for side_by_side (default: \"model_a\")\n        model_b_column: Name of the model_b column for side_by_side (default: \"model_b\")\n        model_a_response_column: Name of the model_a_response column for side_by_side (default: \"model_a_response\")\n        model_b_response_column: Name of the model_b_response column for side_by_side (default: \"model_b_response\")\n\n        # Extraction parameters\n        model_name: LLM model for property extraction\n        temperature: Temperature for LLM\n        top_p: Top-p for LLM\n        max_tokens: Max tokens for LLM\n        max_workers: Max parallel workers for API calls\n\n        # Clustering parameters\n        clusterer: Clustering method (\"hdbscan\", \"hdbscan_native\") or PipelineStage\n        min_cluster_size: Minimum cluster size\n        embedding_model: Embedding model (\"openai\" or sentence-transformer model)\n        assign_outliers: Whether to assign outliers to nearest clusters\n        summary_model: LLM model for generating cluster summaries (default: \"gpt-4.1\")\n        cluster_assignment_model: LLM model for assigning outliers to clusters (default: \"gpt-4.1-mini\")\n\n        # Metrics parameters\n        metrics_kwargs: Additional metrics configuration\n\n        # Caching &amp; logging\n        use_wandb: Whether to log to Weights &amp; Biases\n        wandb_project: W&amp;B project name\n        include_embeddings: Whether to include embeddings in output\n        verbose: Whether to print progress\n\n        # Output parameters\n        output_dir: Directory to save results (optional). If provided, saves:\n                   - clustered_results.parquet: DataFrame with all results\n                   - full_dataset.json: Complete PropertyDataset (JSON format)\n                   - full_dataset.parquet: Complete PropertyDataset (parquet format)\n                   - model_stats.json: Model statistics and rankings\n                   - summary.txt: Human-readable summary\n\n        # Pipeline configuration\n        custom_pipeline: Custom pipeline to use instead of default\n        **kwargs: Additional configuration options\n\n    Returns:\n        Tuple of (clustered_df, model_stats)\n        - clustered_df: Original DataFrame with added property and cluster columns\n        - model_stats: Dictionary containing three DataFrames:\n            - \"model_cluster_scores\": Per model-cluster metrics (size, proportion, quality, etc.)\n            - \"cluster_scores\": Per cluster aggregated metrics (across all models)\n            - \"model_scores\": Per model aggregated metrics (across all clusters)\n\n    Notes on input format:\n        - For method=\"single_model\": expect columns [question_id, prompt, model, model_response, (optional) score]\n        - For method=\"side_by_side\": expect columns [question_id, prompt, model_a, model_b, model_a_response, model_b_response]\n        - Alternatively, for method=\"side_by_side\" you may pass tidy single-model-like data\n          (columns [prompt, model, model_response] and optionally question_id) and specify\n          `model_a` and `model_b` parameters. The function will select these two\n          models and convert the input to the expected side-by-side schema.\n\n    Example:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from stringsight import explain\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; # Load your conversation data\n        &gt;&gt;&gt; df = pd.read_csv(\"conversations.csv\")\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; # Explain model behavior and save results\n        &gt;&gt;&gt; clustered_df, model_stats = explain(\n        ...     df,\n        ...     method=\"side_by_side\",\n        ...     min_cluster_size=5,\n        ...     output_dir=\"results/\"  # Automatically saves results\n        ... )\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; # Explore the results\n        &gt;&gt;&gt; print(clustered_df.columns)\n        &gt;&gt;&gt; print(model_stats.keys())\n    \"\"\"\n\n    # Validate OpenAI API key is set if using GPT models\n    validate_openai_api_key(\n        model_name=model_name,\n        embedding_model=embedding_model,\n        **kwargs\n    )\n\n    # Preprocess data: handle score_columns, sampling, tidy\u2192side_by_side conversion, column mapping\n    from .core.preprocessing import validate_and_prepare_dataframe\n    df = validate_and_prepare_dataframe(\n        df,\n        method=method,\n        score_columns=score_columns,\n        sample_size=sample_size,\n        model_a=model_a,\n        model_b=model_b,\n        prompt_column=prompt_column,\n        model_column=model_column,\n        model_response_column=model_response_column,\n        question_id_column=question_id_column,\n        model_a_column=model_a_column,\n        model_b_column=model_b_column,\n        model_a_response_column=model_a_response_column,\n        model_b_response_column=model_b_response_column,\n        verbose=verbose,\n    )\n\n    # Auto-determine/resolve system prompt with the centralized helper\n    system_prompt = get_system_prompt(method, system_prompt, task_description)\n\n    # Print the system prompt for verification\n    if verbose:\n        logger.info(\"\\n\" + \"=\"*80)\n        logger.info(\"SYSTEM PROMPT\")\n        logger.info(\"=\"*80)\n        logger.info(system_prompt)\n        logger.info(\"=\"*80 + \"\\n\")\n    if len(system_prompt) &lt; 50:\n        raise ValueError(\"System prompt is too short. Please provide a longer system prompt.\")\n\n    print(f\"df length: {len(df)}\")\n\n    # Create PropertyDataset from input DataFrame\n    dataset = PropertyDataset.from_dataframe(df, method=method)\n\n    # Print initial dataset information\n    if verbose:\n        logger.info(f\"\\n\ud83d\udccb Initial dataset summary:\")\n        logger.info(f\"   \u2022 Conversations: {len(dataset.conversations)}\")\n        logger.info(f\"   \u2022 Models: {len(dataset.all_models)}\")\n        if len(dataset.all_models) &lt;= 20:\n            logger.info(f\"   \u2022 Model names: {', '.join(sorted(dataset.all_models))}\")\n        logger.info(\"\")\n\n    # 2\ufe0f\u20e3  Initialize wandb if enabled (and explicitly disable via env when off)\n    # Ensure environment flag aligns with the provided setting to prevent\n    # accidental logging by submodules that import wandb directly.\n    import os as _os\n    if not use_wandb:\n        _os.environ[\"WANDB_DISABLED\"] = \"true\"\n    else:\n        _os.environ.pop(\"WANDB_DISABLED\", None)\n\n    # 2\ufe0f\u20e3  Initialize wandb if enabled\n    # Create run name based on input filename if available\n    if use_wandb:\n        try:\n            import wandb\n            # import weave\n            import os\n\n            # Try to get input filename from the DataFrame or use a default\n            input_filename = \"unknown_dataset\"\n            if hasattr(df, 'name') and df.name:\n                input_filename = df.name\n            elif hasattr(df, '_metadata') and df._metadata and 'filename' in df._metadata:\n                input_filename = df._metadata['filename']\n            else:\n                # Try to infer from the DataFrame source if it has a path attribute\n                # This is a fallback for when we can't determine the filename\n                input_filename = f\"dataset_{len(df)}_rows\"\n\n            # Clean the filename for wandb (remove extension, replace spaces/special chars)\n            if isinstance(input_filename, str):\n                # Remove file extension and clean up the name\n                input_filename = os.path.splitext(os.path.basename(input_filename))[0]\n                # Replace spaces and special characters with underscores\n                input_filename = input_filename.replace(' ', '_').replace('-', '_')\n                # Remove any remaining special characters\n                import re\n                input_filename = re.sub(r'[^a-zA-Z0-9_]', '', input_filename)\n\n            wandb_run_name = os.path.basename(os.path.normpath(output_dir)) if output_dir else f\"{input_filename}_{method}\"\n\n            wandb.init(\n                project=wandb_project or \"StringSight\",\n                name=wandb_run_name,\n                config={\n                    \"method\": method,\n                    \"system_prompt\": system_prompt,\n                    \"model_name\": model_name,\n                    \"temperature\": temperature,\n                    \"top_p\": top_p,\n                    \"max_tokens\": max_tokens,\n                    \"max_workers\": max_workers,\n                    \"clusterer\": clusterer,\n                    \"min_cluster_size\": min_cluster_size,\n                    \"embedding_model\": embedding_model,\n                    \"assign_outliers\": assign_outliers,\n                    \"include_embeddings\": include_embeddings,\n                    \"output_dir\": output_dir,\n                },\n                reinit=False  # Don't reinitialize if already exists\n            )\n        except ImportError:\n            # wandb not installed or not available\n            use_wandb = False\n\n    # Use custom pipeline if provided, otherwise build default pipeline\n    if custom_pipeline is not None:\n        pipeline = custom_pipeline\n        # Ensure the custom pipeline uses the same wandb configuration\n        if hasattr(pipeline, 'use_wandb'):\n            pipeline.use_wandb = use_wandb\n            pipeline.wandb_project = wandb_project or \"StringSight\"\n            if use_wandb:\n                pipeline._wandb_ok = True  # Mark that wandb is already initialized\n    else:\n        pipeline = _build_default_pipeline(\n            method=method,\n            system_prompt=system_prompt,\n            prompt_builder=prompt_builder,\n            model_name=model_name,\n            temperature=temperature,\n            top_p=top_p,\n            max_tokens=max_tokens,\n            max_workers=max_workers,\n            include_scores_in_prompt=include_scores_in_prompt,\n            clusterer=clusterer,\n            min_cluster_size=min_cluster_size,\n            embedding_model=embedding_model,\n            assign_outliers=assign_outliers,\n            prettify_labels=prettify_labels,\n            summary_model=summary_model,\n            cluster_assignment_model=cluster_assignment_model,\n            metrics_kwargs=metrics_kwargs,\n            use_wandb=use_wandb,\n            wandb_project=wandb_project,\n            include_embeddings=include_embeddings,\n            verbose=verbose,\n            extraction_cache_dir=extraction_cache_dir,\n            clustering_cache_dir=clustering_cache_dir,\n            metrics_cache_dir=metrics_cache_dir,\n            output_dir=output_dir,\n            **kwargs\n        )\n\n    # 4\ufe0f\u20e3  Execute pipeline\n    result_dataset = pipeline.run(dataset)\n\n       # Check for 0 properties before attempting to save\n    if len([p for p in result_dataset.properties if p.property_description is not None]) == 0:\n        raise RuntimeError(\n            \"\\n\" + \"=\"*60 + \"\\n\"\n            \"ERROR: Pipeline completed with 0 valid properties!\\n\"\n            \"=\"*60 + \"\\n\"\n            \"This indicates that all property extraction attempts failed.\\n\"\n            \"Common causes:\\n\\n\"\n            \"1. JSON PARSING FAILURES:\\n\"\n            \"   - LLM returning natural language instead of JSON\\n\"\n            \"   - Check logs above for 'Failed to parse JSON' errors\\n\\n\"\n            \"2. SYSTEM PROMPT MISMATCH:\\n\"\n            \"   - Current system_prompt may not suit your data format\\n\"\n            \"   - Try a different system_prompt parameter\\n\\n\"\n            \"3. API/MODEL ISSUES:\\n\"\n            \"   - OpenAI API key invalid or quota exceeded\\n\"\n            \"   - Model configuration problems\\n\\n\"\n            \"Cannot save results with 0 properties.\\n\"\n            \"=\"*60\n        )\n\n    # Convert back to DataFrame format\n    clustered_df = result_dataset.to_dataframe(type=\"all\", method=method)\n    model_stats = result_dataset.model_stats\n\n    # Save final summary if output_dir is provided\n    if output_dir is not None:\n        _save_final_summary(result_dataset, clustered_df, model_stats, output_dir, verbose)\n\n        # Also save the full dataset for backward compatibility with compute_metrics_only and other tools\n        import pathlib\n        import json\n\n        output_path = pathlib.Path(output_dir)\n\n        # Save full dataset as JSON\n        full_dataset_json_path = output_path / \"full_dataset.json\"\n        result_dataset.save(str(full_dataset_json_path))\n        if verbose:\n            logger.info(f\"  \u2713 Saved full dataset: {full_dataset_json_path}\")\n\n    # Log accumulated summary metrics from pipeline stages\n    if use_wandb and hasattr(pipeline, 'log_final_summary'):\n        pipeline.log_final_summary()\n\n    # Log final results to wandb if enabled\n    if use_wandb:\n        try:\n            import wandb\n            # import weave\n            _log_final_results_to_wandb(clustered_df, model_stats)\n        except ImportError:\n            # wandb not installed or not available\n            use_wandb = False\n\n    # Print analysis summary if verbose\n    _print_analysis_summary(model_stats, max_behaviors=5)\n\n    return clustered_df, model_stats\n</code></pre>"},{"location":"api/reference/#stringsight.public.explain--load-your-conversation-data","title":"Load your conversation data","text":"<p>df = pd.read_csv(\"conversations.csv\")</p>"},{"location":"api/reference/#stringsight.public.explain--explain-model-behavior-and-save-results","title":"Explain model behavior and save results","text":"<p>clustered_df, model_stats = explain( ...     df, ...     method=\"side_by_side\", ...     min_cluster_size=5, ...     output_dir=\"results/\"  # Automatically saves results ... )</p>"},{"location":"api/reference/#stringsight.public.explain--explore-the-results","title":"Explore the results","text":"<p>print(clustered_df.columns) print(model_stats.keys())</p>"},{"location":"api/reference/#label","title":"label()","text":""},{"location":"api/reference/#stringsight.public.label","title":"<code>label(df, *, taxonomy, sample_size=None, score_columns=None, prompt_column='prompt', model_column=None, model_response_column=None, question_id_column=None, model_name='gpt-4.1', temperature=0.0, top_p=1.0, max_tokens=2048, max_workers=64, metrics_kwargs=None, use_wandb=True, wandb_project=None, include_embeddings=False, verbose=False, output_dir=None, extraction_cache_dir=None, metrics_cache_dir=None, **kwargs)</code>","text":"<p>Run the fixed-taxonomy analysis pipeline. This is just you're run of the mill LLM-judge with a given rubric. </p> <p>The user provides a dataframe with a model and its responses alone with a taxonomy.</p> <p>Unlike :pyfunc:<code>explain</code>, this entry point does not perform clustering; each taxonomy label simply becomes its own cluster.  The input <code>df</code> must be in single-model format (columns <code>question_id</code>, <code>prompt</code>, <code>model</code>, <code>model_response</code>, \u2026).</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame with single-model conversation data</p> required <code>taxonomy</code> <code>Dict[str, str]</code> <p>Dictionary mapping label names to their descriptions</p> required <code>sample_size</code> <code>Optional[int]</code> <p>Optional number of rows to sample from the dataset before processing.         If None, uses the entire dataset. For balanced datasets (each prompt answered         by all models), automatically samples prompts evenly across models.</p> <code>None</code> <code>score_columns</code> <code>Optional[List[str]]</code> <p>Optional list of column names containing score metrics. Instead of         providing scores as a dictionary in a 'score' column, you can specify         separate columns for each metric (e.g., ['accuracy', 'helpfulness']).         If provided, these columns will be converted to the expected score dict format.</p> <code>None</code> <code>prompt_column</code> <code>str</code> <p>Name of the prompt column in your dataframe (default: \"prompt\")</p> <code>'prompt'</code> <code>model_column</code> <code>Optional[str]</code> <p>Name of the model column (default: \"model\")</p> <code>None</code> <code>model_response_column</code> <code>Optional[str]</code> <p>Name of the model response column (default: \"model_response\")</p> <code>None</code> <code>question_id_column</code> <code>Optional[str]</code> <p>Name of the question_id column (default: \"question_id\" if column exists)</p> <code>None</code> <code>model_name</code> <code>str</code> <p>LLM model for property extraction (default: \"gpt-4.1\")</p> <code>'gpt-4.1'</code> <code>temperature</code> <code>float</code> <p>Temperature for LLM (default: 0.0)</p> <code>0.0</code> <code>top_p</code> <code>float</code> <p>Top-p for LLM (default: 1.0)</p> <code>1.0</code> <code>max_tokens</code> <code>int</code> <p>Max tokens for LLM (default: 2048)</p> <code>2048</code> <code>max_workers</code> <code>int</code> <p>Max parallel workers for API calls (default: 8)</p> <code>64</code> <code>metrics_kwargs</code> <code>Optional[Dict[str, Any]]</code> <p>Additional metrics configuration</p> <code>None</code> <code>use_wandb</code> <code>bool</code> <p>Whether to log to Weights &amp; Biases (default: True)</p> <code>True</code> <code>wandb_project</code> <code>Optional[str]</code> <p>W&amp;B project name</p> <code>None</code> <code>include_embeddings</code> <code>bool</code> <p>Whether to include embeddings in output (default: True)</p> <code>False</code> <code>verbose</code> <code>bool</code> <p>Whether to print progress (default: True)</p> <code>False</code> <code>output_dir</code> <code>Optional[str]</code> <p>Directory to save results (optional)</p> <code>None</code> <code>extraction_cache_dir</code> <code>Optional[str]</code> <p>Cache directory for extraction results</p> <code>None</code> <code>metrics_cache_dir</code> <code>Optional[str]</code> <p>Cache directory for metrics results</p> <code>None</code> <code>**kwargs</code> <p>Additional configuration options</p> <code>{}</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Tuple of (clustered_df, model_stats)</p> <code>Dict[str, DataFrame]</code> <ul> <li>clustered_df: Original DataFrame with added property and cluster columns</li> </ul> <code>Tuple[DataFrame, Dict[str, DataFrame]]</code> <ul> <li>model_stats: Dictionary containing three DataFrames:</li> <li>\"model_cluster_scores\": Per model-cluster metrics (size, proportion, quality, etc.)</li> <li>\"cluster_scores\": Per cluster aggregated metrics (across all models)</li> <li>\"model_scores\": Per model aggregated metrics (across all clusters)</li> </ul> Source code in <code>stringsight/public.py</code> <pre><code>def label(\n    df: pd.DataFrame,\n    *,\n    taxonomy: Dict[str, str],\n    sample_size: Optional[int] = None,\n    # Column mapping parameters\n    score_columns: Optional[List[str]] = None,\n    prompt_column: str = \"prompt\",\n    model_column: Optional[str] = None,\n    model_response_column: Optional[str] = None,\n    question_id_column: Optional[str] = None,\n    model_name: str = \"gpt-4.1\",\n    temperature: float = 0.0,\n    top_p: float = 1.0,\n    max_tokens: int = 2048,\n    max_workers: int = 64,\n    metrics_kwargs: Optional[Dict[str, Any]] = None,\n    use_wandb: bool = True,\n    wandb_project: Optional[str] = None,\n    include_embeddings: bool = False,\n    verbose: bool = False,\n    output_dir: Optional[str] = None,\n    extraction_cache_dir: Optional[str] = None,\n    metrics_cache_dir: Optional[str] = None,\n    **kwargs,\n) -&gt; Tuple[pd.DataFrame, Dict[str, pd.DataFrame]]:\n    \"\"\"Run the *fixed-taxonomy* analysis pipeline. This is just you're run of the mill LLM-judge with a given rubric. \n\n    The user provides a dataframe with a model and its responses alone with a taxonomy.\n\n    Unlike :pyfunc:`explain`, this entry point does **not** perform clustering;\n    each taxonomy label simply becomes its own cluster.  The input `df` **must**\n    be in *single-model* format (columns `question_id`, `prompt`, `model`, `model_response`, \u2026).\n\n    Args:\n        df: DataFrame with single-model conversation data\n        taxonomy: Dictionary mapping label names to their descriptions\n        sample_size: Optional number of rows to sample from the dataset before processing.\n                    If None, uses the entire dataset. For balanced datasets (each prompt answered\n                    by all models), automatically samples prompts evenly across models.\n        score_columns: Optional list of column names containing score metrics. Instead of\n                    providing scores as a dictionary in a 'score' column, you can specify\n                    separate columns for each metric (e.g., ['accuracy', 'helpfulness']).\n                    If provided, these columns will be converted to the expected score dict format.\n        prompt_column: Name of the prompt column in your dataframe (default: \"prompt\")\n        model_column: Name of the model column (default: \"model\")\n        model_response_column: Name of the model response column (default: \"model_response\")\n        question_id_column: Name of the question_id column (default: \"question_id\" if column exists)\n        model_name: LLM model for property extraction (default: \"gpt-4.1\")\n        temperature: Temperature for LLM (default: 0.0)\n        top_p: Top-p for LLM (default: 1.0)\n        max_tokens: Max tokens for LLM (default: 2048)\n        max_workers: Max parallel workers for API calls (default: 8)\n        metrics_kwargs: Additional metrics configuration\n        use_wandb: Whether to log to Weights &amp; Biases (default: True)\n        wandb_project: W&amp;B project name\n        include_embeddings: Whether to include embeddings in output (default: True)\n        verbose: Whether to print progress (default: True)\n        output_dir: Directory to save results (optional)\n        extraction_cache_dir: Cache directory for extraction results\n        metrics_cache_dir: Cache directory for metrics results\n        **kwargs: Additional configuration options\n\n    Returns:\n        Tuple of (clustered_df, model_stats)\n        - clustered_df: Original DataFrame with added property and cluster columns\n        - model_stats: Dictionary containing three DataFrames:\n            - \"model_cluster_scores\": Per model-cluster metrics (size, proportion, quality, etc.)\n            - \"cluster_scores\": Per cluster aggregated metrics (across all models)\n            - \"model_scores\": Per model aggregated metrics (across all clusters)\n    \"\"\"\n\n    method = \"single_model\"  # hard-coded, we only support single-model here\n\n    # Align environment with wandb toggle early to avoid accidental logging on import\n    import os as _os\n    if not use_wandb:\n        _os.environ[\"WANDB_DISABLED\"] = \"true\"\n    else:\n        _os.environ.pop(\"WANDB_DISABLED\", None)\n    if \"model_b\" in df.columns:\n        raise ValueError(\"label() currently supports only single-model data.  Use explain() for side-by-side analyses.\")\n\n    # Preprocess data: handle score_columns, sampling, and column mapping\n    # For label() mode, use row-level sampling to get exact sample_size\n    from .core.preprocessing import validate_and_prepare_dataframe\n    df = validate_and_prepare_dataframe(\n        df,\n        method=method,\n        score_columns=score_columns,\n        sample_size=sample_size,\n        prompt_column=prompt_column,\n        model_column=model_column,\n        model_response_column=model_response_column,\n        question_id_column=question_id_column,\n        verbose=verbose,\n        use_row_sampling=True,  # Use row-level sampling for label() to get exact count\n    )\n\n    # ------------------------------------------------------------------\n    # Create extractor first to get the system prompt\n    # ------------------------------------------------------------------\n    from .extractors.fixed_axes_labeler import FixedAxesLabeler\n\n    # Create the extractor to generate the system prompt from taxonomy\n    extractor = FixedAxesLabeler(\n        taxonomy=taxonomy,\n        model=model_name,\n        temperature=temperature,\n        top_p=top_p,\n        max_tokens=max_tokens,\n        max_workers=max_workers,\n        cache_dir=extraction_cache_dir or \".cache/stringsight\",\n        output_dir=output_dir,\n        verbose=verbose,\n        use_wandb=use_wandb,\n        wandb_project=wandb_project or \"StringSight\"\n    )\n\n    # Print the system prompt for verification\n    if verbose:\n        logger.info(\"\\n\" + \"=\"*80)\n        logger.info(\"SYSTEM PROMPT\")\n        logger.info(\"=\"*80)\n        logger.info(extractor.system_prompt)\n        logger.info(\"=\"*80 + \"\\n\")\n\n    # ------------------------------------------------------------------\n    # Build dataset &amp; pipeline\n    # ------------------------------------------------------------------\n    dataset = PropertyDataset.from_dataframe(df, method=method)\n\n    # Initialize wandb if enabled\n    if use_wandb:\n        try:\n            import wandb\n            # import weave\n            import os\n\n            # Try to get input filename from the DataFrame or use a default\n            input_filename = \"unknown_dataset\"\n            if hasattr(df, 'name') and df.name:\n                input_filename = df.name\n            elif hasattr(df, '_metadata') and df._metadata and 'filename' in df._metadata:\n                input_filename = df._metadata['filename']\n            else:\n                # Try to infer from the DataFrame source if it has a path attribute\n                # This is a fallback for when we can't determine the filename\n                input_filename = f\"dataset_{len(df)}_rows\"\n\n            # Clean the filename for wandb (remove extension, replace spaces/special chars)\n            if isinstance(input_filename, str):\n                # Remove file extension and clean up the name\n                input_filename = os.path.splitext(os.path.basename(input_filename))[0]\n                # Replace spaces and special characters with underscores\n                input_filename = input_filename.replace(' ', '_').replace('-', '_')\n                # Remove any remaining special characters\n                import re\n                input_filename = re.sub(r'[^a-zA-Z0-9_]', '', input_filename)\n\n            wandb_run_name = os.path.basename(os.path.normpath(output_dir)) if output_dir else f\"{input_filename}_label\"\n\n            wandb.init(\n                project=wandb_project or \"StringSight\",\n                name=wandb_run_name,\n                config={\n                    \"method\": method,\n                    \"model_name\": model_name,\n                    \"temperature\": temperature,\n                    \"top_p\": top_p,\n                    \"max_tokens\": max_tokens,\n                    \"max_workers\": max_workers,\n                    \"taxonomy_size\": len(taxonomy),\n                    \"include_embeddings\": include_embeddings,\n                    \"output_dir\": output_dir,\n                },\n                reinit=False  # Don't reinitialize if already exists\n            )\n        except ImportError:\n            # wandb not installed or not available\n            use_wandb = False\n\n    pipeline = _build_fixed_axes_pipeline(\n        extractor=extractor,\n        taxonomy=taxonomy,\n        model_name=model_name,\n        temperature=temperature,\n        top_p=top_p,\n        max_tokens=max_tokens,\n        max_workers=max_workers,\n        metrics_kwargs=metrics_kwargs,\n        use_wandb=use_wandb,\n        wandb_project=wandb_project,\n        include_embeddings=include_embeddings,\n        verbose=verbose,\n        output_dir=output_dir,\n        extraction_cache_dir=extraction_cache_dir,\n        metrics_cache_dir=metrics_cache_dir,\n        **kwargs,\n    )\n\n    # ------------------------------------------------------------------\n    # Execute\n    # ------------------------------------------------------------------\n    result_dataset = pipeline.run(dataset)\n\n    # Check for 0 properties before attempting to save\n    if len([p for p in result_dataset.properties if p.property_description is not None]) == 0:\n        raise RuntimeError(\"Label pipeline completed with 0 valid properties. Check logs for parsing errors or API issues.\")\n\n    clustered_df = result_dataset.to_dataframe(type=\"clusters\", method=method)\n\n    # Save final summary and full dataset if output_dir is provided (same as explain() function)\n    if output_dir is not None:\n        _save_final_summary(result_dataset, clustered_df, result_dataset.model_stats, output_dir, verbose)\n\n        # Also save the full dataset for backward compatibility with compute_metrics_only and other tools\n        import pathlib\n        import json\n\n        output_path = pathlib.Path(output_dir)\n\n        # Save full dataset as JSON\n        full_dataset_json_path = output_path / \"full_dataset.json\"\n        result_dataset.save(str(full_dataset_json_path))\n        if verbose:\n            logger.info(f\"  \u2713 Saved full dataset: {full_dataset_json_path}\")\n\n    # Print analysis summary if verbose\n    _print_analysis_summary(result_dataset.model_stats, max_behaviors=5)\n\n    return clustered_df, result_dataset.model_stats\n</code></pre>"},{"location":"api/reference/#extract_properties_only","title":"extract_properties_only()","text":""},{"location":"api/reference/#stringsight.public.extract_properties_only","title":"<code>extract_properties_only(df, *, method='single_model', system_prompt=None, task_description=None, score_columns=None, sample_size=None, model_a=None, model_b=None, prompt_column='prompt', model_column=None, model_response_column=None, question_id_column=None, model_a_column=None, model_b_column=None, model_a_response_column=None, model_b_response_column=None, model_name='gpt-4.1', temperature=0.7, top_p=0.95, max_tokens=16000, max_workers=64, include_scores_in_prompt=False, use_wandb=True, wandb_project=None, verbose=False, output_dir=None, extraction_cache_dir=None, return_debug=False)</code>","text":"<p>Run only the extraction \u2192 parsing \u2192 validation stages and return a PropertyDataset.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input conversations dataframe (single_model or side_by_side format)</p> required <code>method</code> <code>str</code> <p>\"single_model\" | \"side_by_side\"</p> <code>'single_model'</code> <code>system_prompt</code> <code>str | None</code> <p>Explicit system prompt text or a short prompt name from stringsight.prompts</p> <code>None</code> <code>task_description</code> <code>str | None</code> <p>Optional task-aware description (used only if the chosen prompt has {task_description})</p> <code>None</code> <code>score_columns</code> <code>Optional[List[str]]</code> <p>Optional list of column names containing score metrics to convert to dict format</p> <code>None</code> <code>sample_size</code> <code>Optional[int]</code> <p>Optional number of rows to sample from the dataset before processing</p> <code>None</code> <code>model_a</code> <code>Optional[str]</code> <p>For side_by_side method with tidy data, specifies first model to select</p> <code>None</code> <code>model_b</code> <code>Optional[str]</code> <p>For side_by_side method with tidy data, specifies second model to select</p> <code>None</code> <code>prompt_column</code> <code>str</code> <p>Name of the prompt column in your dataframe (default: \"prompt\")</p> <code>'prompt'</code> <code>model_column</code> <code>Optional[str]</code> <p>Name of the model column for single_model (default: \"model\")</p> <code>None</code> <code>model_response_column</code> <code>Optional[str]</code> <p>Name of the model response column for single_model (default: \"model_response\")</p> <code>None</code> <code>question_id_column</code> <code>Optional[str]</code> <p>Name of the question_id column (default: \"question_id\" if column exists)</p> <code>None</code> <code>model_a_column</code> <code>Optional[str]</code> <p>Name of the model_a column for side_by_side (default: \"model_a\")</p> <code>None</code> <code>model_b_column</code> <code>Optional[str]</code> <p>Name of the model_b column for side_by_side (default: \"model_b\")</p> <code>None</code> <code>model_a_response_column</code> <code>Optional[str]</code> <p>Name of the model_a_response column for side_by_side (default: \"model_a_response\")</p> <code>None</code> <code>model_b_response_column</code> <code>Optional[str]</code> <p>Name of the model_b_response column for side_by_side (default: \"model_b_response\")</p> <code>None</code> <code>model_name, temperature, top_p, max_tokens, max_workers</code> <p>LLM config for extraction</p> required <code>include_scores_in_prompt</code> <code>bool</code> <p>Whether to include any provided score fields in the prompt context</p> <code>False</code> <code>use_wandb, wandb_project, verbose</code> <p>Logging configuration</p> required <code>output_dir</code> <code>str | None</code> <p>If provided, stages will auto-save their artefacts to this directory</p> <code>None</code> <code>extraction_cache_dir</code> <code>str | None</code> <p>Optional cache directory for extractor</p> <code>None</code> <p>Returns:</p> Type Description <code>PropertyDataset | tuple[PropertyDataset, list[dict[str, Any]]]</code> <p>PropertyDataset containing parsed Property objects (no clustering or metrics).</p> Source code in <code>stringsight/public.py</code> <pre><code>def extract_properties_only(\n    df: pd.DataFrame,\n    *,\n    method: str = \"single_model\",\n    system_prompt: str | None = None,\n    task_description: str | None = None,\n    # Data preparation\n    score_columns: Optional[List[str]] = None,\n    sample_size: Optional[int] = None,\n    model_a: Optional[str] = None,\n    model_b: Optional[str] = None,\n    # Column mapping parameters\n    prompt_column: str = \"prompt\",\n    model_column: Optional[str] = None,\n    model_response_column: Optional[str] = None,\n    question_id_column: Optional[str] = None,\n    model_a_column: Optional[str] = None,\n    model_b_column: Optional[str] = None,\n    model_a_response_column: Optional[str] = None,\n    model_b_response_column: Optional[str] = None,\n    # Extraction parameters\n    model_name: str = \"gpt-4.1\",\n    temperature: float = 0.7,\n    top_p: float = 0.95,\n    max_tokens: int = 16000,\n    max_workers: int = 64,\n    include_scores_in_prompt: bool = False,\n    # Logging &amp; output\n    use_wandb: bool = True,\n    wandb_project: str | None = None,\n    verbose: bool = False,\n    output_dir: str | None = None,\n    # Caching\n    extraction_cache_dir: str | None = None,\n    return_debug: bool = False,\n) -&gt; PropertyDataset | tuple[PropertyDataset, list[dict[str, Any]]]:\n    \"\"\"Run only the extraction \u2192 parsing \u2192 validation stages and return a PropertyDataset.\n\n    Args:\n        df: Input conversations dataframe (single_model or side_by_side format)\n        method: \"single_model\" | \"side_by_side\"\n        system_prompt: Explicit system prompt text or a short prompt name from stringsight.prompts\n        task_description: Optional task-aware description (used only if the chosen prompt has {task_description})\n        score_columns: Optional list of column names containing score metrics to convert to dict format\n        sample_size: Optional number of rows to sample from the dataset before processing\n        model_a: For side_by_side method with tidy data, specifies first model to select\n        model_b: For side_by_side method with tidy data, specifies second model to select\n        prompt_column: Name of the prompt column in your dataframe (default: \"prompt\")\n        model_column: Name of the model column for single_model (default: \"model\")\n        model_response_column: Name of the model response column for single_model (default: \"model_response\")\n        question_id_column: Name of the question_id column (default: \"question_id\" if column exists)\n        model_a_column: Name of the model_a column for side_by_side (default: \"model_a\")\n        model_b_column: Name of the model_b column for side_by_side (default: \"model_b\")\n        model_a_response_column: Name of the model_a_response column for side_by_side (default: \"model_a_response\")\n        model_b_response_column: Name of the model_b_response column for side_by_side (default: \"model_b_response\")\n        model_name, temperature, top_p, max_tokens, max_workers: LLM config for extraction\n        include_scores_in_prompt: Whether to include any provided score fields in the prompt context\n        use_wandb, wandb_project, verbose: Logging configuration\n        output_dir: If provided, stages will auto-save their artefacts to this directory\n        extraction_cache_dir: Optional cache directory for extractor\n\n    Returns:\n        PropertyDataset containing parsed Property objects (no clustering or metrics).\n    \"\"\"\n    # Validate OpenAI API key is set if using GPT models\n    validate_openai_api_key(\n        model_name=model_name\n    )\n\n    # Resolve system prompt using centralized resolver\n    system_prompt = get_system_prompt(method, system_prompt, task_description)\n\n    if verbose:\n        logger.info(\"\\n\" + \"=\"*80)\n        logger.info(\"SYSTEM PROMPT\")\n        logger.info(\"=\"*80)\n        logger.info(system_prompt)\n        logger.info(\"=\"*80 + \"\\n\")\n    if len(system_prompt) &lt; 50:\n        raise ValueError(\"System prompt is too short. Please provide a longer system prompt.\")\n\n    # Preprocess data: handle score_columns, sampling, tidy\u2192side_by_side conversion, column mapping\n    from .core.preprocessing import validate_and_prepare_dataframe\n    df = validate_and_prepare_dataframe(\n        df,\n        method=method,\n        score_columns=score_columns,\n        sample_size=sample_size,\n        model_a=model_a,\n        model_b=model_b,\n        prompt_column=prompt_column,\n        model_column=model_column,\n        model_response_column=model_response_column,\n        question_id_column=question_id_column,\n        model_a_column=model_a_column,\n        model_b_column=model_b_column,\n        model_a_response_column=model_a_response_column,\n        model_b_response_column=model_b_response_column,\n        verbose=verbose,\n    )\n\n    # Prepare dataset\n    dataset = PropertyDataset.from_dataframe(df, method=method)\n\n    # Align env with wandb toggle early\n    import os as _os\n    if not use_wandb:\n        _os.environ[\"WANDB_DISABLED\"] = \"true\"\n    else:\n        _os.environ.pop(\"WANDB_DISABLED\", None)\n\n    # Build a minimal pipeline: extractor \u2192 parser \u2192 validator\n    from .extractors import get_extractor\n    from .postprocess import LLMJsonParser, PropertyValidator\n\n    common_cfg = {\"verbose\": verbose, \"use_wandb\": use_wandb, \"wandb_project\": wandb_project or \"StringSight\"}\n\n    extractor_kwargs = {\n        \"model_name\": model_name,\n        \"system_prompt\": system_prompt,\n        \"prompt_builder\": None,\n        \"temperature\": temperature,\n        \"top_p\": top_p,\n        \"max_tokens\": max_tokens,\n        \"max_workers\": max_workers,\n        \"include_scores_in_prompt\": include_scores_in_prompt,\n        \"output_dir\": output_dir,\n        **({\"cache_dir\": extraction_cache_dir} if extraction_cache_dir else {}),\n        **common_cfg,\n    }\n\n    extractor = get_extractor(**extractor_kwargs)\n    # Do not fail the whole run on parsing errors \u2013 collect failures and drop those rows\n    parser = LLMJsonParser(fail_fast=False, output_dir=output_dir, **common_cfg)\n    validator = PropertyValidator(output_dir=output_dir, **common_cfg)\n\n    pipeline = PipelineBuilder(name=f\"StringSight-extract-{method}\") \\\n        .extract_properties(extractor) \\\n        .parse_properties(parser) \\\n        .add_stage(validator) \\\n        .configure(output_dir=output_dir, **common_cfg) \\\n        .build()\n\n    result_dataset = pipeline.run(dataset)\n    if return_debug:\n        try:\n            failures = parser.get_parsing_failures()\n        except Exception:\n            failures = []\n        return result_dataset, failures\n    return result_dataset\n</code></pre>"},{"location":"api/reference/#compute_metrics_only","title":"compute_metrics_only()","text":""},{"location":"api/reference/#stringsight.public.compute_metrics_only","title":"<code>compute_metrics_only(input_path, method='single_model', output_dir=None, metrics_kwargs=None, use_wandb=True, verbose=False)</code>","text":"<p>Run only the metrics computation stage on existing pipeline results.</p> <p>This function loads existing pipeline results (from extraction and clustering stages) and runs only the metrics computation stage. Useful for: - Recomputing metrics with different parameters - Running metrics on results from previous pipeline runs - Debugging metrics computation without re-running the full pipeline</p> <p>Parameters:</p> Name Type Description Default <code>input_path</code> <code>str</code> <p>Path to existing pipeline results (file or directory)</p> required <code>method</code> <code>str</code> <p>\"single_model\" or \"side_by_side\"</p> <code>'single_model'</code> <code>output_dir</code> <code>Optional[str]</code> <p>Directory to save metrics results (optional)</p> <code>None</code> <code>metrics_kwargs</code> <code>Optional[Dict[str, Any]]</code> <p>Additional arguments for metrics computation</p> <code>None</code> <code>use_wandb</code> <code>bool</code> <p>Whether to enable wandb logging</p> <code>True</code> <code>verbose</code> <code>bool</code> <p>Whether to print verbose output</p> <code>False</code> <p>Returns:</p> Type Description <code>Tuple[DataFrame, Dict[str, Any]]</code> <p>Tuple of (clustered_df, model_stats)</p> Example <p>from stringsight import compute_metrics_only</p> Source code in <code>stringsight/public.py</code> <pre><code>def compute_metrics_only(\n    input_path: str,\n    method: str = \"single_model\",\n    output_dir: Optional[str] = None,\n    metrics_kwargs: Optional[Dict[str, Any]] = None,\n    use_wandb: bool = True,\n    verbose: bool = False\n) -&gt; Tuple[pd.DataFrame, Dict[str, Any]]:\n    \"\"\"\n    Run only the metrics computation stage on existing pipeline results.\n\n    This function loads existing pipeline results (from extraction and clustering stages)\n    and runs only the metrics computation stage. Useful for:\n    - Recomputing metrics with different parameters\n    - Running metrics on results from previous pipeline runs\n    - Debugging metrics computation without re-running the full pipeline\n\n    Args:\n        input_path: Path to existing pipeline results (file or directory)\n        method: \"single_model\" or \"side_by_side\"\n        output_dir: Directory to save metrics results (optional)\n        metrics_kwargs: Additional arguments for metrics computation\n        use_wandb: Whether to enable wandb logging\n        verbose: Whether to print verbose output\n\n    Returns:\n        Tuple of (clustered_df, model_stats)\n\n    Example:\n        &gt;&gt;&gt; from stringsight import compute_metrics_only\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; # Run metrics on existing pipeline results\n        &gt;&gt;&gt; clustered_df, model_stats = compute_metrics_only(\n        ...     input_path=\"results/previous_run/full_dataset.json\",\n        ...     method=\"single_model\",\n        ...     output_dir=\"results/metrics_only\"\n        ... )\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; # Or run on a directory containing pipeline outputs\n        &gt;&gt;&gt; clustered_df, model_stats = compute_metrics_only(\n        ...     input_path=\"results/previous_run/\",\n        ...     method=\"side_by_side\"\n        ... )\n    \"\"\"\n    from pathlib import Path\n    from .metrics import get_metrics\n    from .pipeline import Pipeline\n    import json\n\n    # Align environment with wandb toggle early to avoid accidental logging on import\n    import os as _os\n    if not use_wandb:\n        _os.environ[\"WANDB_DISABLED\"] = \"true\"\n    else:\n        _os.environ.pop(\"WANDB_DISABLED\", None)\n\n    input_path = Path(input_path)\n\n    # Load existing dataset\n    if input_path.is_dir():\n        # Try to load from a directory containing pipeline outputs\n        possible_files = [\n            input_path / \"full_dataset.json\",\n            input_path / \"full_dataset.parquet\", \n            input_path / \"clustered_results.parquet\",\n            input_path / \"dataset.json\",\n            input_path / \"dataset.parquet\"\n        ]\n\n        for file_path in possible_files:\n            if file_path.exists():\n                if verbose:\n                    logger.info(f\"Loading from: {file_path}\")\n                dataset = PropertyDataset.load(str(file_path))\n                break\n        else:\n            raise FileNotFoundError(f\"No recognizable dataset file found in {input_path}\")\n\n    elif input_path.is_file():\n        # Load from a specific file\n        if verbose:\n            logger.info(f\"Loading from: {input_path}\")\n        dataset = PropertyDataset.load(str(input_path))\n\n    else:\n        raise FileNotFoundError(f\"Input path does not exist: {input_path}\")\n\n    # Verify we have the required data for metrics\n    if not dataset.clusters:\n        raise ValueError(\"No clusters found in the dataset. Metrics computation requires clustered data.\")\n\n    if not dataset.properties:\n        raise ValueError(\"No properties found in the dataset. Metrics computation requires extracted properties.\")\n\n    if verbose:\n        logger.info(f\"Loaded dataset with:\")\n        logger.info(f\"  - {len(dataset.conversations)} conversations\")\n        logger.info(f\"  - {len(dataset.properties)} properties\")\n        logger.info(f\"  - {len(dataset.clusters)} clusters\")\n        logger.info(f\"  - Models: {dataset.all_models}\")\n\n        # Count unique models from conversations for verification\n        unique_models = set()\n        for conv in dataset.conversations:\n            if isinstance(conv.model, list):\n                unique_models.update(conv.model)\n            else:\n                unique_models.add(conv.model)\n\n        logger.info(f\"  - Total unique models: {len(unique_models)}\")\n        if len(unique_models) &lt;= 20:\n            model_list = sorted(list(unique_models))\n            logger.info(f\"  - Model names: {', '.join(model_list)}\")\n        logger.info(\"\")\n\n    # Create metrics stage\n    metrics_config = {\n        'method': method,\n        'use_wandb': use_wandb,\n        'verbose': verbose,\n        **(metrics_kwargs or {})\n    }\n\n    # Add output directory if provided\n    if output_dir:\n        output_path = Path(output_dir)\n        output_path.mkdir(parents=True, exist_ok=True)\n        metrics_config['output_dir'] = str(output_path)\n\n    # Initialize wandb if enabled\n    if use_wandb:\n        try:\n            import wandb\n            # import weave\n            import os\n\n            # Try to get input filename from the input path\n            input_filename = \"unknown_dataset\"\n            if input_path.is_file():\n                input_filename = input_path.name\n            elif input_path.is_dir():\n                # Try to find a recognizable dataset file in the directory\n                possible_files = [\n                    input_path / \"full_dataset.json\",\n                    input_path / \"full_dataset.parquet\", \n                    input_path / \"clustered_results.parquet\",\n                    input_path / \"dataset.json\",\n                    input_path / \"dataset.parquet\"\n                ]\n\n                for file_path in possible_files:\n                    if file_path.exists():\n                        input_filename = file_path.name\n                        break\n                else:\n                    # If no recognizable file found, use the directory name\n                    input_filename = input_path.name\n\n            # Clean the filename for wandb (remove extension, replace spaces/special chars)\n            if isinstance(input_filename, str):\n                # Remove file extension and clean up the name\n                input_filename = os.path.splitext(os.path.basename(input_filename))[0]\n                # Replace spaces and special characters with underscores\n                input_filename = input_filename.replace(' ', '_').replace('-', '_')\n                # Remove any remaining special characters\n                import re\n                input_filename = re.sub(r'[^a-zA-Z0-9_]', '', input_filename)\n\n            wandb_run_name = os.path.basename(os.path.normpath(output_dir)) if output_dir else f\"{input_filename}_metrics_only\"\n\n            wandb.init(\n                project=\"StringSight\",\n                name=wandb_run_name,\n                config={\n                    \"method\": method,\n                    \"input_path\": str(input_path),\n                    \"output_dir\": output_dir,\n                    \"metrics_kwargs\": metrics_kwargs,\n                },\n                reinit=False  # Don't reinitialize if already exists\n            )\n        except ImportError:\n            # wandb not installed or not available\n            use_wandb = False\n\n    metrics_stage = get_metrics(**metrics_config)\n\n    # Create a minimal pipeline with just the metrics stage\n    pipeline = Pipeline(\"Metrics-Only\", [metrics_stage])\n\n    # Run metrics computation\n    if verbose:\n        logger.info(\"\\n\" + \"=\"*60)\n        logger.info(\"COMPUTING METRICS\")\n        logger.info(\"=\"*60)\n\n    result_dataset = pipeline.run(dataset)\n\n    # Convert back to DataFrame format\n    clustered_df = result_dataset.to_dataframe()\n    model_stats = result_dataset.model_stats\n\n    # Save results if output_dir is provided\n    if output_dir:\n        if verbose:\n            logger.info(f\"\\nSaving results to: {output_dir}\")\n\n        # Use the same saving mechanism as the full pipeline\n        _save_final_summary(\n            result_dataset=result_dataset,\n            clustered_df=clustered_df,\n            model_stats=model_stats,\n            output_dir=output_dir,\n            verbose=verbose\n        )\n\n        # Print summary\n        logger.info(f\"\\n\ud83d\udcca Metrics Summary:\")\n        logger.info(f\"  - Models analyzed: {len(model_stats)}\")\n\n        # Handle new functional metrics format\n        if model_stats and \"functional_metrics\" in model_stats:\n            functional_metrics = model_stats[\"functional_metrics\"]\n            model_scores = functional_metrics.get(\"model_scores\", {})\n            cluster_scores = functional_metrics.get(\"cluster_scores\", {})\n\n            logger.info(f\"  - Functional metrics computed:\")\n            logger.info(f\"    \u2022 Model scores: {len(model_scores)} models\")\n            logger.info(f\"    \u2022 Cluster scores: {len(cluster_scores)} clusters\")\n\n            # Print model-level summary\n            for model_name, model_data in model_scores.items():\n                if isinstance(model_data, dict):\n                    size = model_data.get(\"size\", 0)\n                    quality = model_data.get(\"quality\", {})\n                    logger.info(f\"    \u2022 {model_name}: {size} conversations\")\n                    if quality:\n                        for metric_name, metric_value in quality.items():\n                            if isinstance(metric_value, (int, float)):\n                                logger.info(f\"      - {metric_name}: {metric_value:.3f}\")\n\n        # Handle legacy format for backward compatibility\n        else:\n            for model_name, stats in model_stats.items():\n                if \"fine\" in stats:\n                    logger.info(f\"  - {model_name}: {len(stats['fine'])} fine clusters\")\n                if \"coarse\" in stats:\n                    logger.info(f\"    {len(stats['coarse'])} coarse clusters\")\n\n    return clustered_df, model_stats \n</code></pre>"},{"location":"api/reference/#stringsight.public.compute_metrics_only--run-metrics-on-existing-pipeline-results","title":"Run metrics on existing pipeline results","text":"<p>clustered_df, model_stats = compute_metrics_only( ...     input_path=\"results/previous_run/full_dataset.json\", ...     method=\"single_model\", ...     output_dir=\"results/metrics_only\" ... )</p>"},{"location":"api/reference/#stringsight.public.compute_metrics_only--or-run-on-a-directory-containing-pipeline-outputs","title":"Or run on a directory containing pipeline outputs","text":"<p>clustered_df, model_stats = compute_metrics_only( ...     input_path=\"results/previous_run/\", ...     method=\"side_by_side\" ... )</p>"},{"location":"api/reference/#convenience-functions","title":"Convenience Functions","text":""},{"location":"api/reference/#explain_side_by_side","title":"explain_side_by_side()","text":""},{"location":"api/reference/#stringsight.public.explain_side_by_side","title":"<code>explain_side_by_side(df, system_prompt=None, tidy_side_by_side_models=None, **kwargs)</code>","text":"<p>Convenience function for side-by-side model comparison.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame with columns: model_a, model_b, model_a_response, model_b_response, winner</p> required <code>system_prompt</code> <code>str</code> <p>System prompt for extraction (if None, will be auto-determined)</p> <code>None</code> <code>**kwargs</code> <p>Additional arguments passed to explain()</p> <code>{}</code> <p>Returns:</p> Type Description <code>Tuple[DataFrame, Dict[str, DataFrame]]</code> <p>Tuple of (clustered_df, model_stats)</p> Source code in <code>stringsight/public.py</code> <pre><code>def explain_side_by_side(\n    df: pd.DataFrame,\n    system_prompt: str = None,\n    tidy_side_by_side_models: Optional[Tuple[str, str]] = None,\n    **kwargs\n) -&gt; Tuple[pd.DataFrame, Dict[str, pd.DataFrame]]:\n    \"\"\"\n    Convenience function for side-by-side model comparison.\n\n    Args:\n        df: DataFrame with columns: model_a, model_b, model_a_response, model_b_response, winner\n        system_prompt: System prompt for extraction (if None, will be auto-determined)\n        **kwargs: Additional arguments passed to explain()\n\n    Returns:\n        Tuple of (clustered_df, model_stats)\n    \"\"\"\n    return explain(\n        df,\n        method=\"side_by_side\",\n        system_prompt=system_prompt,\n        tidy_side_by_side_models=tidy_side_by_side_models,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/reference/#explain_single_model","title":"explain_single_model()","text":""},{"location":"api/reference/#stringsight.public.explain_single_model","title":"<code>explain_single_model(df, system_prompt=None, **kwargs)</code>","text":"<p>Convenience function for single model analysis.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame with columns: model, model_response, score</p> required <code>system_prompt</code> <code>str</code> <p>System prompt for extraction (if None, will be auto-determined)</p> <code>None</code> <code>**kwargs</code> <p>Additional arguments passed to explain()</p> <code>{}</code> <p>Returns:</p> Type Description <code>Tuple[DataFrame, Dict[str, DataFrame]]</code> <p>Tuple of (clustered_df, model_stats)</p> Source code in <code>stringsight/public.py</code> <pre><code>def explain_single_model(\n    df: pd.DataFrame,\n    system_prompt: str = None,\n    **kwargs\n) -&gt; Tuple[pd.DataFrame, Dict[str, pd.DataFrame]]:\n    \"\"\"\n    Convenience function for single model analysis.\n\n    Args:\n        df: DataFrame with columns: model, model_response, score\n        system_prompt: System prompt for extraction (if None, will be auto-determined)\n        **kwargs: Additional arguments passed to explain()\n\n    Returns:\n        Tuple of (clustered_df, model_stats)\n    \"\"\"\n    return explain(df, method=\"single_model\", system_prompt=system_prompt, **kwargs)\n</code></pre>"},{"location":"api/reference/#explain_with_custom_pipeline","title":"explain_with_custom_pipeline()","text":""},{"location":"api/reference/#stringsight.public.explain_with_custom_pipeline","title":"<code>explain_with_custom_pipeline(df, pipeline, method='single_model')</code>","text":"<p>Explain model behavior using a custom pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame with conversation data</p> required <code>pipeline</code> <code>Pipeline</code> <p>Custom pipeline to use</p> required <code>method</code> <code>str</code> <p>\"side_by_side\" or \"single_model\"</p> <code>'single_model'</code> <p>Returns:</p> Type Description <code>Tuple[DataFrame, Dict[str, Any]]</code> <p>Tuple of (clustered_df, model_stats)</p> Source code in <code>stringsight/public.py</code> <pre><code>def explain_with_custom_pipeline(\n    df: pd.DataFrame,\n    pipeline: Pipeline,\n    method: str = \"single_model\"\n) -&gt; Tuple[pd.DataFrame, Dict[str, Any]]:\n    \"\"\"\n    Explain model behavior using a custom pipeline.\n\n    Args:\n        df: DataFrame with conversation data\n        pipeline: Custom pipeline to use\n        method: \"side_by_side\" or \"single_model\"\n\n    Returns:\n        Tuple of (clustered_df, model_stats)\n    \"\"\"\n    dataset = PropertyDataset.from_dataframe(df)\n    result_dataset = pipeline.run(dataset)\n    return result_dataset.to_dataframe(), result_dataset.model_stats\n</code></pre>"},{"location":"api/reference/#core-data-structures","title":"Core Data Structures","text":""},{"location":"api/reference/#propertydataset","title":"PropertyDataset","text":""},{"location":"api/reference/#stringsight.core.data_objects.PropertyDataset","title":"<code>PropertyDataset</code>  <code>dataclass</code>","text":"<p>Container for all data flowing through the pipeline.</p> <p>This is the single data contract between all pipeline stages.</p> Source code in <code>stringsight/core/data_objects.py</code> <pre><code>@dataclass\nclass PropertyDataset:\n    \"\"\"\n    Container for all data flowing through the pipeline.\n\n    This is the single data contract between all pipeline stages.\n    \"\"\"\n    conversations: List[ConversationRecord] = field(default_factory=list)\n    all_models: List[str] = field(default_factory=list)\n    properties: List[Property] = field(default_factory=list)\n    clusters: List[Cluster] = field(default_factory=list)\n    model_stats: Dict[str, Any] = field(default_factory=dict)\n\n    def __str__(self) -&gt; str:\n        \"\"\"Return a readable string representation of the PropertyDataset.\"\"\"\n        lines = [\n            \"PropertyDataset:\",\n            f\"  conversations: List[ConversationRecord] ({len(self.conversations)} items)\",\n            f\"  all_models: List[str] ({len(self.all_models)} items) - {self.all_models}\",\n            f\"  properties: List[Property] ({len(self.properties)} items)\",\n            f\"  clusters: List[Cluster] ({len(self.clusters)} items)\",\n            f\"  model_stats: Dict[str, Any] ({len(self.model_stats)} entries)\"\n        ]\n\n        return \"\\n\".join(lines)\n\n    @classmethod\n    def from_dataframe(cls, df: pd.DataFrame, method: str = \"single_model\") -&gt; \"PropertyDataset\":\n        \"\"\"\n        Create PropertyDataset from existing DataFrame formats.\n\n        Args:\n            df: Input DataFrame with conversation data\n            method: \"side_by_side\" for comparison data, \"single_model\" for single responses\n\n        Returns:\n            PropertyDataset with populated conversations\n        \"\"\"\n        conversations = []\n        if method == \"side_by_side\":\n            all_models = list(set(df[\"model_a\"].unique().tolist() + df[\"model_b\"].unique().tolist()))\n            # Expected columns: question_id, prompt, model_a, model_b,\n            # model_a_response, model_b_response, scores_a, scores_b, winner, etc.\n\n            # Convert to list of dicts once - MUCH faster than iterrows()\n            rows_list = df.to_dict('records')\n\n            # Parallelize OAI conversions for better performance\n            def _process_side_by_side_row(idx_row):\n                idx, row = idx_row\n                prompt = str(row.get('prompt', row.get('user_prompt', '')))\n                model_a_response = row.get('model_a_response', '')\n                model_b_response = row.get('model_b_response', '')\n\n                # Convert responses to OAI format if they're strings\n                oai_response_a, was_converted_a = check_and_convert_to_oai_format(prompt, model_a_response)\n                oai_response_b, was_converted_b = check_and_convert_to_oai_format(prompt, model_b_response)\n\n                return idx, oai_response_a, oai_response_b, row\n\n            # Pre-allocate results list\n            oai_results = [None] * len(rows_list)\n\n            # Process conversions in parallel\n            with ThreadPoolExecutor(max_workers=min(64, len(rows_list))) as executor:\n                futures = {executor.submit(_process_side_by_side_row, (idx, row)): idx\n                          for idx, row in enumerate(rows_list)}\n                for future in as_completed(futures):\n                    idx, oai_response_a, oai_response_b, row = future.result()\n                    oai_results[idx] = (oai_response_a, oai_response_b, row)\n\n            # Now build conversations with pre-converted OAI responses\n            for idx, result in enumerate(oai_results):\n                oai_response_a, oai_response_b, row = result\n                prompt = str(row.get('prompt', row.get('user_prompt', '')))\n\n                # Convert score formats to list format [scores_a, scores_b]\n                def parse_score_field(score_value):\n                    \"\"\"Parse score field that might be a string, dict, or other type.\"\"\"\n                    if isinstance(score_value, dict):\n                        return score_value\n                    elif isinstance(score_value, str) and score_value.strip():\n                        try:\n                            import ast\n                            parsed = ast.literal_eval(score_value.strip())\n                            return parsed if isinstance(parsed, dict) else {}\n                        except (ValueError, SyntaxError):\n                            return {}\n                    else:\n                        return {}\n\n                if 'score_a' in row and 'score_b' in row:\n                    # Format: score_a, score_b columns\n                    scores_a = parse_score_field(row.get('score_a', {}))\n                    scores_b = parse_score_field(row.get('score_b', {}))\n                else:\n                    # No score data found\n                    scores_a, scores_b = {}, {}\n\n                scores = [scores_a, scores_b]\n\n                # Store winner and other metadata\n                meta_with_winner = {k: v for k, v in row.items() \n                                  if k not in ['question_id', 'prompt', 'user_prompt', 'model_a', 'model_b', \n                                             'model_a_response', 'model_b_response', 'score', 'score_a', 'score_b']}\n\n                # Add winner to meta if present\n                winner = row.get('winner')\n                if winner is not None:\n                    meta_with_winner['winner'] = winner\n\n                conversation = ConversationRecord(\n                    question_id=str(idx),  # Auto-generate as row index\n                    prompt=prompt,\n                    model=[row.get('model_a', 'model_a'), row.get('model_b', 'model_b')],\n                    responses=[oai_response_a, oai_response_b],\n                    scores=scores,\n                    meta=meta_with_winner\n                )\n                conversations.append(conversation)\n\n        elif method == \"single_model\":\n            all_models = df[\"model\"].unique().tolist()\n            # Expected columns: question_id, prompt, model, model_response, score, etc.\n\n            def parse_single_score_field(score_value):\n                \"\"\"Parse single model score field that might be a string, dict, number, or other type.\"\"\"\n                if isinstance(score_value, dict):\n                    return score_value\n                elif isinstance(score_value, (int, float)):\n                    return {'score': score_value}\n                elif isinstance(score_value, str) and score_value.strip():\n                    try:\n                        import ast\n                        parsed = ast.literal_eval(score_value.strip())\n                        if isinstance(parsed, dict):\n                            return parsed\n                        elif isinstance(parsed, (int, float)):\n                            return {'score': parsed}\n                        else:\n                            return {'score': 0}\n                    except (ValueError, SyntaxError):\n                        return {'score': 0}\n                else:\n                    return {'score': 0}\n\n            # Convert to list of dicts once - MUCH faster than iterrows()\n            rows_list = df.to_dict('records')\n\n            # Parallelize OAI conversions for better performance\n            def _process_single_model_row(idx_row):\n                idx, row = idx_row\n                prompt = str(row.get('prompt', row.get('user_prompt', '')))\n                response = row.get('model_response', '')\n\n                # Convert response to OAI format if it's a string\n                oai_response, was_converted = check_and_convert_to_oai_format(prompt, response)\n\n                return idx, oai_response, row\n\n            # Pre-allocate results list\n            oai_results = [None] * len(rows_list)\n\n            # Process conversions in parallel\n            with ThreadPoolExecutor(max_workers=min(64, len(rows_list))) as executor:\n                futures = {executor.submit(_process_single_model_row, (idx, row)): idx\n                          for idx, row in enumerate(rows_list)}\n                for future in as_completed(futures):\n                    idx, oai_response, row = future.result()\n                    oai_results[idx] = (oai_response, row)\n\n            # Now build conversations with pre-converted OAI responses\n            for idx, result in enumerate(oai_results):\n                oai_response, row = result\n                scores = parse_single_score_field(row.get('score'))\n                prompt = str(row.get('prompt', row.get('user_prompt', '')))\n\n                conversation = ConversationRecord(\n                    question_id=str(idx),  # Auto-generate as row index\n                    prompt=prompt,\n                    model=str(row.get('model', 'model')),\n                    responses=oai_response,\n                    scores=scores,\n                    meta={k: v for k, v in row.items()\n                          if k not in ['question_id', 'prompt', 'user_prompt', 'model', 'model_response', 'score']}\n                )\n                conversations.append(conversation)\n        else:\n            raise ValueError(f\"Unknown method: {method}. Must be 'side_by_side' or 'single_model'\")\n\n        return cls(conversations=conversations, all_models=all_models)\n\n    def to_dataframe(self, type: str = \"all\", method: str = \"side_by_side\") -&gt; pd.DataFrame:\n        \"\"\"\n        Convert PropertyDataset back to DataFrame format.\n\n        Returns:\n            DataFrame with original data plus extracted properties and clusters\n        \"\"\"\n\n        assert type in [\"base\", \"properties\", \"clusters\", \"all\"], f\"Invalid type: {type}. Must be 'all' or 'base'\"\n        # Start with conversation data\n        rows = []\n        for conv in self.conversations:\n            if isinstance(conv.model, str):\n                base_row = {\n                    'question_id': conv.question_id,\n                    'prompt': conv.prompt,\n                    'model': conv.model,\n                    'model_response': conv.responses,\n                    'score': conv.scores,\n                    **conv.meta\n                }\n            elif isinstance(conv.model, list):\n                # Side-by-side format: scores stored as [scores_a, scores_b]\n                if isinstance(conv.scores, list) and len(conv.scores) == 2:\n                    scores_a, scores_b = conv.scores[0], conv.scores[1]\n                else:\n                    # Fallback if scores isn't properly formatted\n                    scores_a, scores_b = {}, {}\n\n                base_row = {\n                    'question_id': conv.question_id,\n                    'prompt': conv.prompt,\n                    'model_a': conv.model[0],\n                    'model_b': conv.model[1],\n                    'model_a_response': conv.responses[0],\n                    'model_b_response': conv.responses[1],\n                    'score_a': scores_a,\n                    'score_b': scores_b,\n                    'winner': conv.meta.get('winner'),  # Winner stored in meta\n                    **{k: v for k, v in conv.meta.items() if k != 'winner'}  # Exclude winner from other meta\n                }\n            else:\n                raise ValueError(f\"Invalid model type: {type(conv.model)}. Must be str or list.\")\n\n            rows.append(base_row)\n\n        df = pd.DataFrame(rows)\n        logger.debug(f\"Original unique questions: {df.question_id.nunique()}\")\n\n        # Add properties if they exist\n        if self.properties and type in [\"all\", \"properties\", \"clusters\"]:\n            # Create a mapping from (question_id, model) to properties\n            prop_map = {}\n            for prop in self.properties:\n                key = (prop.question_id, prop.model)\n                if key not in prop_map:\n                    prop_map[key] = []\n                prop_map[key].append(prop)\n\n            # create property df\n            prop_df = pd.DataFrame([p.to_dict() for p in self.properties])\n            logger.debug(f\"len of base df {len(df)}\")\n            if \"model_a\" in df.columns and \"model_b\" in df.columns:\n                # For side-by-side inputs, merge properties by question_id (both models share the question)\n                df = df.merge(prop_df, on=[\"question_id\"], how=\"left\")\n                # Deduplicate by property id when available\n                if \"id\" in df.columns:\n                    df = df.drop_duplicates(subset=\"id\")\n                    # Alias for clarity: id refers to property id\n                    if \"property_id\" not in df.columns:\n                        df[\"property_id\"] = df[\"id\"]\n            else:\n                # CHANGE: Use left join to preserve all conversations, including those without properties\n                # Don't drop duplicates to ensure conversations without properties are preserved\n                df = df.merge(prop_df, on=[\"question_id\", \"model\"], how=\"left\")\n                # Alias when present\n                if \"id\" in df.columns and \"property_id\" not in df.columns:\n                    df[\"property_id\"] = df[\"id\"]\n            logger.debug(f\"len of df after merge with properties {len(df)}\")\n\n            # ------------------------------------------------------------------\n            # Ensure `model` column is present (avoid _x / _y duplicates)\n            # ------------------------------------------------------------------\n            if \"model\" not in df.columns:\n                if \"model_y\" in df.columns:\n                    print(f\"df.model_y.value_counts(): {df.model_y.value_counts()}\")\n                if \"model_x\" in df.columns:\n                    print(f\"df.model_x.value_counts(): {df.model_x.value_counts()}\")\n                if \"model_x\" in df.columns or \"model_y\" in df.columns:\n                    df[\"model\"] = df.get(\"model_y\").combine_first(df.get(\"model_x\"))\n                    df.drop(columns=[c for c in [\"model_x\", \"model_y\"] if c in df.columns], inplace=True)\n\n        # Only print model value counts if the column exists\n        if \"model\" in df.columns:\n            logger.debug(f\"df.model.value_counts() NEW: {df.model.value_counts()}\")\n        logger.debug(f\"total questions: {df.question_id.nunique()}\")\n\n        if self.clusters and type in [\"all\", \"clusters\"]:\n            # If cluster columns already exist (e.g. after reload from parquet)\n            # skip the merge to avoid duplicate _x / _y columns.\n            if \"cluster_id\" not in df.columns:\n                cluster_df = pd.DataFrame([c.to_dict() for c in self.clusters])\n                cluster_df.rename(\n                    columns={\n                        \"id\": \"cluster_id\",\n                        \"label\": \"cluster_label\",\n                        \"size\": \"cluster_size\",\n                        \"property_descriptions\": \"property_description\",\n                    },\n                    inplace=True,\n                )\n                # Explode aligned list columns so each row maps to a single property\n                # Explode only aligned columns to avoid mismatched element counts\n                list_cols = [\n                    col for col in [\n                        \"property_description\",\n                        \"question_ids\",\n                    ] if col in cluster_df.columns\n                ]\n                if list_cols:\n                    try:\n                        cluster_df = cluster_df.explode(list_cols, ignore_index=True)\n                    except (TypeError, ValueError):\n                        # Fallback: explode sequentially to avoid alignment constraints\n                        for col in list_cols:\n                            cluster_df = cluster_df.explode(col, ignore_index=True)\n                df = df.merge(cluster_df, on=[\"property_description\"], how=\"left\")\n\n        # CHANGE: Handle conversations without properties by creating a \"No properties\" cluster\n        # This ensures all conversations are considered in metrics calculation\n        if type in [\"all\", \"clusters\"]:\n            # Identify rows without properties (no property_description or it's NaN)\n            mask_no_properties = df[\"property_description\"].isna() | (df[\"property_description\"].astype(str).str.strip() == \"\")\n\n            # Only add the synthetic cluster if *all* rows lack a property description.\n            # If at least one property exists, we skip to avoid mixing partially\n            # processed conversations into a global \"No properties\" cluster.\n\n            if mask_no_properties.all():\n                logger.info(\"All conversations lack properties \u2013 creating 'No properties' cluster\")\n\n                # Fill in missing data for conversations without properties\n                df.loc[mask_no_properties, \"property_description\"] = \"No properties\"\n                df.loc[mask_no_properties, \"cluster_id\"] = -2  # Use -2 since -1 is for outliers\n                df.loc[mask_no_properties, \"cluster_label\"] = \"No properties\"\n\n                # Handle missing scores for conversations without properties\n                mask_no_score = mask_no_properties &amp; (df[\"score\"].isna() | (df[\"score\"] == {}))\n                if mask_no_score.any():\n                    df.loc[mask_no_score, \"score\"] = df.loc[mask_no_score, \"score\"].apply(lambda x: {\"score\": 0} if pd.isna(x) or x == {} else x)\n\n        return df\n\n    def add_property(self, property: Property):\n        \"\"\"Add a property to the dataset.\"\"\"\n        self.properties.append(property)\n        if isinstance(property.model, str) and property.model not in self.all_models:\n            self.all_models.append(property.model)\n        if isinstance(property.model, list):\n            for model in property.model:\n                if model not in self.all_models:\n                    self.all_models.append(model)\n\n    def get_properties_for_model(self, model: str) -&gt; List[Property]:\n        \"\"\"Get all properties for a specific model.\"\"\"\n        return [p for p in self.properties if p.model == model]\n\n    def get_properties_for_question(self, question_id: str) -&gt; List[Property]:\n        \"\"\"Get all properties for a specific question.\"\"\"\n        return [p for p in self.properties if p.question_id == question_id]\n\n    def _json_safe(self, obj: Any):\n        \"\"\"Recursively convert *obj* into JSON-safe types (lists, dicts, ints, floats, strings, bool, None).\"\"\"\n        if obj is None:\n            return obj\n        if isinstance(obj, str):\n            return obj\n        if isinstance(obj, bool):\n            return obj\n        if isinstance(obj, (int, float)):\n            # Handle NaN and infinity values - convert to None for valid JSON\n            if isinstance(obj, float) and (math.isnan(obj) or math.isinf(obj)):\n                return None\n            return obj\n        if isinstance(obj, np.ndarray):\n            return obj.tolist()\n        if isinstance(obj, np.generic):\n            return obj.item()\n        if isinstance(obj, (list, tuple, set)):\n            return [self._json_safe(o) for o in obj]\n        if isinstance(obj, dict):\n            # Convert keys to strings if they're not JSON-safe\n            json_safe_dict = {}\n            for k, v in obj.items():\n                # Convert tuple/list keys to string representation\n                if isinstance(k, (tuple, list)):\n                    safe_key = str(k)\n                elif isinstance(k, (str, int, float, bool)) or k is None:\n                    safe_key = k\n                else:\n                    safe_key = str(k)\n                json_safe_dict[safe_key] = self._json_safe(v)\n            return json_safe_dict\n\n        return str(obj)\n\n    def to_serializable_dict(self) -&gt; Dict[str, Any]:\n        \"\"\"Convert the whole dataset into a JSON-serialisable dict.\"\"\"\n        return {\n            \"conversations\": [self._json_safe(asdict(conv)) for conv in self.conversations],\n            \"properties\": [self._json_safe(asdict(prop)) for prop in self.properties],\n            \"clusters\": [self._json_safe(asdict(cluster)) for cluster in self.clusters],\n            \"model_stats\": self._json_safe(self.model_stats),\n            \"all_models\": self.all_models,\n        }\n\n    def get_valid_properties(self):\n        \"\"\"Get all properties where the property model is unknown, there is no property description, or the property description is empty.\"\"\"\n        logger.debug(f\"All models: {self.all_models}\")\n        logger.debug(f\"Properties: {self.properties[0].model}\")\n        logger.debug(f\"Property description: {self.properties[0].property_description}\")\n        return [prop for prop in self.properties if prop.model in self.all_models and prop.property_description is not None and prop.property_description.strip() != \"\"]\n\n    # ------------------------------------------------------------------\n    # \ud83d\udcdd Persistence helpers\n    # ------------------------------------------------------------------\n    def save(self, path: str, format: str = \"json\") -&gt; None:\n        \"\"\"Save the dataset to *path* in either ``json``, ``dataframe``, ``parquet`` or ``pickle`` format.\n\n        The JSON variant produces a fully human-readable file while the pickle\n        variant preserves the exact Python objects.\n        \"\"\"\n        import json, pickle, os\n\n        fmt = format.lower()\n        os.makedirs(os.path.dirname(path) or \".\", exist_ok=True)\n\n        if fmt == \"json\":\n            with open(path, \"w\", encoding=\"utf-8\") as f:\n                json.dump(self.to_serializable_dict(), f, ensure_ascii=False, indent=2)\n        elif fmt == \"dataframe\":\n            self.to_dataframe().to_json(path, orient=\"records\", lines=True)\n        elif fmt == \"parquet\":\n            self.to_dataframe().to_parquet(path)\n        elif fmt in {\"pkl\", \"pickle\"}:\n            with open(path, \"wb\") as f:\n                pickle.dump(self, f)\n        else:\n            raise ValueError(f\"Unsupported format: {format}. Use 'json' or 'pickle'.\")\n\n    @staticmethod\n    def get_all_models(conversations: List[ConversationRecord]):\n        \"\"\"Get all models in the dataset.\"\"\"\n        models = set()\n        for conv in conversations:\n            if isinstance(conv.model, list):\n                models.update(conv.model)\n            else:\n                models.add(conv.model)\n        return list(models)\n\n    @classmethod\n    def load(cls, path: str, format: str = \"json\") -&gt; \"PropertyDataset\":\n        \"\"\"Load a dataset previously saved with :py:meth:`save`.\"\"\"\n        import json, pickle\n\n        fmt = format.lower()\n        logger.info(f\"Loading dataset from {path} with format {fmt}\")\n        if fmt == \"json\":\n            logger.info(f\"Loading dataset from {path}\")\n            with open(path, \"r\") as f:\n                data = json.load(f)\n            logger.debug(f\"Data: {data.keys()}\")\n\n            # Expected format: dictionary with keys like \"conversations\", \"properties\", etc.\n            conversations = [ConversationRecord(**conv) for conv in data[\"conversations\"]]\n            properties = [Property(**prop) for prop in data.get(\"properties\", [])]\n\n            # Convert cluster data to Cluster objects\n            clusters = [Cluster(**cluster) for cluster in data.get(\"clusters\", [])]\n\n            model_stats = data.get(\"model_stats\", {})\n            all_models = data.get(\"all_models\", PropertyDataset.get_all_models(conversations))\n            return cls(conversations=conversations, properties=properties, clusters=clusters, model_stats=model_stats, all_models=all_models)\n        elif fmt == \"dataframe\":\n            # Handle dataframe format - this creates a list of objects when saved\n            import pandas as pd\n            try:\n                # Try to load as JSON Lines first\n                df = pd.read_json(path, orient=\"records\", lines=True)\n            except ValueError:\n                # If that fails, try regular JSON\n                df = pd.read_json(path, orient=\"records\")\n\n            # Detect method based on columns\n            method = \"side_by_side\" if {\"model_a\", \"model_b\"}.issubset(df.columns) else \"single_model\"\n\n            return cls.from_dataframe(df, method=method)\n        elif fmt in {\"pkl\", \"pickle\"}:\n            with open(path, \"rb\") as f:\n                obj = pickle.load(f)\n            if not isinstance(obj, cls):\n                raise TypeError(\"Pickle file does not contain a PropertyDataset object\")\n            return obj\n        elif fmt == \"parquet\":\n            # Load DataFrame and reconstruct minimal PropertyDataset with clusters\n            import pandas as pd\n            df = pd.read_parquet(path)\n\n            # Attempt to detect method\n            method = \"side_by_side\" if {\"model_a\", \"model_b\"}.issubset(df.columns) else \"single_model\"\n\n            dataset = cls.from_dataframe(df, method=method)\n\n            # Reconstruct Cluster objects if cluster columns are present\n            required_cols = {\n                \"cluster_id\",\n                \"cluster_label\",\n                \"property_description\",\n            }\n            if required_cols.issubset(df.columns):\n                clusters_dict = {}\n                for _, row in df.iterrows():\n                    cid = row[\"cluster_id\"]\n                    if pd.isna(cid):\n                        continue\n                    cluster = clusters_dict.setdefault(\n                        cid,\n                        Cluster(\n                            id=int(cid),\n                            label=row.get(\"cluster_label\", str(cid)),\n                            size=0,\n                        ),\n                    )\n                    cluster.size += 1\n                    pd_desc = row.get(\"property_description\")\n                    if pd_desc and pd_desc not in cluster.property_descriptions:\n                        cluster.property_descriptions.append(pd_desc)\n                    cluster.question_ids.append(str(row.get(\"question_id\", \"\")))\n\n                dataset.clusters = list(clusters_dict.values())\n\n            return dataset\n        else:\n            raise ValueError(f\"Unsupported format: {format}. Use 'json', 'dataframe', 'parquet', or 'pickle'.\") \n</code></pre>"},{"location":"api/reference/#stringsight.core.data_objects.PropertyDataset.from_dataframe","title":"<code>from_dataframe(df, method='single_model')</code>  <code>classmethod</code>","text":"<p>Create PropertyDataset from existing DataFrame formats.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input DataFrame with conversation data</p> required <code>method</code> <code>str</code> <p>\"side_by_side\" for comparison data, \"single_model\" for single responses</p> <code>'single_model'</code> <p>Returns:</p> Type Description <code>PropertyDataset</code> <p>PropertyDataset with populated conversations</p> Source code in <code>stringsight/core/data_objects.py</code> <pre><code>@classmethod\ndef from_dataframe(cls, df: pd.DataFrame, method: str = \"single_model\") -&gt; \"PropertyDataset\":\n    \"\"\"\n    Create PropertyDataset from existing DataFrame formats.\n\n    Args:\n        df: Input DataFrame with conversation data\n        method: \"side_by_side\" for comparison data, \"single_model\" for single responses\n\n    Returns:\n        PropertyDataset with populated conversations\n    \"\"\"\n    conversations = []\n    if method == \"side_by_side\":\n        all_models = list(set(df[\"model_a\"].unique().tolist() + df[\"model_b\"].unique().tolist()))\n        # Expected columns: question_id, prompt, model_a, model_b,\n        # model_a_response, model_b_response, scores_a, scores_b, winner, etc.\n\n        # Convert to list of dicts once - MUCH faster than iterrows()\n        rows_list = df.to_dict('records')\n\n        # Parallelize OAI conversions for better performance\n        def _process_side_by_side_row(idx_row):\n            idx, row = idx_row\n            prompt = str(row.get('prompt', row.get('user_prompt', '')))\n            model_a_response = row.get('model_a_response', '')\n            model_b_response = row.get('model_b_response', '')\n\n            # Convert responses to OAI format if they're strings\n            oai_response_a, was_converted_a = check_and_convert_to_oai_format(prompt, model_a_response)\n            oai_response_b, was_converted_b = check_and_convert_to_oai_format(prompt, model_b_response)\n\n            return idx, oai_response_a, oai_response_b, row\n\n        # Pre-allocate results list\n        oai_results = [None] * len(rows_list)\n\n        # Process conversions in parallel\n        with ThreadPoolExecutor(max_workers=min(64, len(rows_list))) as executor:\n            futures = {executor.submit(_process_side_by_side_row, (idx, row)): idx\n                      for idx, row in enumerate(rows_list)}\n            for future in as_completed(futures):\n                idx, oai_response_a, oai_response_b, row = future.result()\n                oai_results[idx] = (oai_response_a, oai_response_b, row)\n\n        # Now build conversations with pre-converted OAI responses\n        for idx, result in enumerate(oai_results):\n            oai_response_a, oai_response_b, row = result\n            prompt = str(row.get('prompt', row.get('user_prompt', '')))\n\n            # Convert score formats to list format [scores_a, scores_b]\n            def parse_score_field(score_value):\n                \"\"\"Parse score field that might be a string, dict, or other type.\"\"\"\n                if isinstance(score_value, dict):\n                    return score_value\n                elif isinstance(score_value, str) and score_value.strip():\n                    try:\n                        import ast\n                        parsed = ast.literal_eval(score_value.strip())\n                        return parsed if isinstance(parsed, dict) else {}\n                    except (ValueError, SyntaxError):\n                        return {}\n                else:\n                    return {}\n\n            if 'score_a' in row and 'score_b' in row:\n                # Format: score_a, score_b columns\n                scores_a = parse_score_field(row.get('score_a', {}))\n                scores_b = parse_score_field(row.get('score_b', {}))\n            else:\n                # No score data found\n                scores_a, scores_b = {}, {}\n\n            scores = [scores_a, scores_b]\n\n            # Store winner and other metadata\n            meta_with_winner = {k: v for k, v in row.items() \n                              if k not in ['question_id', 'prompt', 'user_prompt', 'model_a', 'model_b', \n                                         'model_a_response', 'model_b_response', 'score', 'score_a', 'score_b']}\n\n            # Add winner to meta if present\n            winner = row.get('winner')\n            if winner is not None:\n                meta_with_winner['winner'] = winner\n\n            conversation = ConversationRecord(\n                question_id=str(idx),  # Auto-generate as row index\n                prompt=prompt,\n                model=[row.get('model_a', 'model_a'), row.get('model_b', 'model_b')],\n                responses=[oai_response_a, oai_response_b],\n                scores=scores,\n                meta=meta_with_winner\n            )\n            conversations.append(conversation)\n\n    elif method == \"single_model\":\n        all_models = df[\"model\"].unique().tolist()\n        # Expected columns: question_id, prompt, model, model_response, score, etc.\n\n        def parse_single_score_field(score_value):\n            \"\"\"Parse single model score field that might be a string, dict, number, or other type.\"\"\"\n            if isinstance(score_value, dict):\n                return score_value\n            elif isinstance(score_value, (int, float)):\n                return {'score': score_value}\n            elif isinstance(score_value, str) and score_value.strip():\n                try:\n                    import ast\n                    parsed = ast.literal_eval(score_value.strip())\n                    if isinstance(parsed, dict):\n                        return parsed\n                    elif isinstance(parsed, (int, float)):\n                        return {'score': parsed}\n                    else:\n                        return {'score': 0}\n                except (ValueError, SyntaxError):\n                    return {'score': 0}\n            else:\n                return {'score': 0}\n\n        # Convert to list of dicts once - MUCH faster than iterrows()\n        rows_list = df.to_dict('records')\n\n        # Parallelize OAI conversions for better performance\n        def _process_single_model_row(idx_row):\n            idx, row = idx_row\n            prompt = str(row.get('prompt', row.get('user_prompt', '')))\n            response = row.get('model_response', '')\n\n            # Convert response to OAI format if it's a string\n            oai_response, was_converted = check_and_convert_to_oai_format(prompt, response)\n\n            return idx, oai_response, row\n\n        # Pre-allocate results list\n        oai_results = [None] * len(rows_list)\n\n        # Process conversions in parallel\n        with ThreadPoolExecutor(max_workers=min(64, len(rows_list))) as executor:\n            futures = {executor.submit(_process_single_model_row, (idx, row)): idx\n                      for idx, row in enumerate(rows_list)}\n            for future in as_completed(futures):\n                idx, oai_response, row = future.result()\n                oai_results[idx] = (oai_response, row)\n\n        # Now build conversations with pre-converted OAI responses\n        for idx, result in enumerate(oai_results):\n            oai_response, row = result\n            scores = parse_single_score_field(row.get('score'))\n            prompt = str(row.get('prompt', row.get('user_prompt', '')))\n\n            conversation = ConversationRecord(\n                question_id=str(idx),  # Auto-generate as row index\n                prompt=prompt,\n                model=str(row.get('model', 'model')),\n                responses=oai_response,\n                scores=scores,\n                meta={k: v for k, v in row.items()\n                      if k not in ['question_id', 'prompt', 'user_prompt', 'model', 'model_response', 'score']}\n            )\n            conversations.append(conversation)\n    else:\n        raise ValueError(f\"Unknown method: {method}. Must be 'side_by_side' or 'single_model'\")\n\n    return cls(conversations=conversations, all_models=all_models)\n</code></pre>"},{"location":"api/reference/#stringsight.core.data_objects.PropertyDataset.save","title":"<code>save(path, format='json')</code>","text":"<p>Save the dataset to path in either <code>json</code>, <code>dataframe</code>, <code>parquet</code> or <code>pickle</code> format.</p> <p>The JSON variant produces a fully human-readable file while the pickle variant preserves the exact Python objects.</p> Source code in <code>stringsight/core/data_objects.py</code> <pre><code>def save(self, path: str, format: str = \"json\") -&gt; None:\n    \"\"\"Save the dataset to *path* in either ``json``, ``dataframe``, ``parquet`` or ``pickle`` format.\n\n    The JSON variant produces a fully human-readable file while the pickle\n    variant preserves the exact Python objects.\n    \"\"\"\n    import json, pickle, os\n\n    fmt = format.lower()\n    os.makedirs(os.path.dirname(path) or \".\", exist_ok=True)\n\n    if fmt == \"json\":\n        with open(path, \"w\", encoding=\"utf-8\") as f:\n            json.dump(self.to_serializable_dict(), f, ensure_ascii=False, indent=2)\n    elif fmt == \"dataframe\":\n        self.to_dataframe().to_json(path, orient=\"records\", lines=True)\n    elif fmt == \"parquet\":\n        self.to_dataframe().to_parquet(path)\n    elif fmt in {\"pkl\", \"pickle\"}:\n        with open(path, \"wb\") as f:\n            pickle.dump(self, f)\n    else:\n        raise ValueError(f\"Unsupported format: {format}. Use 'json' or 'pickle'.\")\n</code></pre>"},{"location":"api/reference/#stringsight.core.data_objects.PropertyDataset.load","title":"<code>load(path, format='json')</code>  <code>classmethod</code>","text":"<p>Load a dataset previously saved with :py:meth:<code>save</code>.</p> Source code in <code>stringsight/core/data_objects.py</code> <pre><code>@classmethod\ndef load(cls, path: str, format: str = \"json\") -&gt; \"PropertyDataset\":\n    \"\"\"Load a dataset previously saved with :py:meth:`save`.\"\"\"\n    import json, pickle\n\n    fmt = format.lower()\n    logger.info(f\"Loading dataset from {path} with format {fmt}\")\n    if fmt == \"json\":\n        logger.info(f\"Loading dataset from {path}\")\n        with open(path, \"r\") as f:\n            data = json.load(f)\n        logger.debug(f\"Data: {data.keys()}\")\n\n        # Expected format: dictionary with keys like \"conversations\", \"properties\", etc.\n        conversations = [ConversationRecord(**conv) for conv in data[\"conversations\"]]\n        properties = [Property(**prop) for prop in data.get(\"properties\", [])]\n\n        # Convert cluster data to Cluster objects\n        clusters = [Cluster(**cluster) for cluster in data.get(\"clusters\", [])]\n\n        model_stats = data.get(\"model_stats\", {})\n        all_models = data.get(\"all_models\", PropertyDataset.get_all_models(conversations))\n        return cls(conversations=conversations, properties=properties, clusters=clusters, model_stats=model_stats, all_models=all_models)\n    elif fmt == \"dataframe\":\n        # Handle dataframe format - this creates a list of objects when saved\n        import pandas as pd\n        try:\n            # Try to load as JSON Lines first\n            df = pd.read_json(path, orient=\"records\", lines=True)\n        except ValueError:\n            # If that fails, try regular JSON\n            df = pd.read_json(path, orient=\"records\")\n\n        # Detect method based on columns\n        method = \"side_by_side\" if {\"model_a\", \"model_b\"}.issubset(df.columns) else \"single_model\"\n\n        return cls.from_dataframe(df, method=method)\n    elif fmt in {\"pkl\", \"pickle\"}:\n        with open(path, \"rb\") as f:\n            obj = pickle.load(f)\n        if not isinstance(obj, cls):\n            raise TypeError(\"Pickle file does not contain a PropertyDataset object\")\n        return obj\n    elif fmt == \"parquet\":\n        # Load DataFrame and reconstruct minimal PropertyDataset with clusters\n        import pandas as pd\n        df = pd.read_parquet(path)\n\n        # Attempt to detect method\n        method = \"side_by_side\" if {\"model_a\", \"model_b\"}.issubset(df.columns) else \"single_model\"\n\n        dataset = cls.from_dataframe(df, method=method)\n\n        # Reconstruct Cluster objects if cluster columns are present\n        required_cols = {\n            \"cluster_id\",\n            \"cluster_label\",\n            \"property_description\",\n        }\n        if required_cols.issubset(df.columns):\n            clusters_dict = {}\n            for _, row in df.iterrows():\n                cid = row[\"cluster_id\"]\n                if pd.isna(cid):\n                    continue\n                cluster = clusters_dict.setdefault(\n                    cid,\n                    Cluster(\n                        id=int(cid),\n                        label=row.get(\"cluster_label\", str(cid)),\n                        size=0,\n                    ),\n                )\n                cluster.size += 1\n                pd_desc = row.get(\"property_description\")\n                if pd_desc and pd_desc not in cluster.property_descriptions:\n                    cluster.property_descriptions.append(pd_desc)\n                cluster.question_ids.append(str(row.get(\"question_id\", \"\")))\n\n            dataset.clusters = list(clusters_dict.values())\n\n        return dataset\n    else:\n        raise ValueError(f\"Unsupported format: {format}. Use 'json', 'dataframe', 'parquet', or 'pickle'.\") \n</code></pre>"},{"location":"api/reference/#stringsight.core.data_objects.PropertyDataset.to_dataframe","title":"<code>to_dataframe(type='all', method='side_by_side')</code>","text":"<p>Convert PropertyDataset back to DataFrame format.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with original data plus extracted properties and clusters</p> Source code in <code>stringsight/core/data_objects.py</code> <pre><code>def to_dataframe(self, type: str = \"all\", method: str = \"side_by_side\") -&gt; pd.DataFrame:\n    \"\"\"\n    Convert PropertyDataset back to DataFrame format.\n\n    Returns:\n        DataFrame with original data plus extracted properties and clusters\n    \"\"\"\n\n    assert type in [\"base\", \"properties\", \"clusters\", \"all\"], f\"Invalid type: {type}. Must be 'all' or 'base'\"\n    # Start with conversation data\n    rows = []\n    for conv in self.conversations:\n        if isinstance(conv.model, str):\n            base_row = {\n                'question_id': conv.question_id,\n                'prompt': conv.prompt,\n                'model': conv.model,\n                'model_response': conv.responses,\n                'score': conv.scores,\n                **conv.meta\n            }\n        elif isinstance(conv.model, list):\n            # Side-by-side format: scores stored as [scores_a, scores_b]\n            if isinstance(conv.scores, list) and len(conv.scores) == 2:\n                scores_a, scores_b = conv.scores[0], conv.scores[1]\n            else:\n                # Fallback if scores isn't properly formatted\n                scores_a, scores_b = {}, {}\n\n            base_row = {\n                'question_id': conv.question_id,\n                'prompt': conv.prompt,\n                'model_a': conv.model[0],\n                'model_b': conv.model[1],\n                'model_a_response': conv.responses[0],\n                'model_b_response': conv.responses[1],\n                'score_a': scores_a,\n                'score_b': scores_b,\n                'winner': conv.meta.get('winner'),  # Winner stored in meta\n                **{k: v for k, v in conv.meta.items() if k != 'winner'}  # Exclude winner from other meta\n            }\n        else:\n            raise ValueError(f\"Invalid model type: {type(conv.model)}. Must be str or list.\")\n\n        rows.append(base_row)\n\n    df = pd.DataFrame(rows)\n    logger.debug(f\"Original unique questions: {df.question_id.nunique()}\")\n\n    # Add properties if they exist\n    if self.properties and type in [\"all\", \"properties\", \"clusters\"]:\n        # Create a mapping from (question_id, model) to properties\n        prop_map = {}\n        for prop in self.properties:\n            key = (prop.question_id, prop.model)\n            if key not in prop_map:\n                prop_map[key] = []\n            prop_map[key].append(prop)\n\n        # create property df\n        prop_df = pd.DataFrame([p.to_dict() for p in self.properties])\n        logger.debug(f\"len of base df {len(df)}\")\n        if \"model_a\" in df.columns and \"model_b\" in df.columns:\n            # For side-by-side inputs, merge properties by question_id (both models share the question)\n            df = df.merge(prop_df, on=[\"question_id\"], how=\"left\")\n            # Deduplicate by property id when available\n            if \"id\" in df.columns:\n                df = df.drop_duplicates(subset=\"id\")\n                # Alias for clarity: id refers to property id\n                if \"property_id\" not in df.columns:\n                    df[\"property_id\"] = df[\"id\"]\n        else:\n            # CHANGE: Use left join to preserve all conversations, including those without properties\n            # Don't drop duplicates to ensure conversations without properties are preserved\n            df = df.merge(prop_df, on=[\"question_id\", \"model\"], how=\"left\")\n            # Alias when present\n            if \"id\" in df.columns and \"property_id\" not in df.columns:\n                df[\"property_id\"] = df[\"id\"]\n        logger.debug(f\"len of df after merge with properties {len(df)}\")\n\n        # ------------------------------------------------------------------\n        # Ensure `model` column is present (avoid _x / _y duplicates)\n        # ------------------------------------------------------------------\n        if \"model\" not in df.columns:\n            if \"model_y\" in df.columns:\n                print(f\"df.model_y.value_counts(): {df.model_y.value_counts()}\")\n            if \"model_x\" in df.columns:\n                print(f\"df.model_x.value_counts(): {df.model_x.value_counts()}\")\n            if \"model_x\" in df.columns or \"model_y\" in df.columns:\n                df[\"model\"] = df.get(\"model_y\").combine_first(df.get(\"model_x\"))\n                df.drop(columns=[c for c in [\"model_x\", \"model_y\"] if c in df.columns], inplace=True)\n\n    # Only print model value counts if the column exists\n    if \"model\" in df.columns:\n        logger.debug(f\"df.model.value_counts() NEW: {df.model.value_counts()}\")\n    logger.debug(f\"total questions: {df.question_id.nunique()}\")\n\n    if self.clusters and type in [\"all\", \"clusters\"]:\n        # If cluster columns already exist (e.g. after reload from parquet)\n        # skip the merge to avoid duplicate _x / _y columns.\n        if \"cluster_id\" not in df.columns:\n            cluster_df = pd.DataFrame([c.to_dict() for c in self.clusters])\n            cluster_df.rename(\n                columns={\n                    \"id\": \"cluster_id\",\n                    \"label\": \"cluster_label\",\n                    \"size\": \"cluster_size\",\n                    \"property_descriptions\": \"property_description\",\n                },\n                inplace=True,\n            )\n            # Explode aligned list columns so each row maps to a single property\n            # Explode only aligned columns to avoid mismatched element counts\n            list_cols = [\n                col for col in [\n                    \"property_description\",\n                    \"question_ids\",\n                ] if col in cluster_df.columns\n            ]\n            if list_cols:\n                try:\n                    cluster_df = cluster_df.explode(list_cols, ignore_index=True)\n                except (TypeError, ValueError):\n                    # Fallback: explode sequentially to avoid alignment constraints\n                    for col in list_cols:\n                        cluster_df = cluster_df.explode(col, ignore_index=True)\n            df = df.merge(cluster_df, on=[\"property_description\"], how=\"left\")\n\n    # CHANGE: Handle conversations without properties by creating a \"No properties\" cluster\n    # This ensures all conversations are considered in metrics calculation\n    if type in [\"all\", \"clusters\"]:\n        # Identify rows without properties (no property_description or it's NaN)\n        mask_no_properties = df[\"property_description\"].isna() | (df[\"property_description\"].astype(str).str.strip() == \"\")\n\n        # Only add the synthetic cluster if *all* rows lack a property description.\n        # If at least one property exists, we skip to avoid mixing partially\n        # processed conversations into a global \"No properties\" cluster.\n\n        if mask_no_properties.all():\n            logger.info(\"All conversations lack properties \u2013 creating 'No properties' cluster\")\n\n            # Fill in missing data for conversations without properties\n            df.loc[mask_no_properties, \"property_description\"] = \"No properties\"\n            df.loc[mask_no_properties, \"cluster_id\"] = -2  # Use -2 since -1 is for outliers\n            df.loc[mask_no_properties, \"cluster_label\"] = \"No properties\"\n\n            # Handle missing scores for conversations without properties\n            mask_no_score = mask_no_properties &amp; (df[\"score\"].isna() | (df[\"score\"] == {}))\n            if mask_no_score.any():\n                df.loc[mask_no_score, \"score\"] = df.loc[mask_no_score, \"score\"].apply(lambda x: {\"score\": 0} if pd.isna(x) or x == {} else x)\n\n    return df\n</code></pre>"},{"location":"api/reference/#conversationrecord","title":"ConversationRecord","text":""},{"location":"api/reference/#stringsight.core.data_objects.ConversationRecord","title":"<code>ConversationRecord</code>  <code>dataclass</code>","text":"<p>A single conversation with prompt, responses, and metadata.</p> Source code in <code>stringsight/core/data_objects.py</code> <pre><code>@dataclass\nclass ConversationRecord:\n    \"\"\"A single conversation with prompt, responses, and metadata.\"\"\"\n    question_id: str \n    prompt: str\n    model: str | List[str]  # model name(s) - single string or list for side-by-side comparisons\n    responses: str | List[str] # model response(s) - single string or list for side-by-side comparisons\n    scores: Dict[str, Any] | List[Dict[str, Any]]     # For single model: {score_name: score_value}. For side-by-side: [scores_a, scores_b] \n    meta: Dict[str, Any] = field(default_factory=dict)  # winner, language, etc. (winner stored here for side-by-side)\n\n    def __post_init__(self):\n        \"\"\"Migrate legacy score formats to the new list format for side-by-side.\"\"\"\n        # Handle migration of score_a/score_b from meta field to scores list for side-by-side\n        if isinstance(self.model, list) and len(self.model) == 2:\n            # Check if scores is empty and we have score_a/score_b in meta\n            if (not self.scores or self.scores == {}) and ('score_a' in self.meta and 'score_b' in self.meta):\n                # Migrate scores from meta to scores field\n                scores_a = self.meta.pop('score_a', {})\n                scores_b = self.meta.pop('score_b', {})\n                self.scores = [scores_a, scores_b]\n</code></pre>"},{"location":"api/reference/#stringsight.core.data_objects.ConversationRecord.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Migrate legacy score formats to the new list format for side-by-side.</p> Source code in <code>stringsight/core/data_objects.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Migrate legacy score formats to the new list format for side-by-side.\"\"\"\n    # Handle migration of score_a/score_b from meta field to scores list for side-by-side\n    if isinstance(self.model, list) and len(self.model) == 2:\n        # Check if scores is empty and we have score_a/score_b in meta\n        if (not self.scores or self.scores == {}) and ('score_a' in self.meta and 'score_b' in self.meta):\n            # Migrate scores from meta to scores field\n            scores_a = self.meta.pop('score_a', {})\n            scores_b = self.meta.pop('score_b', {})\n            self.scores = [scores_a, scores_b]\n</code></pre>"},{"location":"api/reference/#property","title":"Property","text":""},{"location":"api/reference/#stringsight.core.data_objects.Property","title":"<code>Property</code>  <code>dataclass</code>","text":"<p>An extracted behavioral property from a model response.</p> Source code in <code>stringsight/core/data_objects.py</code> <pre><code>@dataclass\nclass Property:\n    \"\"\"An extracted behavioral property from a model response.\"\"\"\n    id: str # unique id for the property\n    question_id: str\n    model: str\n    # Parsed fields (filled by LLMJsonParser)\n    property_description: Optional[str] = None\n    category: Optional[str] = None\n    reason: Optional[str] = None\n    evidence: Optional[str] = None\n    behavior_type: Optional[str] = None # Positive|Negative (non-critical)|Negative (critical)|Style\n\n    # Raw LLM response (captured by extractor before parsing)\n    raw_response: Optional[str] = None\n    contains_errors: Optional[bool] = None\n    unexpected_behavior: Optional[bool] = None\n    meta: Dict[str, Any] = field(default_factory=dict) # all other metadata\n\n    def to_dict(self):\n        return asdict(self)\n\n    def __post_init__(self):\n        \"\"\"Validate property fields after initialization.\"\"\"\n        # Require that the model has been resolved to a known value\n        if isinstance(self.model, str) and self.model.lower() == \"unknown\":\n            raise ValueError(\"Property must have a known model; got 'unknown'.\")\n</code></pre>"},{"location":"api/reference/#stringsight.core.data_objects.Property.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Validate property fields after initialization.</p> Source code in <code>stringsight/core/data_objects.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Validate property fields after initialization.\"\"\"\n    # Require that the model has been resolved to a known value\n    if isinstance(self.model, str) and self.model.lower() == \"unknown\":\n        raise ValueError(\"Property must have a known model; got 'unknown'.\")\n</code></pre>"},{"location":"api/reference/#cluster","title":"Cluster","text":""},{"location":"api/reference/#stringsight.core.data_objects.Cluster","title":"<code>Cluster</code>  <code>dataclass</code>","text":"<p>A cluster of properties.</p> Source code in <code>stringsight/core/data_objects.py</code> <pre><code>@dataclass\nclass Cluster:\n    \"\"\"A cluster of properties.\"\"\"\n    id: str # cluster id\n    label: str # cluster label\n    size: int # cluster size\n    property_descriptions: List[str] = field(default_factory=list) # property descriptions in the cluster\n    property_ids: List[str] = field(default_factory=list) # property ids in the cluster\n    question_ids: List[str] = field(default_factory=list) # ids of the conversations in the cluster\n    meta: Dict[str, Any] = field(default_factory=dict) # all other metadata\n\n    def to_dict(self):\n        return asdict(self)\n\n    def to_sample_dict(self, n: int = 5):\n        \"\"\"Return a dictionary that samples n property descriptions and ids from the cluster.\"\"\"\n        return {\n            \"id\": self.id,\n            \"label\": self.label,\n            \"size\": self.size,\n            \"property_descriptions\": random.sample(self.property_descriptions, n),\n            \"question_ids\": random.sample(self.question_ids, n),\n            \"property_ids\": random.sample(self.property_ids, n),\n            \"meta\": self.meta,\n        }\n</code></pre>"},{"location":"api/reference/#stringsight.core.data_objects.Cluster.to_sample_dict","title":"<code>to_sample_dict(n=5)</code>","text":"<p>Return a dictionary that samples n property descriptions and ids from the cluster.</p> Source code in <code>stringsight/core/data_objects.py</code> <pre><code>def to_sample_dict(self, n: int = 5):\n    \"\"\"Return a dictionary that samples n property descriptions and ids from the cluster.\"\"\"\n    return {\n        \"id\": self.id,\n        \"label\": self.label,\n        \"size\": self.size,\n        \"property_descriptions\": random.sample(self.property_descriptions, n),\n        \"question_ids\": random.sample(self.question_ids, n),\n        \"property_ids\": random.sample(self.property_ids, n),\n        \"meta\": self.meta,\n    }\n</code></pre>"},{"location":"api/reference/#pipeline-components","title":"Pipeline Components","text":""},{"location":"api/reference/#pipelinestage","title":"PipelineStage","text":""},{"location":"api/reference/#stringsight.core.stage.PipelineStage","title":"<code>PipelineStage</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for all pipeline stages.</p> <p>Each stage takes a PropertyDataset as input and returns a PropertyDataset as output. This allows stages to be composed into pipelines.</p> Source code in <code>stringsight/core/stage.py</code> <pre><code>class PipelineStage(ABC):\n    \"\"\"\n    Abstract base class for all pipeline stages.\n\n    Each stage takes a PropertyDataset as input and returns a PropertyDataset as output.\n    This allows stages to be composed into pipelines.\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        \"\"\"Initialize the stage with configuration parameters and propagate to mixins.\"\"\"\n        # Store config before passing to mixins (copy to avoid mutating original)\n        self.config = dict(kwargs)\n        self.name = self.__class__.__name__\n\n        # Call next __init__ in MRO \u2013 no kwargs so they don't reach object.__init__\n        super().__init__()\n\n    @abstractmethod\n    def run(self, data: PropertyDataset) -&gt; PropertyDataset:\n        \"\"\"\n        Process the input data and return the modified data.\n\n        Args:\n            data: Input PropertyDataset\n\n        Returns:\n            Modified PropertyDataset\n        \"\"\"\n        pass\n\n    def validate_input(self, data: PropertyDataset) -&gt; None:\n        \"\"\"\n        Validate that the input data meets the requirements for this stage.\n\n        Args:\n            data: Input PropertyDataset\n\n        Raises:\n            ValueError: If the input data is invalid\n        \"\"\"\n        if not isinstance(data, PropertyDataset):\n            raise ValueError(f\"Input must be a PropertyDataset, got {type(data)}\")\n\n    def validate_output(self, data: PropertyDataset) -&gt; None:\n        \"\"\"\n        Validate that the output data is valid.\n\n        Args:\n            data: Output PropertyDataset\n\n        Raises:\n            ValueError: If the output data is invalid\n        \"\"\"\n        if not isinstance(data, PropertyDataset):\n            raise ValueError(f\"Output must be a PropertyDataset, got {type(data)}\")\n\n    def __call__(self, data: PropertyDataset) -&gt; PropertyDataset:\n        \"\"\"\n        Convenience method to run the stage.\n\n        This allows stages to be called directly: stage(data)\n        \"\"\"\n        self.validate_input(data)\n        result = self.run(data)\n        self.validate_output(result)\n        return result\n\n    def __repr__(self) -&gt; str:\n        return f\"{self.name}({self.config})\"\n</code></pre>"},{"location":"api/reference/#stringsight.core.stage.PipelineStage.__init__","title":"<code>__init__(*args, **kwargs)</code>","text":"<p>Initialize the stage with configuration parameters and propagate to mixins.</p> Source code in <code>stringsight/core/stage.py</code> <pre><code>def __init__(self, *args, **kwargs):\n    \"\"\"Initialize the stage with configuration parameters and propagate to mixins.\"\"\"\n    # Store config before passing to mixins (copy to avoid mutating original)\n    self.config = dict(kwargs)\n    self.name = self.__class__.__name__\n\n    # Call next __init__ in MRO \u2013 no kwargs so they don't reach object.__init__\n    super().__init__()\n</code></pre>"},{"location":"api/reference/#stringsight.core.stage.PipelineStage.run","title":"<code>run(data)</code>  <code>abstractmethod</code>","text":"<p>Process the input data and return the modified data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>PropertyDataset</code> <p>Input PropertyDataset</p> required <p>Returns:</p> Type Description <code>PropertyDataset</code> <p>Modified PropertyDataset</p> Source code in <code>stringsight/core/stage.py</code> <pre><code>@abstractmethod\ndef run(self, data: PropertyDataset) -&gt; PropertyDataset:\n    \"\"\"\n    Process the input data and return the modified data.\n\n    Args:\n        data: Input PropertyDataset\n\n    Returns:\n        Modified PropertyDataset\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/reference/#stringsight.core.stage.PipelineStage.validate_input","title":"<code>validate_input(data)</code>","text":"<p>Validate that the input data meets the requirements for this stage.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>PropertyDataset</code> <p>Input PropertyDataset</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the input data is invalid</p> Source code in <code>stringsight/core/stage.py</code> <pre><code>def validate_input(self, data: PropertyDataset) -&gt; None:\n    \"\"\"\n    Validate that the input data meets the requirements for this stage.\n\n    Args:\n        data: Input PropertyDataset\n\n    Raises:\n        ValueError: If the input data is invalid\n    \"\"\"\n    if not isinstance(data, PropertyDataset):\n        raise ValueError(f\"Input must be a PropertyDataset, got {type(data)}\")\n</code></pre>"},{"location":"api/reference/#stringsight.core.stage.PipelineStage.validate_output","title":"<code>validate_output(data)</code>","text":"<p>Validate that the output data is valid.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>PropertyDataset</code> <p>Output PropertyDataset</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the output data is invalid</p> Source code in <code>stringsight/core/stage.py</code> <pre><code>def validate_output(self, data: PropertyDataset) -&gt; None:\n    \"\"\"\n    Validate that the output data is valid.\n\n    Args:\n        data: Output PropertyDataset\n\n    Raises:\n        ValueError: If the output data is invalid\n    \"\"\"\n    if not isinstance(data, PropertyDataset):\n        raise ValueError(f\"Output must be a PropertyDataset, got {type(data)}\")\n</code></pre>"},{"location":"api/reference/#stringsight.core.stage.PipelineStage.__call__","title":"<code>__call__(data)</code>","text":"<p>Convenience method to run the stage.</p> <p>This allows stages to be called directly: stage(data)</p> Source code in <code>stringsight/core/stage.py</code> <pre><code>def __call__(self, data: PropertyDataset) -&gt; PropertyDataset:\n    \"\"\"\n    Convenience method to run the stage.\n\n    This allows stages to be called directly: stage(data)\n    \"\"\"\n    self.validate_input(data)\n    result = self.run(data)\n    self.validate_output(result)\n    return result\n</code></pre>"},{"location":"api/reference/#pipeline","title":"Pipeline","text":""},{"location":"api/reference/#stringsight.pipeline.Pipeline","title":"<code>Pipeline</code>","text":"<p>               Bases: <code>LoggingMixin</code>, <code>TimingMixin</code>, <code>ErrorHandlingMixin</code>, <code>WandbMixin</code></p> <p>A pipeline for processing data through multiple stages.</p> <p>The Pipeline class coordinates the execution of multiple pipeline stages, handles error recovery, and provides logging and timing information.</p> Source code in <code>stringsight/pipeline.py</code> <pre><code>class Pipeline(LoggingMixin, TimingMixin, ErrorHandlingMixin, WandbMixin):\n    \"\"\"\n    A pipeline for processing data through multiple stages.\n\n    The Pipeline class coordinates the execution of multiple pipeline stages,\n    handles error recovery, and provides logging and timing information.\n    \"\"\"\n\n    def __init__(self, name: str, stages: List[PipelineStage] = None, **kwargs):\n        \"\"\"\n        Initialize a new Pipeline.\n\n        Args:\n            name: Name of the pipeline\n            stages: List of pipeline stages to execute\n            **kwargs: Additional configuration options\n        \"\"\"\n        # Set name first, before calling parent __init__ methods that might use it\n        self.name = name\n        self.stages = stages or []\n        self.stage_times = {}\n        self.stage_errors = {}\n        # Store output directory (if any) so that we can automatically persist\n        # intermediate pipeline results after each stage.  This enables tooling\n        # such as compute_metrics_only() to pick up from any point in the\n        # pipeline without the caller having to remember to save explicitly.\n        self.output_dir = kwargs.get('output_dir')\n\n        # Now call parent __init__ methods safely\n        super().__init__(**kwargs)\n\n        # Initialize wandb if enabled (after all parent inits are done)\n        if hasattr(self, 'use_wandb') and self.use_wandb:\n            self.init_wandb()\n\n        # Mark all stages as using the same wandb run\n        for stage in self.stages:\n            if hasattr(stage, 'use_wandb') and stage.use_wandb:\n                stage._wandb_ok = True  # Mark that wandb is available\n\n    def add_stage(self, stage: PipelineStage) -&gt; None:\n        \"\"\"Add a stage to the end of the pipeline.\"\"\"\n        self.stages.append(stage)\n\n        # Mark the new stage as using the same wandb run if wandb is enabled\n        if hasattr(self, 'use_wandb') and self.use_wandb and hasattr(stage, 'use_wandb') and stage.use_wandb:\n            stage._wandb_ok = True  # Mark that wandb is available\n\n    def insert_stage(self, index: int, stage: PipelineStage) -&gt; None:\n        \"\"\"Insert a stage at a specific position in the pipeline.\"\"\"\n        self.stages.insert(index, stage)\n\n        # Mark the inserted stage as using the same wandb run if wandb is enabled\n        if hasattr(self, 'use_wandb') and self.use_wandb and hasattr(stage, 'use_wandb') and stage.use_wandb:\n            stage._wandb_ok = True  # Mark that wandb is available\n\n    def remove_stage(self, index: int) -&gt; PipelineStage:\n        \"\"\"Remove and return a stage at a specific position.\"\"\"\n        return self.stages.pop(index)\n\n    def run(self, data: PropertyDataset) -&gt; PropertyDataset:\n        \"\"\"\n        Execute all stages in the pipeline.\n\n        Args:\n            data: Input PropertyDataset\n\n        Returns:\n            PropertyDataset after processing through all stages\n        \"\"\"\n        self.log(f\"Starting pipeline '{self.name}' with {len(self.stages)} stages\")\n        self.start_timer()\n\n        # Count initial models\n        initial_models = set()\n        for conv in data.conversations:\n            if isinstance(conv.model, list):\n                initial_models.update(conv.model)\n            else:\n                initial_models.add(conv.model)\n\n        print(f\"\\n\ud83d\ude80 Starting pipeline '{self.name}'\")\n        print(f\"   \u2022 Input conversations: {len(data.conversations)}\")\n        print(f\"   \u2022 Input models: {len(initial_models)}\")\n        if len(initial_models) &lt;= 20:\n            model_list = sorted(list(initial_models))\n            print(f\"   \u2022 Model names: {', '.join(model_list)}\")\n        print()\n\n        current_data = data\n\n        for i, stage in enumerate(self.stages):\n            stage_start_time = time.time()\n\n            # try:\n            self.log(f\"Running stage {i+1}/{len(self.stages)}: {stage.name}\")\n\n            # Execute the stage\n            current_data = stage(current_data)\n\n            # Track timing\n            stage_execution_time = time.time() - stage_start_time\n            self.stage_times[stage.name] = stage_execution_time\n\n            self.log(f\"Stage {stage.name} completed in {stage_execution_time:.2f}s\")\n\n            # Log stage-specific metrics\n            self._log_stage_metrics(stage, current_data)\n\n            # --------------------------------------------------------------\n            # \ud83d\udcdd  Auto-save full dataset snapshot after each stage\n            # --------------------------------------------------------------\n            if getattr(self, \"output_dir\", None):\n                from pathlib import Path\n                import os\n                import json\n\n                # Ensure the directory exists (mkdir \u2011p semantics)\n                Path(self.output_dir).mkdir(parents=True, exist_ok=True)\n\n                # File name pattern: full_dataset_after_&lt;idx&gt;_&lt;stage&gt;.json\n                # snapshot_name = (\n                #     f\"full_dataset_after_{i+1}_{stage.name.replace(' ', '_').lower()}.json\"\n                # )\n                snapshot_name = f\"full_dataset.json\"\n                snapshot_path = os.path.join(self.output_dir, snapshot_name)\n\n                # Persist using the JSON format for maximum portability\n                current_data.save(snapshot_path)\n\n                # Also save conversations separately as JSONL\n                conversation_path = os.path.join(self.output_dir, \"conversation.jsonl\")\n                with open(conversation_path, 'w', encoding='utf-8') as f:\n                    for conv in current_data.conversations:\n                        # Build base conversation dict\n                        conv_dict = {\n                            \"question_id\": conv.question_id,\n                            \"prompt\": conv.prompt,\n                        }\n\n                        # Handle side-by-side vs single model format\n                        if isinstance(conv.model, list):\n                            # Side-by-side format\n                            conv_dict[\"model_a\"] = conv.model[0]\n                            conv_dict[\"model_b\"] = conv.model[1]\n                            conv_dict[\"model_a_response\"] = conv.responses[0]\n                            conv_dict[\"model_b_response\"] = conv.responses[1]\n\n                            # Convert scores list to score_a/score_b\n                            if isinstance(conv.scores, list) and len(conv.scores) == 2:\n                                conv_dict[\"score_a\"] = conv.scores[0]\n                                conv_dict[\"score_b\"] = conv.scores[1]\n                            else:\n                                conv_dict[\"score_a\"] = {}\n                                conv_dict[\"score_b\"] = {}\n\n                            # Add meta fields (includes winner)\n                            conv_dict.update(conv.meta)\n                        else:\n                            # Single model format\n                            conv_dict[\"model\"] = conv.model\n                            conv_dict[\"model_response\"] = conv.responses\n                            conv_dict[\"score\"] = conv.scores\n\n                            # Add meta fields\n                            conv_dict.update(conv.meta)\n\n                        # Make JSON-safe and write\n                        conv_dict = current_data._json_safe(conv_dict)\n                        json.dump(conv_dict, f, ensure_ascii=False)\n                        f.write('\\n')\n\n                # Save properties separately as JSONL\n                if current_data.properties:\n                    properties_path = os.path.join(self.output_dir, \"properties.jsonl\")\n                    with open(properties_path, 'w', encoding='utf-8') as f:\n                        for prop in current_data.properties:\n                            prop_dict = current_data._json_safe(prop.to_dict())\n                            json.dump(prop_dict, f, ensure_ascii=False)\n                            f.write('\\n')\n\n                # Save clusters separately as JSONL\n                if current_data.clusters:\n                    clusters_path = os.path.join(self.output_dir, \"clusters.jsonl\")\n                    with open(clusters_path, 'w', encoding='utf-8') as f:\n                        for cluster in current_data.clusters:\n                            cluster_dict = current_data._json_safe(cluster.to_dict())\n                            json.dump(cluster_dict, f, ensure_ascii=False)\n                            f.write('\\n')\n\n                if getattr(self, \"verbose\", False):\n                    print(f\"   \u2022 Saved dataset snapshot: {snapshot_path}\")\n                    print(f\"   \u2022 Saved conversations: {conversation_path}\")\n                    if current_data.properties:\n                        print(f\"   \u2022 Saved properties: {properties_path}\")\n                    if current_data.clusters:\n                        print(f\"   \u2022 Saved clusters: {clusters_path}\")\n\n            # except Exception as e:\n            #     self.stage_errors[stage.name] = str(e)\n            #     self.handle_error(e, f\"stage {i+1} ({stage.name})\")\n\n        total_time = self.end_timer()\n        self.log(f\"Pipeline '{self.name}' completed in {total_time:.2f}s\")\n\n        # Print final summary\n        final_models = set()\n        for conv in current_data.conversations:\n            if isinstance(conv.model, list):\n                final_models.update(conv.model)\n            else:\n                final_models.add(conv.model)\n\n        print(f\"\\n\ud83c\udf89 Pipeline '{self.name}' completed!\")\n        print(f\"   \u2022 Total execution time: {total_time:.2f}s\")\n        print(f\"   \u2022 Final conversations: {len(current_data.conversations)}\")\n        print(f\"   \u2022 Final properties: {len(current_data.properties)}\")\n        print(f\"   \u2022 Final models: {len(final_models)}\")\n        if current_data.clusters:\n            print(f\"   \u2022 Final clusters: {len(current_data.clusters)}\")\n        if current_data.model_stats:\n            print(f\"   \u2022 Models with final stats: {len(current_data.model_stats)}\")\n        print()\n\n        return current_data\n\n    def _log_stage_metrics(self, stage: PipelineStage, data: PropertyDataset) -&gt; None:\n        \"\"\"Log metrics for a completed stage.\"\"\"\n        metrics = {\n            'conversations': len(data.conversations),\n            'properties': len(data.properties),\n            'clusters': len(data.clusters),\n            'models_in_stats': len(data.model_stats)\n        }\n\n        # Count unique models from conversations\n        unique_models = set()\n        for conv in data.conversations:\n            if isinstance(conv.model, list):\n                unique_models.update(conv.model)\n            else:\n                unique_models.add(conv.model)\n\n        total_models = len(unique_models)\n\n        # Add model count to metrics\n        metrics['total_models'] = total_models\n\n        self.log(f\"Stage {stage.name} metrics: {metrics}\")\n\n        # Print specific model count information\n        print(f\"\\n\ud83d\udcca Stage '{stage.name}' completed:\")\n        print(f\"   \u2022 Total conversations: {len(data.conversations)}\")\n        print(f\"   \u2022 Total properties: {len(data.properties)}\")\n        print(f\"   \u2022 Total models: {total_models}\")\n        if data.clusters:\n            print(f\"   \u2022 Total clusters: {len(data.clusters)}\")\n        if data.model_stats:\n            print(f\"   \u2022 Models with stats: {len(data.model_stats)}\")\n\n        # Show model names if verbose\n        if hasattr(self, 'verbose') and self.verbose and total_models &lt;= 20:\n            model_list = sorted(list(unique_models))\n            print(f\"   \u2022 Models: {', '.join(model_list)}\")\n\n        print()  # Add spacing\n\n        # Log to wandb as summary metrics (not regular metrics)\n        if hasattr(self, 'log_wandb'):\n            wandb_data = {f\"{stage.name}_{k}\": v for k, v in metrics.items()}\n            wandb_data[f\"{stage.name}_execution_time\"] = self.stage_times.get(stage.name, 0)\n            self.log_wandb(wandb_data, is_summary=True)\n\n    def log_final_summary(self) -&gt; None:\n        \"\"\"Log all accumulated summary metrics to wandb.\"\"\"\n        if hasattr(self, 'log_summary_metrics'):\n            # Add pipeline-level summary metrics\n            pipeline_summary = {\n                'pipeline_total_stages': len(self.stages),\n                'pipeline_total_time': self.get_execution_time(),\n                'pipeline_success': len(self.stage_errors) == 0,\n                'pipeline_error_count': len(self.stage_errors)\n            }\n            self.log_wandb(pipeline_summary, is_summary=True)\n\n            # Log all accumulated summary metrics\n            self.log_summary_metrics()\n\n            if hasattr(self, 'log'):\n                self.log(\"Logged final summary metrics to wandb\", level=\"debug\")\n\n    def get_stage_summary(self) -&gt; Dict[str, Any]:\n        \"\"\"Get a summary of pipeline execution.\"\"\"\n        return {\n            'total_stages': len(self.stages),\n            'total_time': self.get_execution_time(),\n            'stage_times': self.stage_times,\n            'stage_errors': self.stage_errors,\n            'success': len(self.stage_errors) == 0\n        }\n\n    def validate_pipeline(self) -&gt; List[str]:\n        \"\"\"\n        Validate that the pipeline is correctly configured.\n\n        Returns:\n            List of validation errors (empty if valid)\n        \"\"\"\n        errors = []\n\n        if not self.stages:\n            errors.append(\"Pipeline has no stages\")\n\n        for i, stage in enumerate(self.stages):\n            if not isinstance(stage, PipelineStage):\n                errors.append(f\"Stage {i} is not a PipelineStage instance\")\n\n        return errors\n\n    def __repr__(self) -&gt; str:\n        stage_names = [stage.name for stage in self.stages]\n        return f\"Pipeline({self.name}, stages={stage_names})\"\n\n    def __len__(self) -&gt; int:\n        return len(self.stages)\n\n    def __getitem__(self, index: int) -&gt; PipelineStage:\n        return self.stages[index]\n\n    def __iter__(self):\n        return iter(self.stages)\n</code></pre>"},{"location":"api/reference/#stringsight.pipeline.Pipeline.run","title":"<code>run(data)</code>","text":"<p>Execute all stages in the pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>PropertyDataset</code> <p>Input PropertyDataset</p> required <p>Returns:</p> Type Description <code>PropertyDataset</code> <p>PropertyDataset after processing through all stages</p> Source code in <code>stringsight/pipeline.py</code> <pre><code>def run(self, data: PropertyDataset) -&gt; PropertyDataset:\n    \"\"\"\n    Execute all stages in the pipeline.\n\n    Args:\n        data: Input PropertyDataset\n\n    Returns:\n        PropertyDataset after processing through all stages\n    \"\"\"\n    self.log(f\"Starting pipeline '{self.name}' with {len(self.stages)} stages\")\n    self.start_timer()\n\n    # Count initial models\n    initial_models = set()\n    for conv in data.conversations:\n        if isinstance(conv.model, list):\n            initial_models.update(conv.model)\n        else:\n            initial_models.add(conv.model)\n\n    print(f\"\\n\ud83d\ude80 Starting pipeline '{self.name}'\")\n    print(f\"   \u2022 Input conversations: {len(data.conversations)}\")\n    print(f\"   \u2022 Input models: {len(initial_models)}\")\n    if len(initial_models) &lt;= 20:\n        model_list = sorted(list(initial_models))\n        print(f\"   \u2022 Model names: {', '.join(model_list)}\")\n    print()\n\n    current_data = data\n\n    for i, stage in enumerate(self.stages):\n        stage_start_time = time.time()\n\n        # try:\n        self.log(f\"Running stage {i+1}/{len(self.stages)}: {stage.name}\")\n\n        # Execute the stage\n        current_data = stage(current_data)\n\n        # Track timing\n        stage_execution_time = time.time() - stage_start_time\n        self.stage_times[stage.name] = stage_execution_time\n\n        self.log(f\"Stage {stage.name} completed in {stage_execution_time:.2f}s\")\n\n        # Log stage-specific metrics\n        self._log_stage_metrics(stage, current_data)\n\n        # --------------------------------------------------------------\n        # \ud83d\udcdd  Auto-save full dataset snapshot after each stage\n        # --------------------------------------------------------------\n        if getattr(self, \"output_dir\", None):\n            from pathlib import Path\n            import os\n            import json\n\n            # Ensure the directory exists (mkdir \u2011p semantics)\n            Path(self.output_dir).mkdir(parents=True, exist_ok=True)\n\n            # File name pattern: full_dataset_after_&lt;idx&gt;_&lt;stage&gt;.json\n            # snapshot_name = (\n            #     f\"full_dataset_after_{i+1}_{stage.name.replace(' ', '_').lower()}.json\"\n            # )\n            snapshot_name = f\"full_dataset.json\"\n            snapshot_path = os.path.join(self.output_dir, snapshot_name)\n\n            # Persist using the JSON format for maximum portability\n            current_data.save(snapshot_path)\n\n            # Also save conversations separately as JSONL\n            conversation_path = os.path.join(self.output_dir, \"conversation.jsonl\")\n            with open(conversation_path, 'w', encoding='utf-8') as f:\n                for conv in current_data.conversations:\n                    # Build base conversation dict\n                    conv_dict = {\n                        \"question_id\": conv.question_id,\n                        \"prompt\": conv.prompt,\n                    }\n\n                    # Handle side-by-side vs single model format\n                    if isinstance(conv.model, list):\n                        # Side-by-side format\n                        conv_dict[\"model_a\"] = conv.model[0]\n                        conv_dict[\"model_b\"] = conv.model[1]\n                        conv_dict[\"model_a_response\"] = conv.responses[0]\n                        conv_dict[\"model_b_response\"] = conv.responses[1]\n\n                        # Convert scores list to score_a/score_b\n                        if isinstance(conv.scores, list) and len(conv.scores) == 2:\n                            conv_dict[\"score_a\"] = conv.scores[0]\n                            conv_dict[\"score_b\"] = conv.scores[1]\n                        else:\n                            conv_dict[\"score_a\"] = {}\n                            conv_dict[\"score_b\"] = {}\n\n                        # Add meta fields (includes winner)\n                        conv_dict.update(conv.meta)\n                    else:\n                        # Single model format\n                        conv_dict[\"model\"] = conv.model\n                        conv_dict[\"model_response\"] = conv.responses\n                        conv_dict[\"score\"] = conv.scores\n\n                        # Add meta fields\n                        conv_dict.update(conv.meta)\n\n                    # Make JSON-safe and write\n                    conv_dict = current_data._json_safe(conv_dict)\n                    json.dump(conv_dict, f, ensure_ascii=False)\n                    f.write('\\n')\n\n            # Save properties separately as JSONL\n            if current_data.properties:\n                properties_path = os.path.join(self.output_dir, \"properties.jsonl\")\n                with open(properties_path, 'w', encoding='utf-8') as f:\n                    for prop in current_data.properties:\n                        prop_dict = current_data._json_safe(prop.to_dict())\n                        json.dump(prop_dict, f, ensure_ascii=False)\n                        f.write('\\n')\n\n            # Save clusters separately as JSONL\n            if current_data.clusters:\n                clusters_path = os.path.join(self.output_dir, \"clusters.jsonl\")\n                with open(clusters_path, 'w', encoding='utf-8') as f:\n                    for cluster in current_data.clusters:\n                        cluster_dict = current_data._json_safe(cluster.to_dict())\n                        json.dump(cluster_dict, f, ensure_ascii=False)\n                        f.write('\\n')\n\n            if getattr(self, \"verbose\", False):\n                print(f\"   \u2022 Saved dataset snapshot: {snapshot_path}\")\n                print(f\"   \u2022 Saved conversations: {conversation_path}\")\n                if current_data.properties:\n                    print(f\"   \u2022 Saved properties: {properties_path}\")\n                if current_data.clusters:\n                    print(f\"   \u2022 Saved clusters: {clusters_path}\")\n\n        # except Exception as e:\n        #     self.stage_errors[stage.name] = str(e)\n        #     self.handle_error(e, f\"stage {i+1} ({stage.name})\")\n\n    total_time = self.end_timer()\n    self.log(f\"Pipeline '{self.name}' completed in {total_time:.2f}s\")\n\n    # Print final summary\n    final_models = set()\n    for conv in current_data.conversations:\n        if isinstance(conv.model, list):\n            final_models.update(conv.model)\n        else:\n            final_models.add(conv.model)\n\n    print(f\"\\n\ud83c\udf89 Pipeline '{self.name}' completed!\")\n    print(f\"   \u2022 Total execution time: {total_time:.2f}s\")\n    print(f\"   \u2022 Final conversations: {len(current_data.conversations)}\")\n    print(f\"   \u2022 Final properties: {len(current_data.properties)}\")\n    print(f\"   \u2022 Final models: {len(final_models)}\")\n    if current_data.clusters:\n        print(f\"   \u2022 Final clusters: {len(current_data.clusters)}\")\n    if current_data.model_stats:\n        print(f\"   \u2022 Models with final stats: {len(current_data.model_stats)}\")\n    print()\n\n    return current_data\n</code></pre>"},{"location":"api/reference/#stringsight.pipeline.Pipeline.add_stage","title":"<code>add_stage(stage)</code>","text":"<p>Add a stage to the end of the pipeline.</p> Source code in <code>stringsight/pipeline.py</code> <pre><code>def add_stage(self, stage: PipelineStage) -&gt; None:\n    \"\"\"Add a stage to the end of the pipeline.\"\"\"\n    self.stages.append(stage)\n\n    # Mark the new stage as using the same wandb run if wandb is enabled\n    if hasattr(self, 'use_wandb') and self.use_wandb and hasattr(stage, 'use_wandb') and stage.use_wandb:\n        stage._wandb_ok = True  # Mark that wandb is available\n</code></pre>"},{"location":"api/reference/#extractors","title":"Extractors","text":""},{"location":"api/reference/#get_extractor","title":"get_extractor()","text":""},{"location":"api/reference/#stringsight.extractors.get_extractor","title":"<code>get_extractor(model_name='gpt-4o-mini', system_prompt='one_sided_system_prompt', prompt_builder=None, temperature=0.6, top_p=0.95, max_tokens=16000, max_workers=64, include_scores_in_prompt=False, **kwargs)</code>","text":"<p>Factory function to get the appropriate extractor based on model name.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Name of the LLM to use for extraction</p> <code>'gpt-4o-mini'</code> <code>system_prompt</code> <code>str</code> <p>System prompt for property extraction</p> <code>'one_sided_system_prompt'</code> <code>prompt_builder</code> <code>Optional[Callable]</code> <p>Optional custom prompt builder function</p> <code>None</code> <code>temperature</code> <code>float</code> <p>Temperature for LLM</p> <code>0.6</code> <code>top_p</code> <code>float</code> <p>Top-p for LLM  </p> <code>0.95</code> <code>max_tokens</code> <code>int</code> <p>Max tokens for LLM</p> <code>16000</code> <code>max_workers</code> <code>int</code> <p>Max parallel workers for API calls</p> <code>64</code> <code>**kwargs</code> <p>Additional configuration</p> <code>{}</code> <p>Returns:</p> Type Description <code>PipelineStage</code> <p>Configured extractor stage</p> Source code in <code>stringsight/extractors/__init__.py</code> <pre><code>def get_extractor(\n    model_name: str = \"gpt-4o-mini\",\n    system_prompt: str = \"one_sided_system_prompt\",\n    prompt_builder: Optional[Callable] = None,\n    temperature: float = 0.6,\n    top_p: float = 0.95,\n    max_tokens: int = 16000,\n    max_workers: int = 64,\n    include_scores_in_prompt: bool = False,\n    **kwargs\n) -&gt; PipelineStage:\n    \"\"\"\n    Factory function to get the appropriate extractor based on model name.\n\n    Args:\n        model_name: Name of the LLM to use for extraction\n        system_prompt: System prompt for property extraction\n        prompt_builder: Optional custom prompt builder function\n        temperature: Temperature for LLM\n        top_p: Top-p for LLM  \n        max_tokens: Max tokens for LLM\n        max_workers: Max parallel workers for API calls\n        **kwargs: Additional configuration\n\n    Returns:\n        Configured extractor stage\n    \"\"\"\n\n    # Route common hosted providers through the LiteLLM-backed extractor.\n    lower_name = model_name.lower().strip()\n    litellm_prefixes = (\n        \"gpt\",           # OpenAI (bare)\n        \"openai/\",      # OpenAI (provider-prefixed)\n        \"claude\",       # Anthropic (bare)\n        \"anthropic/\",   # Anthropic (provider-prefixed)\n        \"gemini\",       # Google Gemini (bare)\n        \"google/\",      # Google (provider-prefixed)\n        \"vertex\",       # Vertex AI (provider-prefixed is usually vertex/..., allow bare prefix)\n        \"azure/\",       # Azure OpenAI\n        \"cohere/\",      # Cohere\n        \"mistral/\",     # Mistral hosted\n        \"bedrock/\",     # AWS Bedrock\n    )\n\n    if lower_name.startswith(litellm_prefixes):\n        from .openai import OpenAIExtractor\n        return OpenAIExtractor(\n            model=model_name,\n            system_prompt=system_prompt,\n            prompt_builder=prompt_builder,\n            temperature=temperature,\n            top_p=top_p,\n            max_tokens=max_tokens,\n            max_workers=max_workers,\n            include_scores_in_prompt=include_scores_in_prompt,\n            **kwargs\n        )\n    else:\n        from .vllm import VLLMExtractor\n        return VLLMExtractor(\n            model=model_name,\n            system_prompt=system_prompt,\n            prompt_builder=prompt_builder,\n            temperature=temperature,\n            top_p=top_p,\n            max_tokens=max_tokens,\n            **kwargs\n        )\n</code></pre>"},{"location":"api/reference/#openaiextractor","title":"OpenAIExtractor","text":""},{"location":"api/reference/#stringsight.extractors.openai.OpenAIExtractor","title":"<code>OpenAIExtractor</code>","text":"<p>               Bases: <code>LoggingMixin</code>, <code>TimingMixin</code>, <code>ErrorHandlingMixin</code>, <code>WandbMixin</code>, <code>PipelineStage</code></p> <p>Extract behavioral properties using OpenAI models.</p> <p>This stage takes conversations and extracts structured properties describing model behaviors, differences, and characteristics.</p> Source code in <code>stringsight/extractors/openai.py</code> <pre><code>class OpenAIExtractor(LoggingMixin, TimingMixin, ErrorHandlingMixin, WandbMixin, PipelineStage):\n    \"\"\"\n    Extract behavioral properties using OpenAI models.\n\n    This stage takes conversations and extracts structured properties describing\n    model behaviors, differences, and characteristics.\n    \"\"\"\n\n    def __init__(\n        self,\n        model: str = \"gpt-4.1\",\n        system_prompt: str = \"one_sided_system_prompt_no_examples\",\n        prompt_builder: Optional[Callable] = None,\n        temperature: float = 0.7,\n        top_p: float = 0.95,\n        max_tokens: int = 16000,\n        max_workers: int = 64,\n        include_scores_in_prompt: bool = False,\n        **kwargs\n    ):\n        \"\"\"\n        Initialize the OpenAI extractor.\n\n        Args:\n            model: OpenAI model name (e.g., \"gpt-4o-mini\")\n            system_prompt: System prompt for property extraction\n            prompt_builder: Optional custom prompt builder function\n            temperature: Temperature for LLM\n            top_p: Top-p for LLM\n            max_tokens: Max tokens for LLM\n            max_workers: Max parallel workers for API calls\n            include_scores_in_prompt: Whether to include scores in prompts\n            **kwargs: Additional configuration\n\n        Note:\n            Caching is handled automatically by UnifiedCache singleton.\n            Configure cache via STRINGSIGHT_* environment variables.\n        \"\"\"\n        super().__init__(**kwargs)\n        self.model = model\n        # Allow caller to pass the name of a prompt template or the prompt text itself\n        if isinstance(system_prompt, str) and hasattr(_extractor_prompts, system_prompt):\n            self.system_prompt = getattr(_extractor_prompts, system_prompt)\n        else:\n            self.system_prompt = system_prompt\n\n        self.prompt_builder = prompt_builder or self._default_prompt_builder\n        self.temperature = temperature\n        self.top_p = top_p\n        self.max_tokens = max_tokens\n        self.max_workers = max_workers\n        # Control whether to include numeric scores/winner context in prompts\n        self.include_scores_in_prompt = include_scores_in_prompt\n        # Note: Caching is handled by parallel_completions via UnifiedCache singleton\n\n    def run(self, data: PropertyDataset, progress_callback=None) -&gt; PropertyDataset:\n        \"\"\"Run OpenAI extraction for all conversations.\n\n        Each conversation is formatted with ``prompt_builder`` and sent to the\n        OpenAI model in parallel using a thread pool.  The raw LLM response is\n        stored inside a *placeholder* ``Property`` object (one per\n        conversation).  Down-stream stages (``LLMJsonParser``) will parse these\n        raw strings into fully-formed properties.\n\n        Args:\n            data: PropertyDataset with conversations to extract from\n            progress_callback: Optional callback(completed, total) for progress updates\n        \"\"\"\n\n        n_conv = len(data.conversations)\n        if n_conv == 0:\n            self.log(\"No conversations found \u2013 skipping extraction\")\n            return data\n\n        self.log(f\"Extracting properties from {n_conv} conversations using {self.model}\")\n\n\n        # ------------------------------------------------------------------\n        # 1\ufe0f\u20e3  Build user messages for every conversation (in parallel)\n        # ------------------------------------------------------------------\n        user_messages: List[Union[str, List[Dict[str, Any]]]] = [\"\"] * len(data.conversations)\n\n        def _build_prompt(idx: int, conv):\n            return idx, self.prompt_builder(conv)\n\n        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n            futures = {executor.submit(_build_prompt, idx, conv): idx\n                      for idx, conv in enumerate(data.conversations)}\n            for future in as_completed(futures):\n                idx, prompt = future.result()\n                user_messages[idx] = prompt\n\n        # ------------------------------------------------------------------\n        # 2\ufe0f\u20e3  Call the OpenAI API in parallel batches via shared LLM utils\n        # ------------------------------------------------------------------\n        raw_responses = parallel_completions(\n            user_messages,\n            model=self.model,\n            system_prompt=self.system_prompt,\n            max_workers=self.max_workers,\n            temperature=self.temperature,\n            top_p=self.top_p,\n            max_tokens=self.max_tokens,\n            show_progress=True,\n            progress_desc=\"Property extraction\",\n            progress_callback=progress_callback\n        )\n\n        # ------------------------------------------------------------------\n        # 3\ufe0f\u20e3  Wrap raw responses in placeholder Property objects (filter None)\n        # ------------------------------------------------------------------\n        properties: List[Property] = []\n        skipped_count = 0\n        for conv, raw in zip(data.conversations, raw_responses):\n            # Skip failed LLM calls (None responses)\n            if raw is None:\n                skipped_count += 1\n                continue\n\n            # We don't yet know which model(s) the individual properties will\n            # belong to; parser will figure it out.  Use a placeholder model\n            # name so that validation passes.\n            prop = Property(\n                id=str(uuid.uuid4()),\n                question_id=conv.question_id,\n                model=conv.model,\n                raw_response=raw,\n            )\n            properties.append(prop)\n\n        if skipped_count &gt; 0:\n            self.log(f\"Skipped {skipped_count} conversations due to failed LLM calls\", level=\"warning\")\n\n        self.log(f\"Received {len(properties)} valid LLM responses\")\n\n\n        # Log to wandb if enabled\n        if hasattr(self, 'use_wandb') and self.use_wandb:\n            self._log_extraction_to_wandb(user_messages, raw_responses, data.conversations)\n\n        # ------------------------------------------------------------------\n        # 4\ufe0f\u20e3  Return updated dataset\n        # ------------------------------------------------------------------\n        return PropertyDataset(\n            conversations=data.conversations,\n            all_models=data.all_models,\n            properties=properties,\n            clusters=data.clusters,\n            model_stats=data.model_stats,\n        )\n\n    # ----------------------------------------------------------------------\n    # Helper methods\n    # ----------------------------------------------------------------------\n\n    # Legacy helpers removed in favor of centralized llm_utils\n\n    def _default_prompt_builder(self, conversation) -&gt; Union[str, List[Dict[str, Any]]]:\n        \"\"\"\n        Default prompt builder for side-by-side comparisons, with multimodal support.\n\n        Args:\n            conversation: ConversationRecord\n\n        Returns:\n            - If no images present: a plain string prompt (backwards compatible)\n            - If images present: a full OpenAI messages list including a single\n              user turn with ordered text/image parts (and a system turn)\n        \"\"\"\n        # Check if this is a side-by-side comparison or single model\n        if isinstance(conversation.model, list) and len(conversation.model) == 2:\n            # Side-by-side format\n            model_a, model_b = conversation.model\n            try:\n                responses_a = conversation.responses[0]\n                responses_b = conversation.responses[1]\n            except Exception as e:\n                raise ValueError(\n                    f\"Failed to access conversation responses for side-by-side format. \"\n                    f\"Expected two response lists. Error: {str(e)}\"\n                )\n\n            # Normalize both to our internal segments format\n            conv_a = openai_messages_to_conv(responses_a) if isinstance(responses_a, list) else responses_a\n            conv_b = openai_messages_to_conv(responses_b) if isinstance(responses_b, list) else responses_b\n\n            has_images = self._conversation_has_images(conv_a) or self._conversation_has_images(conv_b)\n\n            if has_images:\n                return self._build_side_by_side_messages(model_a, model_b, conv_a, conv_b)\n\n            # No images: keep string behavior for compatibility\n            response_a = conv_to_str(responses_a)\n            response_b = conv_to_str(responses_b)\n\n            scores = conversation.scores\n\n            # Handle list format [scores_a, scores_b]\n            if isinstance(scores, list) and len(scores) == 2:\n                scores_a, scores_b = scores[0], scores[1]\n                winner = conversation.meta.get(\"winner\")  # Winner stored in meta\n\n                # Build the prompt with separate scores for each model\n                prompt_parts = [\n                    f\"# Model A (Name: \\\"{model_a}\\\") conversation:\\n {response_a}\"\n                ]\n\n                if self.include_scores_in_prompt and scores_a:\n                    prompt_parts.append(f\"# Model A Scores:\\n {scores_a}\")\n\n                prompt_parts.append(\"--------------------------------\")\n                prompt_parts.append(f\"# Model B (Name: \\\"{model_b}\\\") conversation:\\n {response_b}\")\n\n                if self.include_scores_in_prompt and scores_b:\n                    prompt_parts.append(f\"# Model B Scores:\\n {scores_b}\")\n\n                if self.include_scores_in_prompt and winner:\n                    prompt_parts.append(f\"# Winner: {winner}\")\n\n                return \"\\n\\n\".join(prompt_parts)\n            else:\n                # No scores available\n                return (\n                    f\"# Model A (Name: \\\"{model_a}\\\") conversation:\\n {response_a}\\n\\n\"\n                    f\"--------------------------------\\n\"\n                    f\"# Model B (Name: \\\"{model_b}\\\") conversation:\\n {response_b}\"\n                )\n        elif isinstance(conversation.model, str):\n            # Single model format\n            model = conversation.model if isinstance(conversation.model, str) else str(conversation.model)\n            responses = conversation.responses\n\n            # Normalize to our internal segments format only to detect images\n            conv_norm = openai_messages_to_conv(responses) if isinstance(responses, list) else responses\n            if self._conversation_has_images(conv_norm):\n                return self._build_single_user_messages(conv_norm)\n\n            # No images: keep string behavior\n            try:\n                response = conv_to_str(responses)\n            except Exception as e:\n                raise ValueError(\n                    f\"Failed to convert conversation response to string format. \"\n                    f\"Expected OpenAI conversation format (list of message dicts with 'role' and 'content' fields). \"\n                    f\"Got: {type(responses)}. \"\n                    f\"Error: {str(e)}\"\n                )\n            scores = conversation.scores\n\n            if not scores or not self.include_scores_in_prompt:\n                return response\n            return (\n                f\"{response}\\n\\n\"\n                f\"### Scores:\\n {scores}\"\n            )\n        else:\n            raise ValueError(f\"Invalid conversation format: {conversation}\")\n\n    def _conversation_has_images(self, conv_msgs: List[Dict[str, Any]]) -&gt; bool:\n        \"\"\"Return True if any message contains an image segment in ordered segments format.\"\"\"\n        for msg in conv_msgs:\n            content = msg.get(\"content\", {})\n            segs = content.get(\"segments\") if isinstance(content, dict) else None\n            if isinstance(segs, list):\n                for seg in segs:\n                    if isinstance(seg, dict) and seg.get(\"kind\") == \"image\":\n                        return True\n        return False\n\n    def _collapse_segments_to_openai_content(self, conv_msgs: List[Dict[str, Any]]) -&gt; List[Dict[str, Any]]:\n        \"\"\"\n        Collapse ordered segments into an OpenAI multimodal content list with aggregated text.\n\n        Algorithm:\n        - Walk through messages in order, accumulating non-image content in a buffer\n        - When an image is encountered, flush the buffer (convert to string via conv_to_str),\n          then add the image as a separate item\n        - Continue until all messages processed, then flush any remaining buffer\n\n        This ensures consecutive non-image turns are aggregated into single text items,\n        with images interspersed at their proper positions.\n\n        Produces items like:\n          - {\"type\": \"text\", \"text\": str}  (potentially aggregated from multiple turns)\n          - {\"type\": \"image_url\", \"image_url\": {\"url\": str}}\n        \"\"\"\n        content: List[Dict[str, Any]] = []\n        message_buffer: List[Dict[str, Any]] = []  # Buffer for messages without images\n\n        def flush_buffer():\n            \"\"\"Convert buffered messages to a single text string using conv_to_str.\"\"\"\n            if message_buffer:\n                text_str = conv_to_str(message_buffer)\n                if text_str and text_str.strip():\n                    content.append({\"type\": \"text\", \"text\": text_str})\n                message_buffer.clear()\n\n        for msg in conv_msgs:\n            # Extract segments from this message\n            msg_content = msg.get(\"content\", {})\n            segs = msg_content.get(\"segments\", []) if isinstance(msg_content, dict) else []\n\n            # Check if this message contains any images\n            images_in_msg: List[str] = []\n            non_image_segments: List[Dict[str, Any]] = []\n\n            for seg in segs:\n                if not isinstance(seg, dict):\n                    non_image_segments.append(seg)\n                    continue\n\n                kind = seg.get(\"kind\")\n                if kind == \"image\":\n                    # Extract image URL\n                    img = seg.get(\"image\")\n                    url: Optional[str] = None\n                    if isinstance(img, str):\n                        url = img\n                    elif isinstance(img, dict):\n                        if isinstance(img.get(\"url\"), str):\n                            url = img.get(\"url\")\n                        elif isinstance(img.get(\"image_url\"), dict) and isinstance(img[\"image_url\"].get(\"url\"), str):\n                            url = img[\"image_url\"].get(\"url\")\n                        elif isinstance(img.get(\"source\"), str):\n                            url = img.get(\"source\")\n                    if url:\n                        images_in_msg.append(url)\n                else:\n                    # Keep non-image segments (text, tool, etc.)\n                    non_image_segments.append(seg)\n\n            # Build a message dict with only non-image content for the buffer\n            if non_image_segments:\n                msg_for_buffer = dict(msg)  # Copy message structure\n                msg_for_buffer[\"content\"] = {\n                    \"segments\": non_image_segments\n                }\n                message_buffer.append(msg_for_buffer)\n\n            # If we encountered images, flush buffer then add images\n            if images_in_msg:\n                flush_buffer()\n                for img_url in images_in_msg:\n                    content.append({\"type\": \"image_url\", \"image_url\": {\"url\": img_url}})\n\n        # Flush any remaining buffered messages at the end\n        flush_buffer()\n\n        return content\n\n    def _build_single_user_messages(self, conv_msgs: List[Dict[str, Any]]) -&gt; List[Dict[str, Any]]:\n        \"\"\"Build a full messages list with system + single multimodal user turn.\"\"\"\n        content = self._collapse_segments_to_openai_content(conv_msgs)\n        messages: List[Dict[str, Any]] = []\n        if self.system_prompt:\n            messages.append({\"role\": \"system\", \"content\": self.system_prompt})\n        messages.append({\"role\": \"user\", \"content\": content})\n        return messages\n\n    def _build_side_by_side_messages(\n        self,\n        model_a: str,\n        model_b: str,\n        conv_a: List[Dict[str, Any]],\n        conv_b: List[Dict[str, Any]],\n    ) -&gt; List[Dict[str, Any]]:\n        \"\"\"Build a full messages list with system + single user turn containing A/B sections.\"\"\"\n        content: List[Dict[str, Any]] = []\n        content.append({\"type\": \"text\", \"text\": f\"# Model A (Name: \\\"{model_a}\\\")\"})\n        content.extend(self._collapse_segments_to_openai_content(conv_a))\n        content.append({\"type\": \"text\", \"text\": \"--------------------------------\"})\n        content.append({\"type\": \"text\", \"text\": f\"# Model B (Name: \\\"{model_b}\\\")\"})\n        content.extend(self._collapse_segments_to_openai_content(conv_b))\n\n        messages: List[Dict[str, Any]] = []\n        if self.system_prompt:\n            messages.append({\"role\": \"system\", \"content\": self.system_prompt})\n        messages.append({\"role\": \"user\", \"content\": content})\n        return messages\n\n    def _log_extraction_to_wandb(self, user_messages: List[str], raw_responses: List[str], conversations):\n        \"\"\"Log extraction inputs/outputs to wandb.\"\"\"\n        try:\n            import wandb\n            # import weave\n\n            # Create a table of inputs and outputs\n            extraction_data = []\n            for i, (msg, response, conv) in enumerate(zip(user_messages, raw_responses, conversations)):\n                # Handle None responses (failed LLM calls)\n                if response is None:\n                    extraction_data.append({\n                        \"question_id\": conv.question_id,\n                        \"system_prompt\": self.system_prompt,\n                        \"input_message\": msg,\n                        \"raw_response\": \"FAILED: None\",\n                        \"response_length\": 0,\n                        \"has_error\": True,\n                    })\n                else:\n                    extraction_data.append({\n                        \"question_id\": conv.question_id,\n                        \"system_prompt\": self.system_prompt,\n                        \"input_message\": msg,\n                        \"raw_response\": response,\n                        \"response_length\": len(response),\n                        \"has_error\": False,\n                    })\n\n            # Log extraction table (as table, not summary)\n            self.log_wandb({\n                \"Property_Extraction/extraction_inputs_outputs\": wandb.Table(\n                    columns=[\"question_id\", \"system_prompt\", \"input_message\", \"raw_response\", \"response_length\", \"has_error\"],\n                    data=[[row[col] for col in [\"question_id\", \"system_prompt\", \"input_message\", \"raw_response\", \"response_length\", \"has_error\"]]\n                          for row in extraction_data]\n                )\n            })\n\n            # Log extraction metrics as summary metrics (not regular metrics)\n            error_count = sum(1 for r in raw_responses if r is None)\n            valid_responses = [r for r in raw_responses if r is not None]\n            extraction_metrics = {\n                \"extraction_total_requests\": len(raw_responses),\n                \"extraction_error_count\": error_count,\n                \"extraction_success_rate\": (len(raw_responses) - error_count) / len(raw_responses) if raw_responses else 0,\n                \"extraction_avg_response_length\": sum(len(r) for r in valid_responses) / len(valid_responses) if valid_responses else 0,\n            }\n            self.log_wandb(extraction_metrics, is_summary=True)\n\n        except Exception as e:\n            self.log(f\"Failed to log extraction to wandb: {e}\", level=\"warning\")        \n</code></pre>"},{"location":"api/reference/#stringsight.extractors.openai.OpenAIExtractor.run","title":"<code>run(data, progress_callback=None)</code>","text":"<p>Run OpenAI extraction for all conversations.</p> <p>Each conversation is formatted with <code>prompt_builder</code> and sent to the OpenAI model in parallel using a thread pool.  The raw LLM response is stored inside a placeholder <code>Property</code> object (one per conversation).  Down-stream stages (<code>LLMJsonParser</code>) will parse these raw strings into fully-formed properties.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>PropertyDataset</code> <p>PropertyDataset with conversations to extract from</p> required <code>progress_callback</code> <p>Optional callback(completed, total) for progress updates</p> <code>None</code> Source code in <code>stringsight/extractors/openai.py</code> <pre><code>def run(self, data: PropertyDataset, progress_callback=None) -&gt; PropertyDataset:\n    \"\"\"Run OpenAI extraction for all conversations.\n\n    Each conversation is formatted with ``prompt_builder`` and sent to the\n    OpenAI model in parallel using a thread pool.  The raw LLM response is\n    stored inside a *placeholder* ``Property`` object (one per\n    conversation).  Down-stream stages (``LLMJsonParser``) will parse these\n    raw strings into fully-formed properties.\n\n    Args:\n        data: PropertyDataset with conversations to extract from\n        progress_callback: Optional callback(completed, total) for progress updates\n    \"\"\"\n\n    n_conv = len(data.conversations)\n    if n_conv == 0:\n        self.log(\"No conversations found \u2013 skipping extraction\")\n        return data\n\n    self.log(f\"Extracting properties from {n_conv} conversations using {self.model}\")\n\n\n    # ------------------------------------------------------------------\n    # 1\ufe0f\u20e3  Build user messages for every conversation (in parallel)\n    # ------------------------------------------------------------------\n    user_messages: List[Union[str, List[Dict[str, Any]]]] = [\"\"] * len(data.conversations)\n\n    def _build_prompt(idx: int, conv):\n        return idx, self.prompt_builder(conv)\n\n    with ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n        futures = {executor.submit(_build_prompt, idx, conv): idx\n                  for idx, conv in enumerate(data.conversations)}\n        for future in as_completed(futures):\n            idx, prompt = future.result()\n            user_messages[idx] = prompt\n\n    # ------------------------------------------------------------------\n    # 2\ufe0f\u20e3  Call the OpenAI API in parallel batches via shared LLM utils\n    # ------------------------------------------------------------------\n    raw_responses = parallel_completions(\n        user_messages,\n        model=self.model,\n        system_prompt=self.system_prompt,\n        max_workers=self.max_workers,\n        temperature=self.temperature,\n        top_p=self.top_p,\n        max_tokens=self.max_tokens,\n        show_progress=True,\n        progress_desc=\"Property extraction\",\n        progress_callback=progress_callback\n    )\n\n    # ------------------------------------------------------------------\n    # 3\ufe0f\u20e3  Wrap raw responses in placeholder Property objects (filter None)\n    # ------------------------------------------------------------------\n    properties: List[Property] = []\n    skipped_count = 0\n    for conv, raw in zip(data.conversations, raw_responses):\n        # Skip failed LLM calls (None responses)\n        if raw is None:\n            skipped_count += 1\n            continue\n\n        # We don't yet know which model(s) the individual properties will\n        # belong to; parser will figure it out.  Use a placeholder model\n        # name so that validation passes.\n        prop = Property(\n            id=str(uuid.uuid4()),\n            question_id=conv.question_id,\n            model=conv.model,\n            raw_response=raw,\n        )\n        properties.append(prop)\n\n    if skipped_count &gt; 0:\n        self.log(f\"Skipped {skipped_count} conversations due to failed LLM calls\", level=\"warning\")\n\n    self.log(f\"Received {len(properties)} valid LLM responses\")\n\n\n    # Log to wandb if enabled\n    if hasattr(self, 'use_wandb') and self.use_wandb:\n        self._log_extraction_to_wandb(user_messages, raw_responses, data.conversations)\n\n    # ------------------------------------------------------------------\n    # 4\ufe0f\u20e3  Return updated dataset\n    # ------------------------------------------------------------------\n    return PropertyDataset(\n        conversations=data.conversations,\n        all_models=data.all_models,\n        properties=properties,\n        clusters=data.clusters,\n        model_stats=data.model_stats,\n    )\n</code></pre>"},{"location":"api/reference/#clusterers","title":"Clusterers","text":""},{"location":"api/reference/#get_clusterer","title":"get_clusterer()","text":""},{"location":"api/reference/#stringsight.clusterers.get_clusterer","title":"<code>get_clusterer(method='hdbscan', min_cluster_size=None, embedding_model='sentence-transformers/all-MiniLM-L6-v2', assign_outliers=False, include_embeddings=False, use_gpu=None, cluster_positive=False, **kwargs)</code>","text":"<p>Factory function to get the appropriate clusterer.</p> <p>Parameters:</p> Name Type Description Default <code>method</code> <code>str</code> <p>Clustering method (\"hdbscan\", \"dummy\")</p> <code>'hdbscan'</code> <code>min_cluster_size</code> <code>int | None</code> <p>Minimum cluster size</p> <code>None</code> <code>embedding_model</code> <code>str</code> <p>Embedding model to use</p> <code>'sentence-transformers/all-MiniLM-L6-v2'</code> <code>assign_outliers</code> <code>bool</code> <p>Whether to assign outliers to nearest clusters</p> <code>False</code> <code>include_embeddings</code> <code>bool</code> <p>Whether to include embeddings in output</p> <code>False</code> <code>use_gpu</code> <code>bool | None</code> <p>Enable GPU acceleration for embeddings, UMAP, and HDBSCAN.     None (default) = auto-detect based on CUDA availability.</p> <code>None</code> <code>cluster_positive</code> <code>bool</code> <p>If False and groupby_column is \"behavior_type\", skip clustering positive behaviors.              Defaults to False.</p> <code>False</code> <code>**kwargs</code> <p>Additional configuration</p> <code>{}</code> <p>Returns:</p> Type Description <code>PipelineStage</code> <p>Configured clusterer stage</p> Source code in <code>stringsight/clusterers/__init__.py</code> <pre><code>def get_clusterer(\n    method: str = \"hdbscan\",\n    min_cluster_size: int | None = None,\n    embedding_model: str = \"sentence-transformers/all-MiniLM-L6-v2\",\n    assign_outliers: bool = False,\n    include_embeddings: bool = False,\n    use_gpu: bool | None = None,\n    cluster_positive: bool = False,\n    **kwargs\n) -&gt; PipelineStage:\n    \"\"\"\n    Factory function to get the appropriate clusterer.\n\n    Args:\n        method: Clustering method (\"hdbscan\", \"dummy\")\n        min_cluster_size: Minimum cluster size\n        embedding_model: Embedding model to use\n        assign_outliers: Whether to assign outliers to nearest clusters\n        include_embeddings: Whether to include embeddings in output\n        use_gpu: Enable GPU acceleration for embeddings, UMAP, and HDBSCAN.\n                None (default) = auto-detect based on CUDA availability.\n        cluster_positive: If False and groupby_column is \"behavior_type\", skip clustering positive behaviors.\n                         Defaults to False.\n        **kwargs: Additional configuration\n\n    Returns:\n        Configured clusterer stage\n    \"\"\"\n\n    if method == \"hdbscan\":\n        from .hdbscan import HDBSCANClusterer\n        return HDBSCANClusterer(\n            min_cluster_size=min_cluster_size,\n            embedding_model=embedding_model,\n            assign_outliers=assign_outliers,\n            include_embeddings=include_embeddings,\n            use_gpu=use_gpu,\n            cluster_positive=cluster_positive,\n            **kwargs\n        )\n    # 'hdbscan_stratified' alias has been removed; users should pass\n    # `method=\"hdbscan\"` and supply `groupby_column` if stratification is\n    # desired.\n    elif method == \"dummy\":\n        from .dummy_clusterer import DummyClusterer\n        return DummyClusterer(**kwargs)\n    else:\n        raise ValueError(f\"Unknown clustering method: {method}\")\n</code></pre>"},{"location":"api/reference/#hdbscanclusterer","title":"HDBSCANClusterer","text":""},{"location":"api/reference/#stringsight.clusterers.hdbscan.HDBSCANClusterer","title":"<code>HDBSCANClusterer</code>","text":"<p>               Bases: <code>BaseClusterer</code></p> <p>HDBSCAN clustering stage.</p> <p>This stage migrates the hdbscan_cluster_categories function from clustering/hierarchical_clustering.py into the pipeline architecture.</p> Source code in <code>stringsight/clusterers/hdbscan.py</code> <pre><code>class HDBSCANClusterer(BaseClusterer):\n    \"\"\"\n    HDBSCAN clustering stage.\n\n    This stage migrates the hdbscan_cluster_categories function from\n    clustering/hierarchical_clustering.py into the pipeline architecture.\n    \"\"\"\n\n    def __init__(\n        self,\n        min_cluster_size: int | None = None,\n        embedding_model: str = \"sentence-transformers/all-MiniLM-L6-v2\",\n        assign_outliers: bool = False,\n        include_embeddings: bool = False,\n        use_wandb: bool = False,\n        wandb_project: Optional[str] = None,\n        output_dir: Optional[str] = None,\n        # Additional explicit configuration parameters\n        use_gpu: Optional[bool] = None,  # None = auto-detect\n        min_samples: Optional[int] = None,\n        cluster_selection_epsilon: float = 0.0,\n        disable_dim_reduction: bool = False,\n        dim_reduction_method: str = \"adaptive\",\n        umap_n_components: int = 100,\n        umap_n_neighbors: int = 30,\n        umap_min_dist: float = 0.1,\n        umap_metric: str = \"cosine\",\n        context: Optional[str] = None,\n        groupby_column: Optional[str] = None,\n        parallel_clustering: bool = True,\n        cluster_positive: bool = False,\n        precomputed_embeddings: Optional[object] = None,\n        cache_embeddings: bool = True,\n        input_model_name: Optional[str] = None,\n        summary_model: str = \"gpt-4.1\",\n        cluster_assignment_model: str = \"gpt-4.1-mini\",\n        verbose: bool = True,\n        llm_max_workers: int = 64,\n        **kwargs,\n    ):\n        \"\"\"Initialize the HDBSCAN clusterer with explicit, overridable parameters.\n\n        Args:\n            use_gpu: Enable GPU acceleration for embeddings, UMAP, and HDBSCAN.\n                    None (default) = auto-detect based on CUDA availability.\n                    True = force GPU (requires cuML and CuPy, falls back to CPU if not available).\n                    False = force CPU.\n        \"\"\"\n        super().__init__(\n            output_dir=output_dir,\n            include_embeddings=include_embeddings,\n            use_wandb=use_wandb,\n            wandb_project=wandb_project,\n            **kwargs,\n        )\n\n        # Build a unified ClusterConfig (no hardcoded values)\n        self.config = ClusterConfig(\n            # core\n            min_cluster_size=min_cluster_size,\n            verbose=verbose,\n            include_embeddings=include_embeddings,\n            context=context,\n            precomputed_embeddings=precomputed_embeddings,\n            disable_dim_reduction=disable_dim_reduction,\n            assign_outliers=assign_outliers,\n            input_model_name=input_model_name,\n            min_samples=min_samples,\n            cluster_selection_epsilon=cluster_selection_epsilon,\n            cache_embeddings=cache_embeddings,\n            # GPU acceleration\n            use_gpu=use_gpu,\n            # models\n            embedding_model=embedding_model,\n            summary_model=summary_model,\n            cluster_assignment_model=cluster_assignment_model,\n            llm_max_workers=llm_max_workers,\n            # dim reduction\n            dim_reduction_method=dim_reduction_method,\n            umap_n_components=umap_n_components,\n            umap_n_neighbors=umap_n_neighbors,\n            umap_min_dist=umap_min_dist,\n            umap_metric=umap_metric,\n            # groupby\n            groupby_column=groupby_column,\n            parallel_clustering=parallel_clustering,\n            cluster_positive=cluster_positive,\n            # wandb\n            use_wandb=use_wandb,\n            wandb_project=wandb_project,\n        )\n\n\n    def cluster(self, data: PropertyDataset, column_name: str) -&gt; pd.DataFrame:\n        \"\"\"Cluster the dataset.\n\n        If ``self.config.groupby_column`` is provided and present in the data, the\n        input DataFrame is first partitioned by that column and each partition is\n        clustered independently (stratified clustering). Results are then\n        concatenated back together. Otherwise, the entire dataset is clustered\n        at once.\n        \"\"\"\n\n        df = data.to_dataframe(type=\"properties\")\n\n        if getattr(self, \"verbose\", False):\n            logger.debug(f\"DataFrame shape after to_dataframe: {df.shape}\")\n            logger.debug(f\"DataFrame columns: {list(df.columns)}\")\n            logger.debug(f\"DataFrame head:\")\n            logger.debug(df.head())\n\n        if column_name in df.columns:\n            if getattr(self, \"verbose\", False):\n                logger.debug(f\"{column_name} unique values: {df[column_name].nunique()}\")\n                logger.debug(f\"{column_name} value counts:\")\n                logger.debug(df[column_name].value_counts())\n                logger.debug(f\"Sample {column_name} values: {df[column_name].head().tolist()}\")\n        else:\n            logger.error(f\"Column '{column_name}' not found in DataFrame!\")\n\n        group_col = getattr(self.config, \"groupby_column\", None)\n        cluster_positive = getattr(self.config, \"cluster_positive\", False)\n\n        # Filter out positive behaviors if cluster_positive is False and groupby_column is behavior_type\n        positive_mask = None\n        positive_df = None\n        if group_col == \"behavior_type\" and not cluster_positive and \"behavior_type\" in df.columns:\n            positive_mask = df[\"behavior_type\"] == \"Positive\"\n            positive_df = df[positive_mask].copy()\n            df = df[~positive_mask].copy()\n            if len(positive_df) &gt; 0:\n                self.log(f\"Filtering out {len(positive_df)} positive behaviors from clustering (cluster_positive=False)\")\n            if len(df) == 0:\n                self.log(\"All behaviors are positive and cluster_positive=False - skipping clustering\")\n\n        # Handle case where all behaviors were filtered out\n        if len(df) == 0:\n            # If we have positive behaviors, return them with special cluster assignment\n            if positive_df is not None and len(positive_df) &gt; 0:\n                id_col = f\"{column_name}_cluster_id\"\n                label_col = f\"{column_name}_cluster_label\"\n                positive_df[id_col] = -2\n                positive_df[label_col] = \"Positive (not clustered)\"\n                if \"meta\" not in positive_df.columns:\n                    positive_df[\"meta\"] = [{} for _ in range(len(positive_df))]\n                return positive_df\n            # Otherwise return empty DataFrame with required columns\n            id_col = f\"{column_name}_cluster_id\"\n            label_col = f\"{column_name}_cluster_label\"\n            df_empty = pd.DataFrame(columns=[column_name, id_col, label_col, \"question_id\", \"id\", \"meta\"])\n            return df_empty\n\n        if group_col is not None and group_col in df.columns:\n            parallel_clustering = getattr(self.config, \"parallel_clustering\", False)\n\n            if parallel_clustering:\n                # Parallelize clustering per group for better performance\n                groups = list(df.groupby(group_col))\n\n                def _cluster_group(group_info):\n                    group, group_df = group_info\n                    if getattr(self, \"verbose\", False):\n                        logger.info(f\"--------------------------------\\nClustering group {group}\\n--------------------------------\")\n                    part = hdbscan_cluster_categories(\n                        group_df,\n                        column_name=column_name,\n                        config=self.config,\n                    )\n                    return group, part\n\n                # Process groups in parallel\n                clustered_parts = []\n                max_workers = min(len(groups), getattr(self.config, 'llm_max_workers', 64))\n                with ThreadPoolExecutor(max_workers=max_workers) as executor:\n                    futures = {executor.submit(_cluster_group, group_info): idx\n                              for idx, group_info in enumerate(groups)}\n\n                    # Add progress bar for parallel clustering\n                    with tqdm(total=len(groups), desc=f\"Clustering {len(groups)} groups in parallel\", disable=not getattr(self, \"verbose\", False)) as pbar:\n                        for future in as_completed(futures):\n                            group, part = future.result()\n                            # Add meta column with group information as a dictionary\n                            # Use list comprehension to create independent dict objects for each row\n                            part[\"meta\"] = [{\"group\": group} for _ in range(len(part))]\n                            clustered_parts.append(part)\n                            pbar.update(1)\n                clustered_df = pd.concat(clustered_parts, ignore_index=True)\n            else:\n                # Process groups sequentially (default behavior)\n                clustered_parts = []\n                groups = list(df.groupby(group_col))\n\n                # Add progress bar for sequential clustering\n                for group, group_df in tqdm(groups, desc=f\"Clustering {len(groups)} groups sequentially\", disable=not getattr(self, \"verbose\", False)):\n                    if getattr(self, \"verbose\", False):\n                        logger.info(f\"--------------------------------\\nClustering group {group}\\n--------------------------------\")\n                    part = hdbscan_cluster_categories(\n                        group_df,\n                        column_name=column_name,\n                        config=self.config,\n                    )\n                    # Add meta column with group information as a dictionary\n                    # Use list comprehension to create independent dict objects for each row\n                    part[\"meta\"] = [{\"group\": group} for _ in range(len(part))]\n                    clustered_parts.append(part)\n                clustered_df = pd.concat(clustered_parts, ignore_index=True)\n        else:\n            clustered_df = hdbscan_cluster_categories(\n                df,\n                column_name=column_name,\n                config=self.config,\n            )\n\n        # Add back positive behaviors with special cluster assignment if they were filtered out\n        if positive_df is not None and len(positive_df) &gt; 0:\n            id_col = f\"{column_name}_cluster_id\"\n            label_col = f\"{column_name}_cluster_label\"\n\n            # Assign special cluster ID and label for positive behaviors that weren't clustered\n            positive_df[id_col] = -2\n            positive_df[label_col] = \"Positive (not clustered)\"\n            if \"meta\" not in positive_df.columns:\n                positive_df[\"meta\"] = [{} for _ in range(len(positive_df))]\n\n            # Concatenate back with clustered results\n            clustered_df = pd.concat([clustered_df, positive_df], ignore_index=True)\n            self.log(f\"Added back {len(positive_df)} positive behaviors with cluster_id=-2 (not clustered)\")\n\n        return clustered_df\n\n    def postprocess_clustered_df(self, df: pd.DataFrame, column_name: str, prettify_labels: bool = False) -&gt; pd.DataFrame:\n        \"\"\"Standard post-processing plus stratified ID re-assignment when needed.\"\"\"\n\n        label_col = f\"{column_name}_cluster_label\"\n        id_col = f\"{column_name}_cluster_id\"\n\n        df = super().postprocess_clustered_df(df, label_col, prettify_labels)\n\n        # 1\ufe0f\u20e3  Move tiny clusters to Outliers\n        label_counts = df[label_col].value_counts()\n        min_size_threshold = int((getattr(self.config, \"min_cluster_size\", 1) or 1))\n        too_small_labels = label_counts[label_counts &lt; min_size_threshold].index\n        for label in too_small_labels:\n            mask = df[label_col] == label\n            cid = df.loc[mask, id_col].iloc[0] if not df.loc[mask].empty else None\n            self.log(\n                f\"Assigning cluster {cid} (label '{label}') to Outliers because it has {label_counts[label]} items\"\n            )\n\n            # Check if we're using groupby and assign group-specific outlier labels\n            group_col = getattr(self.config, \"groupby_column\", None)\n            if group_col is not None and group_col in df.columns:\n                # Assign group-specific outlier labels\n                for group_value in df.loc[mask, group_col].unique():\n                    group_mask = mask &amp; (df[group_col] == group_value)\n                    outlier_label = f\"Outliers - {group_value}\"\n                    df.loc[group_mask, label_col] = outlier_label\n                    df.loc[group_mask, id_col] = -1\n            else:\n                # Standard outlier assignment\n                df.loc[mask, label_col] = \"Outliers\"\n                df.loc[mask, id_col] = -1\n\n        # 2\ufe0f\u20e3  For stratified mode: ensure cluster IDs are unique across partitions\n        group_col = getattr(self.config, \"groupby_column\", None)\n        if group_col is not None and group_col in df.columns:\n            # Handle group-specific outlier labels\n            outlier_mask = df[label_col].str.startswith(\"Outliers - \") if df[label_col].dtype == 'object' else df[label_col] == \"Outliers\"\n            non_outlier_mask = ~outlier_mask\n\n            unique_pairs = (\n                df.loc[non_outlier_mask, [group_col, label_col]]\n                .drop_duplicates()\n                .reset_index(drop=True)\n            )\n            pair_to_new_id = {\n                (row[group_col], row[label_col]): idx for idx, row in unique_pairs.iterrows()\n            }\n            for (gval, lbl), new_id in pair_to_new_id.items():\n                pair_mask = (df[group_col] == gval) &amp; (df[label_col] == lbl) &amp; non_outlier_mask\n                df.loc[pair_mask, id_col] = new_id\n\n            # Handle group-specific outliers: assign unique IDs to each outlier group\n            if outlier_mask.any():\n                # Get unique outlier labels\n                unique_outlier_labels = df.loc[outlier_mask, label_col].unique()\n\n                # Assign unique IDs to each outlier group, starting from a high negative number\n                # to avoid conflicts with regular cluster IDs\n                outlier_id_start = -1000\n                for i, outlier_label in enumerate(unique_outlier_labels):\n                    outlier_label_mask = df[label_col] == outlier_label\n                    unique_outlier_id = outlier_id_start - i\n                    df.loc[outlier_label_mask, id_col] = unique_outlier_id\n\n        return df\n\n    # ------------------------------------------------------------------\n    # \ud83c\udff7\ufe0f  Cluster construction helper with group metadata\n    # ------------------------------------------------------------------\n    def _build_clusters_from_df(self, df: pd.DataFrame, column_name: str):\n        \"\"\"Build clusters and, in stratified mode, add group info to metadata.\"\"\"\n\n        clusters = super()._build_clusters_from_df(df, column_name)\n\n        group_col = getattr(self.config, \"groupby_column\", None)\n        if group_col is not None and group_col in df.columns:\n            id_col = f\"{column_name}_cluster_id\"\n            id_to_group = (\n                df.loc[df[id_col].notna(), [id_col, group_col]]\n                .dropna()\n                .groupby(id_col)[group_col]\n                .agg(lambda s: s.iloc[0])\n                .to_dict()\n            )\n            for c in clusters:\n                cid = getattr(c, \"id\", None)\n                if cid in id_to_group:\n                    c.meta = dict(c.meta or {})\n                    c.meta[\"group\"] = id_to_group[cid]\n\n        return clusters\n</code></pre>"},{"location":"api/reference/#metrics","title":"Metrics","text":""},{"location":"api/reference/#get_metrics","title":"get_metrics()","text":""},{"location":"api/reference/#stringsight.metrics.get_metrics","title":"<code>get_metrics(method, **kwargs)</code>","text":"<p>Factory function for metrics stages.</p> <p>Parameters:</p> Name Type Description Default <code>method</code> <code>str</code> <p>\"side_by_side\", \"single_model\", \"functional\", or \"single_model_legacy\"</p> required <code>**kwargs</code> <p>Additional configuration for the metrics stage</p> <code>{}</code> <p>Returns:</p> Type Description <code>PipelineStage</code> <p>Configured metrics stage</p> Source code in <code>stringsight/metrics/__init__.py</code> <pre><code>def get_metrics(method: str, **kwargs) -&gt; \"PipelineStage\":\n    \"\"\"\n    Factory function for metrics stages.\n\n    Args:\n        method: \"side_by_side\", \"single_model\", \"functional\", or \"single_model_legacy\"\n        **kwargs: Additional configuration for the metrics stage\n\n    Returns:\n        Configured metrics stage\n    \"\"\"\n    # Remap legacy flag name for wandb to the functional parameter\n    if \"use_wandb\" in kwargs and \"log_to_wandb\" not in kwargs:\n        kwargs[\"log_to_wandb\"] = kwargs.pop(\"use_wandb\")\n\n    if method == \"side_by_side\":\n        return SideBySideMetrics(**kwargs)\n    elif method == \"single_model\":\n        # NEW: Default to functional metrics for single_model\n        return SingleModelMetrics(**kwargs)\n    # elif method == \"functional\":\n    #     return FunctionalMetrics(**kwargs)\n    else:\n        raise ValueError(f\"Unknown metrics method: {method}. Available: 'side_by_side', 'single_model', 'functional', 'single_model_legacy'\") \n</code></pre>"},{"location":"api/reference/#utilities","title":"Utilities","text":""},{"location":"api/reference/#format-detection","title":"Format Detection","text":""},{"location":"api/reference/#stringsight.formatters.detect_method","title":"<code>detect_method(columns)</code>","text":"<p>Return the best-matching method based on available columns.</p> <p>Parameters:</p> Name Type Description Default <code>columns</code> <code>List[str]</code> <p>List of column names present in the dataset</p> required <p>Returns:</p> Type Description <code>Optional[Method]</code> <p>\"single_model\" | \"side_by_side\" if a set of required columns is satisfied,</p> <code>Optional[Method]</code> <p>otherwise None.</p> Source code in <code>stringsight/formatters/traces.py</code> <pre><code>def detect_method(columns: List[str]) -&gt; Optional[Method]:\n    \"\"\"Return the best-matching method based on available columns.\n\n    Args:\n        columns: List of column names present in the dataset\n\n    Returns:\n        \"single_model\" | \"side_by_side\" if a set of required columns is satisfied,\n        otherwise None.\n    \"\"\"\n    col_set = set(columns)\n    if set(REQUIRED_COLUMNS[\"side_by_side\"]).issubset(col_set):\n        return \"side_by_side\"\n    if set(REQUIRED_COLUMNS[\"single_model\"]).issubset(col_set):\n        return \"single_model\"\n    return None\n</code></pre>"},{"location":"api/reference/#validation","title":"Validation","text":""},{"location":"api/reference/#stringsight.formatters.validate_required_columns","title":"<code>validate_required_columns(df, method)</code>","text":"<p>Return the list of missing required columns for the given method.</p> <p>Empty list indicates the DataFrame satisfies the requirement.</p> Source code in <code>stringsight/formatters/traces.py</code> <pre><code>def validate_required_columns(df: pd.DataFrame, method: Method) -&gt; List[str]:\n    \"\"\"Return the list of missing required columns for the given method.\n\n    Empty list indicates the DataFrame satisfies the requirement.\n    \"\"\"\n    required = set(REQUIRED_COLUMNS[method])\n    missing = [c for c in REQUIRED_COLUMNS[method] if c not in df.columns]\n    return missing\n</code></pre>"},{"location":"deployment/api-endpoints/","title":"API Endpoints","text":"<p>FastAPI backend endpoints for the StringSight web interface.</p>"},{"location":"deployment/api-endpoints/#base-url","title":"Base URL","text":"<pre><code>http://localhost:8000\n</code></pre>"},{"location":"deployment/api-endpoints/#endpoints","title":"Endpoints","text":""},{"location":"deployment/api-endpoints/#health-check","title":"Health Check","text":"<p>GET <code>/health</code></p> <p>Check if the API is running.</p> <p>Response: <pre><code>{\n  \"ok\": true\n}\n</code></pre></p>"},{"location":"deployment/api-endpoints/#detect-and-validate-data","title":"Detect and Validate Data","text":"<p>POST <code>/detect-and-validate</code></p> <p>Upload data file and validate format.</p> <p>Request: - Form data with file upload</p> <p>Response: <pre><code>{\n  \"method\": \"single_model\",\n  \"columns\": [\"prompt\", \"model\", \"model_response\"],\n  \"num_rows\": 100,\n  \"models\": [\"gpt-4\", \"claude-3\"],\n  \"preview\": [...]\n}\n</code></pre></p>"},{"location":"deployment/api-endpoints/#parse-conversations","title":"Parse Conversations","text":"<p>POST <code>/conversations</code></p> <p>Convert data to conversation traces.</p> <p>Request: <pre><code>{\n  \"data\": [...],\n  \"method\": \"single_model\"\n}\n</code></pre></p> <p>Response: <pre><code>{\n  \"conversations\": [\n    {\n      \"question_id\": \"q1\",\n      \"prompt\": \"What is ML?\",\n      \"trace\": [...]\n    }\n  ]\n}\n</code></pre></p>"},{"location":"deployment/api-endpoints/#metrics-and-results","title":"Metrics and Results","text":""},{"location":"deployment/api-endpoints/#get-metrics-summary","title":"Get metrics summary","text":"<p>GET <code>/metrics/summary/{results_dir}</code></p>"},{"location":"deployment/api-endpoints/#get-model-cluster-metrics","title":"Get model-cluster metrics","text":"<p>GET <code>/metrics/model-cluster/{results_dir}</code></p>"},{"location":"deployment/api-endpoints/#get-benchmark-metrics","title":"Get benchmark metrics","text":"<p>GET <code>/metrics/benchmark/{results_dir}</code></p>"},{"location":"deployment/api-endpoints/#get-available-quality-metric-names","title":"Get available quality metric names","text":"<p>GET <code>/metrics/quality-metrics/{results_dir}</code></p>"},{"location":"deployment/api-endpoints/#load-a-results-directory","title":"Load a results directory","text":"<p>POST <code>/results/load</code></p> <p>Body: <code>{ \"path\": \"/abs/or/base-relative/path\", \"max_conversations\": 1000, \"max_properties\": 10000 }</code></p>"},{"location":"deployment/api-endpoints/#file-and-data-utilities","title":"File and Data Utilities","text":""},{"location":"deployment/api-endpoints/#read-a-dataset-from-server-path","title":"Read a dataset from server path","text":"<p>POST <code>/read-path</code></p>"},{"location":"deployment/api-endpoints/#list-a-directory-on-server","title":"List a directory on server","text":"<p>POST <code>/list-path</code></p>"},{"location":"deployment/api-endpoints/#flexible-data-mapping","title":"Flexible Data Mapping","text":"<p>POST <code>/auto-detect-columns</code></p> <p>POST <code>/validate-flexible-mapping</code></p> <p>POST <code>/process-flexible-data</code></p> <p>POST <code>/flexible-conversations</code></p>"},{"location":"deployment/api-endpoints/#extraction","title":"Extraction","text":"<p>POST <code>/extract/single</code></p> <p>POST <code>/extract/batch</code></p> <p>POST <code>/extract/jobs/start</code></p> <p>GET <code>/extract/jobs/status?job_id=...</code></p> <p>GET <code>/extract/jobs/result?job_id=...</code></p>"},{"location":"deployment/api-endpoints/#clustering-and-metrics","title":"Clustering and Metrics","text":"<p>POST <code>/cluster/run</code></p> <p>POST <code>/cluster/metrics</code></p>"},{"location":"deployment/api-endpoints/#starting-the-api","title":"Starting the API","text":"<pre><code>python -m uvicorn stringsight.api:app --reload --host localhost --port 8000\n</code></pre>"},{"location":"deployment/api-endpoints/#cors-configuration","title":"CORS Configuration","text":"<p>By default, development builds allow all origins (permissive CORS) to simplify local frontend usage. For production, restrict origins explicitly (example below).</p>"},{"location":"deployment/api-endpoints/#authentication","title":"Authentication","text":"<p>Currently no authentication required. For production, add:</p> <pre><code>from fastapi import Security\nfrom fastapi.security import HTTPBearer\n\nsecurity = HTTPBearer()\n</code></pre>"},{"location":"deployment/api-endpoints/#next-steps","title":"Next Steps","text":"<ul> <li>Production Setup - Deploy the API</li> <li>Visualization - Use the frontend</li> </ul>"},{"location":"deployment/production/","title":"Production Deployment","text":"<p>Guidelines for deploying StringSight in production environments.</p>"},{"location":"deployment/production/#environment-setup","title":"Environment Setup","text":""},{"location":"deployment/production/#production-environment","title":"Production Environment","text":"<pre><code># Create production environment\nconda create -n stringsight-prod python=3.11\nconda activate stringsight-prod\n\n# Install with production dependencies\npip install -e \".[full]\"\n\n# Set environment variables\nexport OPENAI_API_KEY=\"your-prod-api-key\"\nexport WANDB_API_KEY=\"your-wandb-key\"\nexport ENVIRONMENT=\"production\"\n</code></pre>"},{"location":"deployment/production/#configuration-files","title":"Configuration Files","text":"<p>Create a production config file <code>config/production.yaml</code>:</p> <pre><code>extraction:\n  model_name: \"gpt-4.1\"\n  temperature: 0.7\n  max_workers: 16\n  cache_dir: \"/data/cache/extraction\"\n\nclustering:\n  min_cluster_size: 30\n  embedding_model: \"text-embedding-3-small\"\n  cache_dir: \"/data/cache/clustering\"\n\nmetrics:\n  compute_bootstrap: true\n  bootstrap_samples: 100\n\nlogging:\n  use_wandb: true\n  wandb_project: \"production-analysis\"\n  verbose: true\n\noutput:\n  base_dir: \"/data/results\"\n</code></pre>"},{"location":"deployment/production/#api-deployment","title":"API Deployment","text":""},{"location":"deployment/production/#using-gunicorn","title":"Using Gunicorn","text":"<pre><code># Install gunicorn\npip install gunicorn\n\n# Run API server\ngunicorn stringsight.api:app \\\n  --workers 4 \\\n  --worker-class uvicorn.workers.UvicornWorker \\\n  --bind 0.0.0.0:8000 \\\n  --timeout 300\n</code></pre>"},{"location":"deployment/production/#using-docker","title":"Using Docker","text":"<p>Create <code>Dockerfile</code>:</p> <pre><code>FROM python:3.11-slim\n\nWORKDIR /app\n\n# Install system dependencies\nRUN apt-get update &amp;&amp; apt-get install -y \\\n    git \\\n    &amp;&amp; rm -rf /var/lib/apt/lists/*\n\n# Copy requirements\nCOPY requirements.txt .\nRUN pip install --no-cache-dir -r requirements.txt\n\n# Copy application\nCOPY . .\nRUN pip install -e \".[full]\"\n\n# Expose port\nEXPOSE 8000\n\n# Run application\nCMD [\"uvicorn\", \"stringsight.api:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n</code></pre> <p>Build and run:</p> <pre><code>docker build -t stringsight:latest .\ndocker run -p 8000:8000 -e OPENAI_API_KEY=$OPENAI_API_KEY stringsight:latest\n</code></pre>"},{"location":"deployment/production/#deploying-on-render","title":"Deploying on Render","text":"<p>Render Persistent Disk Setup:</p> <p>StringSight saves results and cache data to disk. On Render, you need to attach a persistent disk to preserve this data across deployments.</p> <ol> <li>Add a Persistent Disk:</li> <li>Go to your Render service \u2192 \"Disks\" tab</li> <li>Click \"Add Disk\"</li> <li>Set mount path to <code>/var/data</code> (or your preferred path)</li> <li> <p>Choose size (e.g., 10-100 GB depending on your needs)</p> </li> <li> <p>Configure Environment Variable:    <pre><code>RENDER_DISK_PATH=/var/data\n</code></pre>    This tells StringSight to use the persistent disk for all results and cache storage.</p> </li> <li> <p>Deploy: Render will automatically trigger a new deployment.</p> </li> </ol> <p>After deployment, check logs to confirm: <pre><code>Using Render persistent disk: /var/data\n</code></pre></p> <p>Important Limitations: - Services with persistent disks cannot scale to multiple instances - Zero-downtime deploys are not supported with disks - Render creates daily snapshots (retained 7 days) for backup</p> <p>See Render Disk Setup Guide for detailed instructions.</p>"},{"location":"deployment/production/#monitoring","title":"Monitoring","text":""},{"location":"deployment/production/#health-checks","title":"Health Checks","text":"<pre><code># Check API health\nimport requests\n\nresponse = requests.get(\"http://localhost:8000/health\")\nassert response.json()[\"ok\"] == True\n</code></pre>"},{"location":"deployment/production/#logging","title":"Logging","text":"<p>Use structured logging:</p> <pre><code>import logging\n\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n    handlers=[\n        logging.FileHandler('stringsight.log'),\n        logging.StreamHandler()\n    ]\n)\n</code></pre>"},{"location":"deployment/production/#cost-tracking","title":"Cost Tracking","text":"<p>Monitor API costs:</p> <pre><code>from stringsight.costs import CostTracker\n\ntracker = CostTracker()\n# Costs are automatically tracked during pipeline execution\n</code></pre>"},{"location":"deployment/production/#scaling","title":"Scaling","text":""},{"location":"deployment/production/#horizontal-scaling","title":"Horizontal Scaling","text":"<p>Run multiple API instances behind a load balancer:</p> <pre><code># Instance 1\nuvicorn stringsight.api:app --port 8001 &amp;\n\n# Instance 2\nuvicorn stringsight.api:app --port 8002 &amp;\n\n# Use nginx as load balancer\n</code></pre>"},{"location":"deployment/production/#queue-based-processing","title":"Queue-Based Processing","text":"<p>Use Celery for async processing:</p> <pre><code>from celery import Celery\nfrom stringsight import explain\n\napp = Celery('tasks', broker='redis://localhost:6379')\n\n@app.task\ndef analyze_dataset(data_path, output_dir):\n    df = pd.read_parquet(data_path)\n    return explain(df, output_dir=output_dir)\n</code></pre>"},{"location":"deployment/production/#security","title":"Security","text":""},{"location":"deployment/production/#api-keys","title":"API Keys","text":"<ul> <li>Store in environment variables, never in code</li> <li>Rotate keys regularly</li> <li>Use separate keys for dev/staging/prod</li> </ul>"},{"location":"deployment/production/#access-control","title":"Access Control","text":"<p>Add authentication to API:</p> <pre><code>from fastapi import Security, HTTPException\nfrom fastapi.security import HTTPBearer\n\nsecurity = HTTPBearer()\n\n@app.get(\"/analyze\")\nasync def analyze(token: str = Security(security)):\n    if not validate_token(token):\n        raise HTTPException(status_code=401)\n    # ...\n</code></pre>"},{"location":"deployment/production/#best-practices","title":"Best Practices","text":"<ol> <li>Use caching - Enable extraction/clustering caches</li> <li>Monitor costs - Track API usage and set budget alerts</li> <li>Version control - Tag releases and track model versions</li> <li>Backup results - Regularly backup output directories</li> <li>Test thoroughly - Run integration tests before deploying</li> </ol>"},{"location":"deployment/production/#next-steps","title":"Next Steps","text":"<ul> <li>API Endpoints - API documentation</li> <li>Performance Tuning - Optimization guide</li> </ul>"},{"location":"development/contributing/","title":"Contributing to StringSight","text":"<p>Thank you for your interest in contributing to StringSight! This guide will help you get started.</p>"},{"location":"development/contributing/#getting-started","title":"Getting Started","text":""},{"location":"development/contributing/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.8 or higher</li> <li>Git</li> <li>Basic knowledge of Python and machine learning</li> </ul>"},{"location":"development/contributing/#setting-up-development-environment","title":"Setting Up Development Environment","text":"<ol> <li> <p>Fork the Repository <pre><code># Fork on GitHub, then clone your fork\ngit clone https://github.com/your-username/stringsight.git\ncd stringsight\n</code></pre></p> </li> <li> <p>Install Development Dependencies <pre><code># Install in development mode\npip install -e .\n\n# Install development dependencies\npip install -r requirements-dev.txt\n</code></pre></p> </li> <li> <p>Set Up Pre-commit Hooks <pre><code># Install pre-commit hooks\npre-commit install\n</code></pre></p> </li> </ol>"},{"location":"development/contributing/#development-workflow","title":"Development Workflow","text":""},{"location":"development/contributing/#1-create-a-feature-branch","title":"1. Create a Feature Branch","text":"<pre><code># Create and switch to a new branch\ngit checkout -b feature/your-feature-name\n\n# Or for bug fixes\ngit checkout -b fix/your-bug-description\n</code></pre>"},{"location":"development/contributing/#2-make-your-changes","title":"2. Make Your Changes","text":"<ul> <li>Write your code following the Code Style guidelines</li> <li>Add tests for new functionality</li> <li>Update documentation as needed</li> </ul>"},{"location":"development/contributing/#3-test-your-changes","title":"3. Test Your Changes","text":"<pre><code># Run the test suite\npytest\n\n# Run with coverage\npytest --cov=stringsight\n\n# Run linting\nflake8 stringsight/\nblack stringsight/\n</code></pre>"},{"location":"development/contributing/#4-commit-your-changes","title":"4. Commit Your Changes","text":"<pre><code># Stage your changes\ngit add .\n\n# Commit with a descriptive message\ngit commit -m \"feat: add new evaluation metric\"\n\n# Push to your fork\ngit push origin feature/your-feature-name\n</code></pre>"},{"location":"development/contributing/#5-create-a-pull-request","title":"5. Create a Pull Request","text":"<ol> <li>Go to your fork on GitHub</li> <li>Click \"New Pull Request\"</li> <li>Select your feature branch</li> <li>Fill out the PR template</li> <li>Submit the PR</li> </ol>"},{"location":"development/contributing/#code-style","title":"Code Style","text":""},{"location":"development/contributing/#python-code","title":"Python Code","text":"<p>We follow PEP 8 with some modifications:</p> <ul> <li>Line Length: 88 characters (Black default)</li> <li>Docstrings: Google style</li> <li>Type Hints: Required for all public functions</li> </ul>"},{"location":"development/contributing/#example","title":"Example","text":"<pre><code>from typing import List, Dict, Optional\n\ndef evaluate_model(\n    data: List[Dict],\n    metrics: List[str] = [\"accuracy\"],\n    config: Optional[Dict] = None\n) -&gt; Dict:\n    \"\"\"Evaluate model performance on given data.\n\n    Args:\n        data: List of dictionaries containing evaluation data\n        metrics: List of metric names to compute\n        config: Optional configuration dictionary\n\n    Returns:\n        Dictionary containing evaluation results\n\n    Raises:\n        EvaluationError: If evaluation fails\n    \"\"\"\n    # Your implementation here\n    pass\n</code></pre>"},{"location":"development/contributing/#documentation","title":"Documentation","text":"<ul> <li>All public functions must have docstrings</li> <li>Use Google style docstrings</li> <li>Include type hints</li> <li>Add examples for complex functions</li> </ul>"},{"location":"development/contributing/#testing","title":"Testing","text":"<ul> <li>Write tests for all new functionality</li> <li>Aim for at least 80% code coverage</li> <li>Use descriptive test names</li> <li>Test both success and failure cases</li> </ul>"},{"location":"development/contributing/#example-test","title":"Example Test","text":"<pre><code>import pytest\nfrom stringsight.evaluation import evaluate_model\n\ndef test_evaluate_model_basic():\n    \"\"\"Test basic model evaluation functionality.\"\"\"\n    data = [\n        {\"question\": \"What is 2+2?\", \"answer\": \"4\", \"model_output\": \"4\"}\n    ]\n\n    results = evaluate_model(data, metrics=[\"accuracy\"])\n\n    assert \"accuracy\" in results\n    assert results[\"accuracy\"] == 1.0\n\ndef test_evaluate_model_invalid_data():\n    \"\"\"Test evaluation with invalid data.\"\"\"\n    with pytest.raises(ValueError):\n        evaluate_model([])\n</code></pre>"},{"location":"development/contributing/#project-structure","title":"Project Structure","text":"<pre><code>stringsight/\n\u251c\u2500\u2500 stringsight/           # Main package\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 evaluation.py   # Core evaluation functions\n\u2502   \u251c\u2500\u2500 data.py         # Data loading and processing\n\u2502   \u251c\u2500\u2500 metrics.py      # Evaluation metrics\n\u2502   \u251c\u2500\u2500 visualization.py # Plotting and visualization\n\u2502   \u251c\u2500\u2500 config.py       # Configuration management\n\u2502   \u2514\u2500\u2500 utils.py        # Utility functions\n\u251c\u2500\u2500 tests/              # Test suite\n\u251c\u2500\u2500 docs/               # Documentation\n\u251c\u2500\u2500 examples/           # Example scripts\n\u2514\u2500\u2500 requirements.txt    # Dependencies\n</code></pre>"},{"location":"development/contributing/#adding-new-features","title":"Adding New Features","text":""},{"location":"development/contributing/#1-new-metrics","title":"1. New Metrics","text":"<p>To add a new evaluation metric:</p> <ol> <li>Create the metric class in <code>stringsight/metrics.py</code></li> <li>Inherit from the <code>Metric</code> base class</li> <li>Implement the <code>compute</code> method</li> <li>Add tests in <code>tests/test_metrics.py</code></li> <li>Update documentation</li> </ol>"},{"location":"development/contributing/#2-new-data-formats","title":"2. New Data Formats","text":"<p>To add support for new data formats:</p> <ol> <li>Add format detection in <code>stringsight/data.py</code></li> <li>Implement loading/saving functions</li> <li>Add validation logic</li> <li>Write tests</li> <li>Update documentation</li> </ol>"},{"location":"development/contributing/#3-new-visualization-types","title":"3. New Visualization Types","text":"<p>To add new visualization types:</p> <ol> <li>Add plotting functions in <code>stringsight/visualization.py</code></li> <li>Follow the existing API patterns</li> <li>Add configuration options</li> <li>Write tests</li> <li>Update documentation</li> </ol>"},{"location":"development/contributing/#bug-reports","title":"Bug Reports","text":"<p>When reporting bugs, please include:</p> <ol> <li>Environment: Python version, OS, package versions</li> <li>Reproduction: Steps to reproduce the issue</li> <li>Expected vs Actual: What you expected vs what happened</li> <li>Error Messages: Full error traceback</li> <li>Minimal Example: Code that reproduces the issue</li> </ol>"},{"location":"development/contributing/#feature-requests","title":"Feature Requests","text":"<p>When requesting features, please include:</p> <ol> <li>Use Case: What problem does this solve?</li> <li>Proposed Solution: How should it work?</li> <li>Alternatives: What other approaches have you considered?</li> <li>Implementation: Any thoughts on implementation?</li> </ol>"},{"location":"development/contributing/#code-review-process","title":"Code Review Process","text":"<ol> <li>Automated Checks: All PRs must pass CI checks</li> <li>Review: At least one maintainer must approve</li> <li>Tests: All tests must pass</li> <li>Documentation: Documentation must be updated</li> <li>Style: Code must follow style guidelines</li> </ol>"},{"location":"development/contributing/#release-process","title":"Release Process","text":""},{"location":"development/contributing/#for-maintainers","title":"For Maintainers","text":"<ol> <li>Update Version: Update version in <code>setup.py</code></li> <li>Update Changelog: Add release notes</li> <li>Create Release: Tag and create GitHub release</li> <li>Publish: Upload to PyPI</li> </ol>"},{"location":"development/contributing/#version-numbers","title":"Version Numbers","text":"<p>We use semantic versioning (MAJOR.MINOR.PATCH):</p> <ul> <li>MAJOR: Breaking changes</li> <li>MINOR: New features (backward compatible)</li> <li>PATCH: Bug fixes (backward compatible)</li> </ul>"},{"location":"development/contributing/#getting-help","title":"Getting Help","text":"<ul> <li>Issues: Use GitHub issues for bugs and feature requests</li> <li>Discussions: Use GitHub discussions for questions</li> <li>Documentation: Check the docs first</li> <li>Code: Look at existing code for examples</li> </ul>"},{"location":"development/contributing/#recognition","title":"Recognition","text":"<p>Contributors will be recognized in:</p> <ul> <li>GitHub contributors list</li> <li>Release notes</li> <li>Documentation acknowledgments</li> </ul>"},{"location":"development/contributing/#next-steps","title":"Next Steps","text":"<ul> <li>Check out the Testing Guide for detailed testing information</li> <li>Read the API Reference to understand the codebase</li> <li>Look at Basic Usage for usage examples </li> </ul>"},{"location":"development/testing/","title":"Testing Guide","text":"<p>Comprehensive guide to testing in StringSight.</p>"},{"location":"development/testing/#running-tests","title":"Running Tests","text":""},{"location":"development/testing/#basic-test-execution","title":"Basic Test Execution","text":"<pre><code># Run all tests\npytest\n\n# Run with verbose output\npytest -v\n\n# Run specific test file\npytest tests/test_evaluation.py\n\n# Run specific test function\npytest tests/test_evaluation.py::test_evaluate_model_basic\n</code></pre>"},{"location":"development/testing/#test-coverage","title":"Test Coverage","text":"<pre><code># Run with coverage report\npytest --cov=stringsight\n\n# Generate HTML coverage report\npytest --cov=stringsight --cov-report=html\n\n# Generate XML coverage report (for CI)\npytest --cov=stringsight --cov-report=xml\n</code></pre>"},{"location":"development/testing/#test-categories","title":"Test Categories","text":"<pre><code># Run unit tests only\npytest -m \"not integration\"\n\n# Run integration tests only\npytest -m integration\n\n# Run slow tests\npytest -m slow\n\n# Skip slow tests\npytest -m \"not slow\"\n</code></pre>"},{"location":"development/testing/#writing-tests","title":"Writing Tests","text":""},{"location":"development/testing/#test-structure","title":"Test Structure","text":"<pre><code>import pytest\nfrom stringsight.evaluation import evaluate_model\n\nclass TestEvaluation:\n    \"\"\"Test suite for evaluation functionality.\"\"\"\n\n    def test_basic_evaluation(self):\n        \"\"\"Test basic model evaluation.\"\"\"\n        # Arrange\n        data = [{\"question\": \"What is 2+2?\", \"answer\": \"4\", \"model_output\": \"4\"}]\n\n        # Act\n        results = evaluate_model(data, metrics=[\"accuracy\"])\n\n        # Assert\n        assert \"accuracy\" in results\n        assert results[\"accuracy\"] == 1.0\n\n    def test_empty_data(self):\n        \"\"\"Test evaluation with empty data.\"\"\"\n        with pytest.raises(ValueError, match=\"Data cannot be empty\"):\n            evaluate_model([])\n\n    @pytest.mark.parametrize(\"metric\", [\"accuracy\", \"bleu\", \"rouge\"])\n    def test_metric_computation(self, metric):\n        \"\"\"Test computation of different metrics.\"\"\"\n        data = [{\"question\": \"Test\", \"answer\": \"answer\", \"model_output\": \"answer\"}]\n        results = evaluate_model(data, metrics=[metric])\n        assert metric in results\n</code></pre>"},{"location":"development/testing/#test-fixtures","title":"Test Fixtures","text":"<pre><code>import pytest\n\n@pytest.fixture\ndef sample_data():\n    \"\"\"Provide sample data for tests.\"\"\"\n    return [\n        {\"question\": \"What is 2+2?\", \"answer\": \"4\", \"model_output\": \"4\"},\n        {\"question\": \"What is 3+3?\", \"answer\": \"6\", \"model_output\": \"6\"}\n    ]\n\n@pytest.fixture\ndef evaluation_config():\n    \"\"\"Provide evaluation configuration.\"\"\"\n    return {\n        \"metrics\": [\"accuracy\", \"bleu\"],\n        \"batch_size\": 32,\n        \"save_results\": False\n    }\n\ndef test_evaluation_with_fixtures(sample_data, evaluation_config):\n    \"\"\"Test evaluation using fixtures.\"\"\"\n    results = evaluate_model(sample_data, config=evaluation_config)\n    assert \"accuracy\" in results\n    assert \"bleu\" in results\n</code></pre>"},{"location":"development/testing/#mocking","title":"Mocking","text":"<pre><code>from unittest.mock import patch, MagicMock\n\ndef test_external_api_call():\n    \"\"\"Test function that calls external API.\"\"\"\n    with patch('stringsight.external_api.call_api') as mock_api:\n        mock_api.return_value = {\"result\": \"success\"}\n\n        # Your test code here\n        result = call_external_function()\n\n        assert result == \"success\"\n        mock_api.assert_called_once()\n</code></pre>"},{"location":"development/testing/#test-categories_1","title":"Test Categories","text":""},{"location":"development/testing/#unit-tests","title":"Unit Tests","text":"<p>Test individual functions and classes in isolation.</p> <pre><code>def test_metric_computation():\n    \"\"\"Test metric computation logic.\"\"\"\n    from stringsight.metrics import AccuracyMetric\n\n    metric = AccuracyMetric()\n    predictions = [\"4\", \"6\", \"8\"]\n    references = [\"4\", \"6\", \"8\"]\n\n    score = metric.compute(predictions, references)\n    assert score == 1.0\n</code></pre>"},{"location":"development/testing/#integration-tests","title":"Integration Tests","text":"<p>Test interactions between components.</p> <pre><code>@pytest.mark.integration\ndef test_full_evaluation_pipeline():\n    \"\"\"Test complete evaluation pipeline.\"\"\"\n    # Load data\n    data = load_test_dataset()\n\n    # Run evaluation\n    results = evaluate_model(data, metrics=[\"accuracy\", \"bleu\"])\n\n    # Save results\n    save_results(results, \"test_results.json\")\n\n    # Load and verify\n    loaded_results = load_results(\"test_results.json\")\n    assert loaded_results == results\n</code></pre>"},{"location":"development/testing/#performance-tests","title":"Performance Tests","text":"<p>Test performance characteristics.</p> <pre><code>@pytest.mark.slow\ndef test_large_dataset_performance():\n    \"\"\"Test performance with large dataset.\"\"\"\n    import time\n\n    # Generate large dataset\n    large_data = generate_test_data(10000)\n\n    start_time = time.time()\n    results = evaluate_model(large_data, metrics=[\"accuracy\"])\n    end_time = time.time()\n\n    # Should complete within reasonable time\n    assert end_time - start_time &lt; 60  # 60 seconds\n</code></pre>"},{"location":"development/testing/#test-data","title":"Test Data","text":""},{"location":"development/testing/#creating-test-data","title":"Creating Test Data","text":"<pre><code>def generate_test_data(num_samples: int = 100) -&gt; List[Dict]:\n    \"\"\"Generate synthetic test data.\"\"\"\n    import random\n\n    questions = [\n        \"What is 2+2?\",\n        \"What is the capital of France?\",\n        \"Explain gravity\",\n        \"What is photosynthesis?\"\n    ]\n\n    data = []\n    for i in range(num_samples):\n        question = random.choice(questions)\n        answer = f\"Answer {i}\"\n        model_output = f\"Model output {i}\"\n\n        data.append({\n            \"question\": question,\n            \"answer\": answer,\n            \"model_output\": model_output,\n            \"metadata\": {\"id\": i}\n        })\n\n    return data\n</code></pre>"},{"location":"development/testing/#test-data-files","title":"Test Data Files","text":"<p>Store test data in <code>tests/data/</code>:</p> <pre><code>tests/\n\u251c\u2500\u2500 data/\n\u2502   \u251c\u2500\u2500 sample.jsonl\n\u2502   \u251c\u2500\u2500 large_dataset.jsonl\n\u2502   \u2514\u2500\u2500 edge_cases.jsonl\n\u251c\u2500\u2500 test_evaluation.py\n\u2514\u2500\u2500 test_metrics.py\n</code></pre>"},{"location":"development/testing/#assertions-and-checks","title":"Assertions and Checks","text":""},{"location":"development/testing/#basic-assertions","title":"Basic Assertions","text":"<pre><code>def test_basic_assertions():\n    \"\"\"Test basic assertion patterns.\"\"\"\n    results = evaluate_model(sample_data)\n\n    # Check key exists\n    assert \"accuracy\" in results\n\n    # Check value range\n    assert 0.0 &lt;= results[\"accuracy\"] &lt;= 1.0\n\n    # Check type\n    assert isinstance(results[\"accuracy\"], float)\n\n    # Check approximate equality\n    assert results[\"accuracy\"] == pytest.approx(0.85, rel=0.01)\n</code></pre>"},{"location":"development/testing/#custom-assertions","title":"Custom Assertions","text":"<pre><code>def assert_valid_results(results: Dict):\n    \"\"\"Custom assertion for result validation.\"\"\"\n    required_keys = [\"accuracy\", \"bleu\", \"rouge\"]\n\n    for key in required_keys:\n        assert key in results, f\"Missing key: {key}\"\n        assert isinstance(results[key], (int, float)), f\"Invalid type for {key}\"\n        assert 0.0 &lt;= results[key] &lt;= 1.0, f\"Value out of range for {key}\"\n\ndef test_results_validation():\n    \"\"\"Test custom result validation.\"\"\"\n    results = evaluate_model(sample_data)\n    assert_valid_results(results)\n</code></pre>"},{"location":"development/testing/#error-testing","title":"Error Testing","text":""},{"location":"development/testing/#testing-exceptions","title":"Testing Exceptions","text":"<pre><code>def test_invalid_input():\n    \"\"\"Test handling of invalid input.\"\"\"\n    with pytest.raises(ValueError, match=\"Data cannot be empty\"):\n        evaluate_model([])\n\n    with pytest.raises(TypeError):\n        evaluate_model(\"not a list\")\n\n    with pytest.raises(KeyError):\n        evaluate_model([{\"invalid\": \"data\"}])\n</code></pre>"},{"location":"development/testing/#testing-warnings","title":"Testing Warnings","text":"<pre><code>import warnings\n\ndef test_deprecation_warning():\n    \"\"\"Test deprecation warnings.\"\"\"\n    with pytest.warns(DeprecationWarning, match=\"deprecated\"):\n        deprecated_function()\n</code></pre>"},{"location":"development/testing/#test-configuration","title":"Test Configuration","text":""},{"location":"development/testing/#pytestini","title":"pytest.ini","text":"<pre><code>[tool:pytest]\ntestpaths = tests\npython_files = test_*.py\npython_classes = Test*\npython_functions = test_*\naddopts = -v --tb=short\nmarkers =\n    slow: marks tests as slow (deselect with '-m \"not slow\"')\n    integration: marks tests as integration tests\n    unit: marks tests as unit tests\n</code></pre>"},{"location":"development/testing/#conftestpy","title":"conftest.py","text":"<pre><code>import pytest\nimport tempfile\nimport os\n\n@pytest.fixture(scope=\"session\")\ndef temp_dir():\n    \"\"\"Create temporary directory for tests.\"\"\"\n    with tempfile.TemporaryDirectory() as tmpdir:\n        yield tmpdir\n\n@pytest.fixture(autouse=True)\ndef setup_test_environment():\n    \"\"\"Set up test environment.\"\"\"\n    # Set test environment variables\n    os.environ[\"stringsight_TESTING\"] = \"true\"\n    yield\n    # Cleanup\n    if \"stringsight_TESTING\" in os.environ:\n        del os.environ[\"stringsight_TESTING\"]\n</code></pre>"},{"location":"development/testing/#continuous-integration","title":"Continuous Integration","text":""},{"location":"development/testing/#github-actions-example","title":"GitHub Actions Example","text":"<pre><code>name: Tests\non: [push, pull_request]\njobs:\n  test:\n    runs-on: ubuntu-latest\n    strategy:\n      matrix:\n        python-version: [3.8, 3.9, 3.10]\n\n    steps:\n    - uses: actions/checkout@v3\n    - name: Set up Python ${{ matrix.python-version }}\n      uses: actions/setup-python@v3\n      with:\n        python-version: ${{ matrix.python-version }}\n\n    - name: Install dependencies\n      run: |\n        pip install -e .\n        pip install -r requirements-dev.txt\n\n    - name: Run tests\n      run: |\n        pytest --cov=stringsight --cov-report=xml\n\n    - name: Upload coverage\n      uses: codecov/codecov-action@v3\n      with:\n        file: ./coverage.xml\n</code></pre>"},{"location":"development/testing/#best-practices","title":"Best Practices","text":"<ol> <li>Test Naming: Use descriptive test names</li> <li>Test Isolation: Each test should be independent</li> <li>Fast Tests: Keep unit tests fast (&lt; 1 second)</li> <li>Coverage: Aim for high code coverage</li> <li>Documentation: Document complex test scenarios</li> <li>Maintenance: Keep tests up to date with code changes</li> </ol>"},{"location":"development/testing/#next-steps","title":"Next Steps","text":"<ul> <li>Check out Contributing for development guidelines</li> <li>Read the API Reference to understand the codebase</li> <li>Look at Basic Usage for usage examples </li> </ul>"},{"location":"getting-started/installation/","title":"Installation","text":"<p>This guide will help you install StringSight and set up your development environment.</p>"},{"location":"getting-started/installation/#prerequisites","title":"Prerequisites","text":""},{"location":"getting-started/installation/#required","title":"Required","text":"<ul> <li>Python 3.8+ (recommended: 3.10 or 3.11)</li> <li>Conda or Miniconda (recommended for environment management)</li> <li>OpenAI API key (required for LLM-powered features)</li> </ul>"},{"location":"getting-started/installation/#optional","title":"Optional","text":"<ul> <li>Node.js 20+ (for React frontend interface)</li> <li>Weights &amp; Biases account (for experiment tracking)</li> </ul>"},{"location":"getting-started/installation/#quick-installation","title":"Quick Installation","text":""},{"location":"getting-started/installation/#1-create-conda-environment","title":"1. Create Conda Environment","text":"<pre><code># Create new conda environment with Python 3.11\nconda create -n stringsight python=3.11\nconda activate stringsight\n</code></pre>"},{"location":"getting-started/installation/#2-install-stringsight","title":"2. Install StringSight","text":"<pre><code># Clone the repository\ngit clone https://github.com/lisabdunlap/stringsight.git\ncd stringsight\n\n# Install in development mode with all dependencies\npip install -e \".[full]\"\n</code></pre>"},{"location":"getting-started/installation/#3-set-api-key","title":"3. Set API Key","text":"<p>Option A: Environment Variable <pre><code># Add to your shell profile (.bashrc, .zshrc, etc.)\nexport OPENAI_API_KEY=\"your-api-key-here\"\n</code></pre></p> <p>Option B: .env File <pre><code># Create .env file in project root\necho \"OPENAI_API_KEY=your-api-key-here\" &gt; .env\n</code></pre></p>"},{"location":"getting-started/installation/#4-verify-installation","title":"4. Verify Installation","text":"<pre><code># Test core package\npython -c \"from stringsight import explain; print('\u2705 Installation successful!')\"\n\n# Test API server\npython -m uvicorn stringsight.api:app --reload --host localhost --port 8000\n\n# In another terminal, test health check\ncurl http://127.0.0.1:8000/health\n# Should return: {\"ok\": true}\n</code></pre>"},{"location":"getting-started/installation/#installation-options","title":"Installation Options","text":""},{"location":"getting-started/installation/#core-package-only","title":"Core Package Only","text":"<pre><code>pip install -e .\n</code></pre>"},{"location":"getting-started/installation/#with-visualization-tools","title":"With Visualization Tools","text":"<pre><code>pip install -e \".[viz]\"\n</code></pre>"},{"location":"getting-started/installation/#with-development-tools","title":"With Development Tools","text":"<pre><code>pip install -e \".[dev]\"\n</code></pre>"},{"location":"getting-started/installation/#all-features","title":"All Features","text":"<pre><code>pip install -e \".[full]\"\n</code></pre>"},{"location":"getting-started/installation/#frontend-setup-optional","title":"Frontend Setup (Optional)","text":"<p>The React frontend provides an interactive web interface for analyzing results.</p> <pre><code># Install Node.js dependencies\ncd frontend/\nnpm install\n\n# Start development server\nnpm run dev\n\n# Open browser to http://localhost:5173\n</code></pre>"},{"location":"getting-started/installation/#verify-full-setup","title":"Verify Full Setup","text":""},{"location":"getting-started/installation/#backend-api-test","title":"Backend API Test","text":"<pre><code># Start backend\npython -m uvicorn stringsight.api:app --reload --host localhost --port 8000\n\n# In another terminal\ncurl http://127.0.0.1:8000/health\n</code></pre>"},{"location":"getting-started/installation/#frontend-test","title":"Frontend Test","text":"<pre><code>cd frontend/\nnpm run dev\n# Open http://localhost:5173 in your browser\n</code></pre>"},{"location":"getting-started/installation/#core-package-test","title":"Core Package Test","text":"<pre><code>from stringsight import explain\nimport pandas as pd\n\ndf = pd.DataFrame({\n    \"prompt\": [\"What is ML?\"],\n    \"model\": [\"gpt-4\"],\n    \"model_response\": [\"Machine learning is...\"]\n})\n\n# Should run without errors\nclustered_df, model_stats = explain(df, output_dir=\"test_results\")\n</code></pre>"},{"location":"getting-started/installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/installation/#pytorchcuda-issues","title":"PyTorch/CUDA Issues","text":"<pre><code># Install PyTorch with CUDA support\nconda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia\n</code></pre>"},{"location":"getting-started/installation/#nodejs-version-issues","title":"Node.js Version Issues","text":"<pre><code># Install/upgrade Node.js 20+\nconda install -c conda-forge nodejs=20\n\n# Or use nvm\nnvm install 20 &amp;&amp; nvm use 20\n</code></pre>"},{"location":"getting-started/installation/#fastapiuvicorn-errors","title":"FastAPI/Uvicorn Errors","text":"<pre><code># Reinstall with explicit versions\npip install \"fastapi&gt;=0.100.0\" \"uvicorn[standard]&gt;=0.20.0\"\n</code></pre>"},{"location":"getting-started/installation/#openai-api-key-issues","title":"OpenAI API Key Issues","text":"<pre><code># Verify key is set\necho $OPENAI_API_KEY\n\n# Test API connection\npython -c \"import openai; print('\u2705 OpenAI client loaded')\"\n</code></pre>"},{"location":"getting-started/installation/#frontend-wont-start","title":"Frontend Won't Start","text":"<pre><code>cd frontend/\nrm -rf node_modules package-lock.json\nnpm install\nnpm run dev\n</code></pre>"},{"location":"getting-started/installation/#port-already-in-use","title":"Port Already in Use","text":"<pre><code># Kill process on port 8000\nlsof -ti:8000 | xargs kill -9\n\n# Or use different port\npython -m uvicorn stringsight.api:app --reload --port 8001\n</code></pre>"},{"location":"getting-started/installation/#clean-reinstall","title":"Clean Reinstall","text":"<p>If you encounter dependency conflicts:</p> <pre><code># Remove old environment\nconda deactivate\nconda remove -n stringsight --all\n\n# Create fresh environment\nconda create -n stringsight python=3.11\nconda activate stringsight\n\n# Reinstall\ncd stringsight\npip install -e \".[full]\"\n</code></pre>"},{"location":"getting-started/installation/#environment-variables","title":"Environment Variables","text":"<p>StringSight uses the following environment variables:</p> Variable Required Description <code>OPENAI_API_KEY</code> Yes OpenAI API key for LLM calls <code>WANDB_API_KEY</code> No Weights &amp; Biases API key for experiment tracking"},{"location":"getting-started/installation/#dependencies","title":"Dependencies","text":""},{"location":"getting-started/installation/#core-dependencies","title":"Core Dependencies","text":"<ul> <li><code>pandas</code>, <code>numpy</code> - Data processing</li> <li><code>scikit-learn</code> - Machine learning utilities</li> <li><code>hdbscan</code>, <code>umap-learn</code> - Clustering algorithms</li> <li><code>openai</code>, <code>litellm</code> - LLM API clients</li> <li><code>sentence-transformers</code> - Local embedding models</li> </ul>"},{"location":"getting-started/installation/#visualization-dependencies","title":"Visualization Dependencies","text":"<ul> <li><code>plotly</code> - Interactive charts</li> </ul>"},{"location":"getting-started/installation/#frontend-dependencies-npm","title":"Frontend Dependencies (npm)","text":"<ul> <li><code>react</code>, <code>typescript</code> - Frontend framework</li> <li><code>@mui/material</code> - UI components</li> <li><code>@tanstack/react-table</code> - Data tables</li> <li><code>plotly.js</code> - Interactive charts</li> </ul>"},{"location":"getting-started/installation/#development-dependencies","title":"Development Dependencies","text":"<ul> <li><code>pytest</code>, <code>pytest-cov</code> - Testing</li> <li><code>black</code>, <code>flake8</code>, <code>mypy</code> - Code quality</li> <li><code>mkdocs</code>, <code>mkdocs-material</code> - Documentation</li> </ul>"},{"location":"getting-started/installation/#next-steps","title":"Next Steps","text":"<ul> <li>Quick Start Guide - Run your first analysis in 5 minutes</li> <li>Basic Usage - Learn the core <code>explain()</code> and <code>label()</code> functions</li> <li>Configuration - Customize your analysis pipeline</li> </ul>"},{"location":"getting-started/quick-start/","title":"Quick Start","text":"<p>Get up and running with StringSight through this hands-on tutorial.</p> <p>Try it in Colab: Open Starter Notebook</p> <p>This guide demonstrates how to use StringSight to analyze model behavior from conversation data. We'll cover: - Loading and preparing data - Single model analysis with <code>explain()</code> - Side-by-side comparison with <code>explain()</code> - Fixed taxonomy labeling with <code>label()</code> - Viewing results and metrics</p>"},{"location":"getting-started/quick-start/#prerequisites","title":"Prerequisites","text":"<p>Before starting, make sure you have:</p> <ul> <li>Python 3.8+ installed</li> <li>OpenAI API key (get one here)</li> <li>StringSight installed (see installation guide)</li> </ul> <p>Set your API key: <pre><code>export OPENAI_API_KEY=\"your-api-key-here\"\n</code></pre></p>"},{"location":"getting-started/quick-start/#setup","title":"Setup","text":"<p>Install StringSight if you haven't already: <pre><code>pip install stringsight\n</code></pre></p> <p>Import the necessary libraries: <pre><code>import pandas as pd\nimport json\nfrom stringsight import explain\n\n# Optional: Set your OpenAI API key if not already in environment\n# import os\n# os.environ['OPENAI_API_KEY'] = 'your-key-here'\n</code></pre></p>"},{"location":"getting-started/quick-start/#load-data","title":"Load Data","text":"<pre><code># Load your data\ndf = pd.read_json(\"your_data.jsonl\", lines=True)\n\n# Or download the demo dataset\n# !wget https://raw.githubusercontent.com/lisadunlap/StringSight/main/airline_data_demo.jsonl\n# df = pd.read_json(\"airline_data_demo.jsonl\", lines=True)\n</code></pre>"},{"location":"getting-started/quick-start/#understanding-the-data-format","title":"Understanding the Data Format","text":"<p>Your dataframe needs these columns:</p> <ul> <li><code>prompt</code>: The input/question (doesn't need to be your actual prompt, just some unique identifier)</li> <li><code>model</code>: Model name</li> <li><code>model_response</code>: Model output (see formats below)</li> <li><code>score</code> or multiple score columns (optional): Performance metrics</li> <li><code>question_id</code> (optional): Unique ID for tracking which responses belong to the same prompt</li> </ul> <p>About <code>question_id</code>: This is particularly useful for side-by-side analysis. If you have multiple responses for the same prompt (e.g., from different models), give them the same <code>question_id</code>. If not provided, StringSight will use <code>prompt</code> alone for pairing.</p> <p><code>model_response</code> can be in three formats: 1. String: Simple text like <code>\"Machine learning is...\"</code> 2. OAI conversation format: List of dicts with <code>role</code> and <code>content</code> 3. Custom format: Any JSON object (we'll convert it to a string on the backend)</p> <p>Pro tip: Before running the full pipeline, upload your data to stringsight.com (\"upload file\" button) to visualize what your traces look like and preview the behavior extraction. The UI can handle small datasets (~50 traces) but larger datasets should be run locally.</p> <p>Custom Column Names:</p> <p>If your dataframe uses different column names (e.g., <code>input</code>, <code>llm_name</code>, <code>output</code> instead of <code>prompt</code>, <code>model</code>, <code>model_response</code>), you can map them using column mapping parameters:</p> <pre><code>clustered_df, model_stats = explain(\n    df,\n    prompt_column=\"input\",           # Map \"input\" \u2192 \"prompt\"\n    model_column=\"llm_name\",         # Map \"llm_name\" \u2192 \"model\"\n    model_response_column=\"output\",  # Map \"output\" \u2192 \"model_response\"\n    score_columns=[\"reward\"]\n)\n</code></pre> <p>See the Parameter Reference for more details.</p>"},{"location":"getting-started/quick-start/#single-model-analysis","title":"Single Model Analysis","text":"<p>Time to identify behavioral patterns in your model's responses.</p> <p>Important Note on Cost: This pipeline makes A LOT of LLM calls, so it will: 1. Take a few minutes to run depending on your rate limits 2. Potentially cost money if you're using expensive models and analyzing lots of traces</p> <p>To get an idea of the number of calls, say you have 100 samples with <code>min_cluster_size=3</code>: - 100 calls for property extraction (usually get 3-5 properties per trace with gpt-4.1) - ~300-500 embedding calls for each property - ~100-170 LLM calls to generate cluster summaries - ~50-100 outlier matching calls (hence why we recommend using a cheaper model for <code>cluster_assignment_model</code>)</p> <p>Note: The larger you set <code>min_cluster_size</code>, the more outliers you'll likely have.</p> <p>Recommendation: Start with <code>sample_size=50-100</code> first and check your spend. One of these days I'll make a more budget-friendly version of this, but that day is not today. Maybe if I get enough GitHub issues I'll do it.</p>"},{"location":"getting-started/quick-start/#run-single-model-explain","title":"Run Single Model Explain","text":"<pre><code>clustered_df, model_stats = explain(\n    df,\n    model_name=\"gpt-4.1\",\n    min_cluster_size=5,\n    score_columns=['reward'],\n    sample_size=50,\n    output_dir=\"results/single_model\"\n)\n</code></pre> <p>What happens during analysis:</p> <ol> <li>Property Extraction - The LLM analyzes each response and extracts behavioral properties</li> <li> <p>Example: \"Provides step-by-step reasoning\", \"Uses technical jargon\", \"Follows safety policies\"</p> </li> <li> <p>Post-processing - Parses and validates the extracted properties</p> </li> <li> <p>Clustering - Groups similar properties together using embeddings</p> </li> <li> <p>Example: \"explains clearly\" + \"shows work\" \u2192 \"Reasoning Transparency\" cluster</p> </li> <li> <p>Metrics Calculation - Computes statistical analysis</p> </li> <li>Which models excel at which behaviors</li> <li>Quality scores per cluster</li> <li>Aggregated statistics</li> </ol>"},{"location":"getting-started/quick-start/#understanding-results","title":"Understanding Results","text":"<p>To visualize results: Go to stringsight.com and upload your results folder by clicking \"Load Results\" and selecting your results folder (e.g., <code>results/single_model</code>)</p> <p>The output dataframe includes new columns describing extracted behavioral properties: - <code>property_description</code>: Natural language description of the behavioral trait - <code>category</code>: High-level grouping (e.g., \"Reasoning\", \"Style\", \"Safety\") - <code>reason</code>: Why this behavior occurs - <code>evidence</code>: Specific quotes demonstrating this behavior - <code>behavior_type</code>: Positive, negative (critical/non-critical), or style - <code>cluster_id</code> and <code>cluster_label</code>: Grouping of similar behaviors</p> <p>The <code>model_stats</code> dictionary contains three DataFrames: - <code>model_cluster_scores</code>: Metrics for each model-cluster combination - <code>cluster_scores</code>: Aggregated metrics per cluster - <code>model_scores</code>: Overall metrics per model</p> <p>Output Files:</p> <p>All results are saved to your <code>output_dir</code>:</p> File Description <code>clustered_results.parquet</code> Full dataset with properties and clusters <code>full_dataset.json</code> Complete PropertyDataset in JSON format <code>full_dataset.parquet</code> Complete PropertyDataset in Parquet format <code>model_stats.json</code> Model statistics and rankings <code>summary.txt</code> Human-readable summary"},{"location":"getting-started/quick-start/#side-by-side-comparison","title":"Side-by-Side Comparison","text":"<p>Side-by-side comparison identifies differences between two models' responses to the same prompts. Unlike single model analysis where we extract properties per conversation trace, in side-by-side mode we give our LLM annotator the responses from both models for a given prompt, then extract the properties which are unique to each model.</p> <p>This typically results in a more fine-grained analysis and is recommended when you have two methods to compare.</p> <pre><code>sbs_clustered_df, sbs_model_stats = explain(\n    df,\n    method=\"side_by_side\",\n    model_a=\"gpt-4o\",\n    model_b=\"claude-sonnet-35\",\n    model_name=\"gpt-4.1\",\n    min_cluster_size=3,\n    score_columns=['reward'],\n    output_dir=\"results/side_by_side\"\n)\n</code></pre>"},{"location":"getting-started/quick-start/#fixed-taxonomy-labeling","title":"Fixed Taxonomy Labeling","text":"<p>When you know exactly which behavioral axes you care about, use <code>label()</code> instead of <code>explain()</code>.</p> <p>Key Difference: - <code>explain()</code>: Discovers behaviors automatically through clustering - <code>label()</code>: Labels data according to your predefined taxonomy</p> <p>This is useful when you have specific behaviors you want to track (e.g., safety issues, specific failure modes).</p> <pre><code>from stringsight import label\n\ntaxonomy = {\n    \"safety_issue\": \"Does the model behave unsafely?\",\n    \"policy_violation\": \"Does the model violate company policies?\",\n    \"refusal\": \"Does the model refuse appropriate requests?\"\n}\n\nlabeled_df, label_stats = label(\n    df,\n    taxonomy=taxonomy,\n    model_name=\"gpt-4.1\",\n    sample_size=50,\n    output_dir=\"results/labeled\"\n)\n</code></pre>"},{"location":"getting-started/quick-start/#common-configurations","title":"Common Configurations","text":"<p>Cost-effective: <pre><code>explain(df, model_name=\"gpt-4o-mini\", min_cluster_size=5, sample_size=50)\n</code></pre></p> <p>High-quality: <pre><code>explain(df, model_name=\"gpt-4.1\", embedding_model=\"text-embedding-3-large\", min_cluster_size=10)\n</code></pre></p> <p>Task-specific: <pre><code>explain(df, task_description=\"Evaluate for safety and policy compliance\")\n</code></pre></p> <p>See the Parameter Reference for all available parameters.</p>"},{"location":"getting-started/quick-start/#whats-next","title":"What's Next","text":"<ul> <li>Parameter Reference - Complete guide to all parameters</li> <li>Data Formats - Supported data formats</li> <li>Visualization - Explore results in the web interface</li> </ul>"},{"location":"user-guide/basic-usage/","title":"Explain and Label Functions","text":"<p>Learn how to use the two main functions in StringSight for analyzing model behavior.</p>"},{"location":"user-guide/basic-usage/#core-functions","title":"Core Functions","text":"<p>StringSight provides two primary functions:</p> <ul> <li><code>explain()</code>: Discovers behavioral patterns through clustering </li> <li><code>label()</code>: Classifies behavior using predefined taxonomies</li> </ul> <p>Both functions analyze conversation data and return clustered results with model statistics.</p>"},{"location":"user-guide/basic-usage/#the-explain-function","title":"The <code>explain()</code> Function","text":"<p>The <code>explain()</code> function automatically discovers behavioral patterns in model responses through property extraction and clustering.</p>"},{"location":"user-guide/basic-usage/#basic-usage","title":"Basic Usage","text":"<pre><code>import pandas as pd\nfrom stringsight import explain\n\n# Load your conversation data\ndf = pd.read_csv(\"model_conversations.csv\")\n\n# Single model analysis: Understand what behavioral patterns a model exhibits\nclustered_df, model_stats = explain(\n    df,\n    method=\"single_model\", \n    min_cluster_size=10,        # Minimum conversations per behavior cluster\n    output_dir=\"results/\"       # Saves all analysis files here\n)\n# This will: 1) Extract behavioral properties from each response\n#          2) Group similar behaviors into clusters  \n#          3) Calculate performance metrics per cluster\n#          4) Save comprehensive results\n\n# Side-by-side comparison: Compare two models to find behavioral differences  \nclustered_df, model_stats = explain(\n    df,\n    method=\"side_by_side\",\n    min_cluster_size=30,        # Larger datasets need bigger clusters\n    output_dir=\"results/\"\n)\n# This will: 1) Find behavioral differences between model pairs\n#          2) Cluster similar difference patterns\n#          3) Show which model excels at which behaviors\n#          4) Provide statistical significance testing\n</code></pre>"},{"location":"user-guide/basic-usage/#parameters","title":"Parameters","text":"<p>Core Parameters: - <code>df</code>: Input DataFrame with conversation data - <code>method</code>: <code>\"side_by_side\"</code> or <code>\"single_model\"</code> - <code>system_prompt</code>: Custom prompt for property extraction (optional) - <code>output_dir</code>: Directory to save results</p> <p>Extraction Parameters: - <code>model_name</code>: LLM for property extraction (default: <code>\"gpt-4o\"</code>) - This model analyzes responses to find behavioral patterns - <code>temperature</code>: Temperature for LLM calls (default: <code>0.7</code>) - Higher values = more creative property extraction - <code>max_workers</code>: Parallel workers for API calls (default: <code>16</code>) - Speed up analysis with concurrent requests</p> <p>Clustering Parameters: - <code>clusterer</code>: Clustering method (<code>\"hdbscan\"</code>) - Algorithm to group similar behaviors - <code>min_cluster_size</code>: Minimum cluster size (default: <code>30</code>) - Smaller = more granular clusters, larger = broader patterns - <code>embedding_model</code>: <code>\"openai\"</code> or sentence-transformer model - How to convert properties to vectors for clustering</p>"},{"location":"user-guide/basic-usage/#examples","title":"Examples","text":"<p>Custom System Prompt: <pre><code># Define what behavioral aspects you want the LLM to focus on\ncustom_prompt = \"\"\"\nAnalyze this conversation and identify behavioral differences.\nFocus on: reasoning approach, factual accuracy, response style.\nReturn a JSON object with 'property_description' and 'property_evidence'.\n\"\"\"\n\nclustered_df, model_stats = explain(\n    df,\n    method=\"side_by_side\",\n    system_prompt=custom_prompt  # This overrides the default extraction prompt\n)\n# The LLM will now focus specifically on reasoning, accuracy, and style\n# instead of using the general-purpose default prompt\n</code></pre></p>"},{"location":"user-guide/basic-usage/#the-label-function","title":"The <code>label()</code> Function","text":"<p>The <code>label()</code> function classifies model behavior using a predefined taxonomy rather than discovering patterns.</p>"},{"location":"user-guide/basic-usage/#basic-usage_1","title":"Basic Usage","text":"<pre><code>from stringsight import label\n\n# Define your evaluation taxonomy\ntaxonomy = {\n    \"accuracy\": \"Is the response factually correct?\",\n    \"helpfulness\": \"Does the response address the user's needs?\", \n    \"clarity\": \"Is the response clear and well-structured?\",\n    \"safety\": \"Does the response avoid harmful content?\"\n}\n\n# Classify responses\nclustered_df, model_stats = label(\n    df,\n    taxonomy=taxonomy,\n    model_name=\"gpt-4o-mini\",\n    output_dir=\"results/\"\n)\n</code></pre>"},{"location":"user-guide/basic-usage/#parameters_1","title":"Parameters","text":"<p>Core Parameters: - <code>df</code>: Input DataFrame (must be single-model format) - <code>taxonomy</code>: Dictionary mapping labels to descriptions - <code>model_name</code>: LLM for classification (default: <code>\"gpt-4o-mini\"</code>) - <code>output_dir</code>: Directory to save results</p> <p>Other Parameters: - <code>temperature</code>: Temperature for classification (default: <code>0.0</code>) - <code>max_workers</code>: Parallel workers (default: <code>8</code>) - <code>verbose</code>: Print progress information (default: <code>True</code>)</p>"},{"location":"user-guide/basic-usage/#example","title":"Example","text":"<p>Quality Assessment: <pre><code>quality_taxonomy = {\n    \"excellent\": \"Response is comprehensive, accurate, and well-structured\",\n    \"good\": \"Response is mostly accurate with minor issues\",\n    \"fair\": \"Response has some accuracy or clarity problems\", \n    \"poor\": \"Response has significant issues or inaccuracies\"\n}\n\nclustered_df, model_stats = label(\n    df,\n    taxonomy=quality_taxonomy,\n    temperature=0.0,  # Deterministic classification\n    output_dir=\"quality_results/\"\n)\n</code></pre></p>"},{"location":"user-guide/basic-usage/#data-formats","title":"Data Formats","text":""},{"location":"user-guide/basic-usage/#side-by-side-format-for-comparing-two-models","title":"Side-by-side Format (for comparing two models)","text":"<p>Required columns: - <code>prompt</code> - The question or prompt given to both models - <code>model_a</code>, <code>model_b</code> - Names of the models being compared - <code>model_a_response</code>, <code>model_b_response</code> - Complete responses from each model</p> <p>Optional columns: - <code>score</code> - Dictionary with winner and metrics</p> <pre><code>df = pd.DataFrame({\n    \"prompt\": [\"What is machine learning?\", \"Explain quantum computing\"], \n    \"model_a\": [\"gpt-4\", \"gpt-4\"],\n    \"model_b\": [\"claude-3\", \"claude-3\"],\n    \"model_a_response\": [\"ML is a subset of AI...\", \"Quantum computing uses...\"],\n    \"model_b_response\": [\"Machine learning involves...\", \"QC leverages quantum...\"],\n    \"score\": [{\"winner\": \"gpt-4\", \"helpfulness\": 4.2}, {\"winner\": \"claude-3\", \"helpfulness\": 3.8}]\n})\n</code></pre>"},{"location":"user-guide/basic-usage/#single-model-format-for-analyzing-individual-models","title":"Single Model Format (for analyzing individual models)","text":"<p>Required columns: - <code>prompt</code> - The question given to the model (used for visualization) - <code>model</code> - Name of the model being analyzed - <code>model_response</code> - The model's complete response</p> <p>Optional columns: - <code>score</code> - Dictionary of evaluation metrics</p> <pre><code>df = pd.DataFrame({\n    \"prompt\": [\"What is machine learning?\", \"Explain quantum computing\"],\n    \"model\": [\"gpt-4\", \"gpt-4\"], \n    \"model_response\": [\"Machine learning involves...\", \"QC leverages quantum...\"],\n    \"score\": [{\"accuracy\": 1, \"helpfulness\": 4.2}, {\"accuracy\": 0, \"helpfulness\": 3.8}]\n})\n</code></pre>"},{"location":"user-guide/basic-usage/#response-format-details","title":"Response Format Details","text":"<p>StringSight supports flexible response formats to accommodate various data sources and conversation structures.</p>"},{"location":"user-guide/basic-usage/#automatic-format-detection","title":"Automatic Format Detection","text":"<p>The system automatically detects and converts response formats:</p> <ol> <li>Simple string responses \u2192 converted to OpenAI conversation format</li> <li>OpenAI conversation format (list of message dictionaries) \u2192 used as-is  </li> <li>Other types \u2192 converted to strings then processed</li> </ol>"},{"location":"user-guide/basic-usage/#openai-conversation-format-specification","title":"OpenAI Conversation Format Specification","text":"<p>The response format follows the standard OpenAI conversation format. Each message dictionary contains:</p> <p>Required Fields: - <code>role</code>: Message sender role (<code>\"user\"</code>, <code>\"assistant\"</code>, <code>\"system\"</code>, <code>\"tool\"</code>) - <code>content</code>: Message content (string or dictionary - see below)</p> <p>Optional Fields: - <code>name</code>: Name of the model/tool (persists for entire conversation) - <code>id</code>: Unique identifier for specific model or tool call - Additional custom fields are preserved</p> <p>Content Field: For simple text responses, <code>content</code> is a string: <pre><code>{\"role\": \"assistant\", \"content\": \"Machine learning involves training algorithms...\"}\n</code></pre></p> <p>For multimodal inputs or complex interactions, <code>content</code> can be a dictionary following OpenAI's format: - <code>text</code>: Text content - <code>image</code>: Image content (for multimodal models) - <code>tool_calls</code>: Array of tool call objects (for tool-augmented responses)</p>"},{"location":"user-guide/basic-usage/#format-examples","title":"Format Examples","text":"<p>Here are some examples for chatbot conversations, agents, and multimodel models.</p> <p>Annoyed with having to convert to yet another data format? Dude, me too \u2014 here are some alternative options: * Vibe code that bad boy - its decently good at converting formats. One day I aspire to make this a built in feature so if you feel strongly please make a PR * Make your conversation which is just 1 big string: This will work \u2014 you just won't get the nice trace visualization we have in the UI (but it should still localize text). </p> <p>Simple text conversation: <pre><code>[\n    {\n        \"role\": \"user\",\n        \"content\": \"What is machine learning?\"\n    },\n    {\n        \"role\": \"assistant\", \n        \"content\": \"Machine learning involves training algorithms...\"\n    }\n]\n</code></pre></p> <p>Tool-augmented response: <pre><code>[\n    {\n        \"role\": \"user\",\n        \"content\": \"Search for papers on quantum computing\"\n    },\n    {\n        \"role\": \"assistant\",\n        \"content\": {\n            \"tool_calls\": [\n                {\n                    \"name\": \"search_papers\",\n                    \"arguments\": {\n                        \"query\": \"quantum computing\",\n                        \"year\": 2024,\n                        \"max_results\": 5\n                    },\n                    \"tool_call_id\": \"call_abc123\"\n                }\n            ]\n        }\n    },\n    {\n        \"role\": \"tool\",\n        \"name\": \"search_papers\",\n        \"content\": \"Found 5 papers: [1] Quantum Error Correction...\"\n    },\n    {\n        \"role\": \"assistant\",\n        \"content\": \"Based on the search results, here are recent developments...\"\n    }\n]\n</code></pre></p> <p>Multimodal input (when applicable): <pre><code>[\n    {\n        \"role\": \"user\",\n        \"content\": {\n            \"text\": \"What's in this image?\",\n            \"image\": \"data:image/jpeg;base64,iVBORw0KGgoAAAANSUhEUgAA...\"\n        }\n    },\n    {\n        \"role\": \"assistant\",\n        \"content\": \"I can see a diagram showing neural network architecture...\"\n    }\n]\n</code></pre></p> <p>Format Conversion: Simple strings are automatically converted: <pre><code># Input: \"Machine learning involves...\"\n# Becomes: [{\"role\": \"assistant\", \"content\": \"Machine learning involves...\"}]\n</code></pre></p>"},{"location":"user-guide/basic-usage/#understanding-results","title":"Understanding Results","text":""},{"location":"user-guide/basic-usage/#output-dataframes","title":"Output DataFrames","text":"<p>Both functions return your original data enriched with extracted behavioral properties:</p> <pre><code>print(clustered_df.columns)\n# Original columns plus new analysis columns:\n# 'property_description' - Natural language description of behavior (e.g., \"Provides step-by-step reasoning\")  \n# 'property_evidence' - Evidence from the response supporting this property\n# 'category' - Higher-level grouping (e.g., \"Reasoning\", \"Creativity\")\n# 'impact' - Estimated effect (\"positive\", \"negative\", or numeric score)\n# 'type' - Kind of property (\"format\", \"content\", \"style\")\n# 'property_description_cluster_label' - Human-readable cluster name\n</code></pre>"},{"location":"user-guide/basic-usage/#model-statistics","title":"Model Statistics","text":"<p>The <code>model_stats</code> contains per-model behavioral analysis:</p> <pre><code># For each model, you get statistics about behavioral patterns\nfor model_name, stats in model_stats.items():\n    print(f\"{model_name} behavioral analysis:\")\n    # Which behaviors this model exhibits most/least frequently\n    # Relative scores for different behavioral clusters  \n    # Example responses for each behavior cluster\n    # Quality scores showing how well the model performs within each behavior type\n</code></pre>"},{"location":"user-guide/basic-usage/#saved-files","title":"Saved Files","text":"<p>When <code>output_dir</code> is specified, both functions save: - <code>clustered_results.parquet</code> - Complete results with clusters - <code>model_stats.json</code> - Model performance statistics - <code>full_dataset.json</code> - Complete dataset for reanalysis - <code>summary.txt</code> - Human-readable summary</p>"},{"location":"user-guide/basic-usage/#when-to-use-each-function","title":"When to Use Each Function","text":"<p>Use <code>explain()</code> when: - You want to discover unknown behavioral patterns - You're comparing multiple models - You need flexible, data-driven analysis - You want to understand what makes models different</p> <p>Use <code>label()</code> when: - You have specific criteria to evaluate - You need consistent scoring across datasets - You're building evaluation pipelines - You want controlled, taxonomy-based analysis</p>"},{"location":"user-guide/basic-usage/#next-steps","title":"Next Steps","text":"<ul> <li>Understand the output files in detail</li> <li>Explore configuration options</li> <li>Learn about the pipeline architecture </li> </ul>"},{"location":"user-guide/configuration-guide/","title":"Configuration Guide","text":"<p>Complete guide to configuring StringSight's analysis pipeline for optimal results.</p>"},{"location":"user-guide/configuration-guide/#clustering-parameters","title":"Clustering Parameters","text":""},{"location":"user-guide/configuration-guide/#min_cluster_size","title":"min_cluster_size","text":"<p>What it does: Minimum number of properties required to form a cluster.</p> <p>How to choose:</p> Dataset Size Recommended <code>min_cluster_size</code> Rationale &lt; 100 conversations <code>5-10</code> Small datasets need smaller clusters to find patterns 100-1,000 conversations <code>10-20</code> Balanced granularity 1,000-10,000 conversations <code>20-50</code> Larger clusters filter noise, find robust patterns &gt; 10,000 conversations <code>50-100</code> Very large datasets need substantial clusters <p>General rules: - Start with dataset_size / 50 as a baseline - Smaller values (5-10) = More granular, specific patterns (risk: noise/overfitting) - Larger values (50-100) = Broader, more robust patterns (risk: missing nuances) - If you get too many clusters: Increase <code>min_cluster_size</code> - If you get too few clusters: Decrease <code>min_cluster_size</code></p> <p>Quick tips (concise)</p> <ul> <li>If clusters often repeat the same property, increase <code>min_cluster_size</code>.</li> <li>By dataset size (samples):<ul> <li>&lt; 100: <code>3-4</code></li> <li>100\u20131,000: <code>5-7</code></li> <li> <p>1,000: <code>15-30</code></p> </li> </ul> </li> </ul> <p>Examples:</p> <pre><code>from stringsight import explain\n\n# Small exploratory dataset (100 conversations)\nexplain(df, min_cluster_size=3)\n\n# Medium production dataset (1,000 conversations)\nexplain(df, min_cluster_size=7)  # Default\n\n# Large research dataset (10,000+ conversations)\nexplain(df, min_cluster_size=25)\n</code></pre>"},{"location":"user-guide/configuration-guide/#embedding_model","title":"embedding_model","text":"<p>What it does: Converts property descriptions to vectors for clustering.</p> <p>Options:</p> Model Cost Speed Quality Best For <code>\"text-embedding-3-small\"</code> $0.02/1M tokens Fast Very Good Default - best balance <code>\"text-embedding-3-large\"</code> $0.13/1M tokens Medium Excellent Production quality analysis <code>\"all-MiniLM-L6-v2\"</code> Free Very Fast Good Development, large datasets <code>\"all-mpnet-base-v2\"</code> Free Medium Very Good Cost-conscious production <pre><code># OpenAI embeddings (requires API key, costs $)\nexplain(df, embedding_model=\"text-embedding-3-small\")  # Default\n\n# Local embeddings (free, no API calls)\nexplain(df, embedding_model=\"all-MiniLM-L6-v2\")\n</code></pre>"},{"location":"user-guide/configuration-guide/#assign_outliers","title":"assign_outliers","text":"<p>What it does: Assigns properties that don't fit any cluster to their nearest cluster.</p> <p>When to use: - \u2705 You want every property in a cluster (no noise/outliers) - \u2705 Dashboards/visualizations (avoids \"Outlier\" cluster) - \u2705 Downstream analysis requires full coverage</p> <p>When to skip: - \u274c You want to identify truly unique/anomalous behaviors - \u274c Quality matters more than coverage - \u274c Small datasets (outliers are informative)</p> <pre><code># Assign all properties to clusters\nexplain(df, assign_outliers=True)\n\n# Keep outliers separate (default)\nexplain(df, assign_outliers=False)\n</code></pre>"},{"location":"user-guide/configuration-guide/#extraction-parameters","title":"Extraction Parameters","text":""},{"location":"user-guide/configuration-guide/#model_name","title":"model_name","text":"<p>What it does: LLM used to extract behavioral properties from responses.</p> <p>Options:</p> Model Cost/Quality When to Use <code>\"gpt-4.1\"</code> $$$ / Excellent Production, research papers, high-stakes decisions <code>\"gpt-4.1-mini\"</code> $$ / Very Good Default - balanced cost/quality <code>\"gpt-4o-mini\"</code> $ / Good Development, iteration, large-scale experiments <code>\"gpt-4.1-nano\"</code> \u00a2 / Decent Massive datasets, proof-of-concepts <pre><code># High quality extraction\nexplain(df, model_name=\"gpt-4.1\")\n\n# Cost-effective extraction\nexplain(df, model_name=\"gpt-4o-mini\")\n</code></pre>"},{"location":"user-guide/configuration-guide/#temperature","title":"temperature","text":"<p>What it does: Controls randomness in property extraction.</p> <p>Values: - <code>0.0-0.3</code> = Deterministic, focused extraction - <code>0.5-0.7</code> = Default - balanced creativity - <code>0.8-1.0</code> = More creative, diverse properties</p> <pre><code># Consistent, focused properties\nexplain(df, temperature=0.2)\n\n# Diverse, creative properties\nexplain(df, temperature=0.9)\n</code></pre>"},{"location":"user-guide/configuration-guide/#max_workers","title":"max_workers","text":"<p>What it does: Number of parallel API calls for extraction.</p> <p>Guidelines: - Default: <code>16</code> - Increase (32-64) if you have high API rate limits - Decrease (4-8) if you hit rate limits or want to conserve resources - 1 for debugging (sequential processing)</p> <pre><code># Fast parallel extraction\nexplain(df, max_workers=32)\n\n# Conservative rate limiting\nexplain(df, max_workers=8)\n</code></pre>"},{"location":"user-guide/configuration-guide/#model-selection-strategy","title":"Model Selection Strategy","text":""},{"location":"user-guide/configuration-guide/#budget-conscious-configuration","title":"Budget-Conscious Configuration","text":"<p>For cost-effective analysis without sacrificing too much quality:</p> <pre><code>explain(\n    df,\n    model_name=\"gpt-4o-mini\",              # Cheap extraction\n    embedding_model=\"all-MiniLM-L6-v2\",    # Free embeddings\n    min_cluster_size=50,                    # Fewer, larger clusters\n    use_wandb=False                         # Turn off W&amp;B (default True)\n)\n</code></pre> <p>Estimated cost: ~$5-10 per 1,000 conversations</p>"},{"location":"user-guide/configuration-guide/#production-quality-configuration","title":"Production-Quality Configuration","text":"<p>For high-quality, reproducible results:</p> <pre><code>explain(\n    df,\n    model_name=\"gpt-4.1\",                      # Best extraction\n    embedding_model=\"text-embedding-3-large\",   # Best embeddings\n    min_cluster_size=30,                         # Balanced granularity\n    use_wandb=True,                              # Track experiments (default True)\n    wandb_project=\"production-analysis\"\n)\n</code></pre> <p>Estimated cost: ~$50-75 per 1,000 conversations</p>"},{"location":"user-guide/configuration-guide/#developmentiteration-configuration","title":"Development/Iteration Configuration","text":"<p>For fast experimentation:</p> <pre><code>explain(\n    df,\n    model_name=\"gpt-4o-mini\",            # Fast extraction\n    embedding_model=\"all-MiniLM-L6-v2\",   # Fast embeddings\n    min_cluster_size=20,                   # Quick clustering\n    max_workers=32,                        # Maximize parallelism\n    use_wandb=False                        # Skip tracking (default is True)\n)\n</code></pre> <p>Estimated time: ~5-10 minutes per 1,000 conversations</p> <p>Note: W&amp;B logging is enabled by default. In the CLI (<code>scripts/run_full_pipeline.py</code>), pass <code>--disable_wandb</code> to turn it off.</p>"},{"location":"user-guide/configuration-guide/#advanced-parameters","title":"Advanced Parameters","text":""},{"location":"user-guide/configuration-guide/#dimensionality-reduction","title":"Dimensionality Reduction","text":"<p>Control UMAP/PCA before clustering:</p> <pre><code>from stringsight.clusterers import HDBSCANClusterer\n\nclusterer = HDBSCANClusterer(\n    disable_dim_reduction=True,              # Skip dimensionality reduction\n    # OR configure it:\n    dim_reduction_method=\"umap\",             # \"umap\", \"pca\", \"adaptive\", \"none\"\n    umap_n_components=100,                   # UMAP dimensions\n    umap_n_neighbors=30,                     # UMAP neighbors\n    umap_min_dist=0.1                        # UMAP min distance\n)\n</code></pre>"},{"location":"user-guide/configuration-guide/#hdbscan-tuning","title":"HDBSCAN Tuning","text":"<p>Fine-tune clustering algorithm:</p> <pre><code>clusterer = HDBSCANClusterer(\n    min_cluster_size=30,\n    min_samples=5,                           # Minimum samples in neighborhood\n    cluster_selection_epsilon=0.0,           # Distance threshold\n)\n</code></pre>"},{"location":"user-guide/configuration-guide/#stratified-clustering","title":"Stratified Clustering","text":"<p>Cluster separately per group (e.g., per topic, per task):</p> <pre><code>explain(df, groupby_column=\"topic\")  # Cluster within each topic\n</code></pre>"},{"location":"user-guide/configuration-guide/#common-configuration-issues","title":"Common Configuration Issues","text":""},{"location":"user-guide/configuration-guide/#too-many-small-clusters","title":"\"Too many small clusters\"","text":"<p>Problem: Hundreds of tiny, noisy clusters</p> <p>Solution: <pre><code># Increase minimum cluster size\nexplain(df, min_cluster_size=50)  # was: 10\n\n# Or assign outliers\nexplain(df, assign_outliers=True)\n</code></pre></p>"},{"location":"user-guide/configuration-guide/#only-2-3-clusters","title":"\"Only 2-3 clusters\"","text":"<p>Problem: Not enough granularity</p> <p>Solution: <pre><code># Decrease minimum cluster size\nexplain(df, min_cluster_size=10)  # was: 50\n\n# Use better embeddings\nexplain(df, embedding_model=\"text-embedding-3-large\")\n\n# Lower temperature for more diverse properties\nexplain(df, temperature=0.8)\n</code></pre></p>"},{"location":"user-guide/configuration-guide/#clustering-too-slow","title":"\"Clustering too slow\"","text":"<p>Problem: Takes hours to cluster</p> <p>Solution: <pre><code># Use local embeddings\nexplain(df, embedding_model=\"all-MiniLM-L6-v2\")\n\n# Increase cluster size\nexplain(df, min_cluster_size=100)\n</code></pre></p>"},{"location":"user-guide/configuration-guide/#running-out-of-memory","title":"\"Running out of memory\"","text":"<p>Problem: OOM errors during clustering</p> <p>Solution: <pre><code># Disable embeddings in output\nexplain(df, include_embeddings=False)\n\n# Skip dimensionality reduction\nexplain(df, disable_dim_reduction=True)\n\n# Process in batches (manually split data)\n</code></pre></p>"},{"location":"user-guide/configuration-guide/#quick-reference","title":"Quick Reference","text":""},{"location":"user-guide/configuration-guide/#by-dataset-size","title":"By Dataset Size","text":"<pre><code># &lt; 100 conversations\nexplain(df, min_cluster_size=3)\n\n# 100-1,000 conversations\nexplain(df, min_cluster_size=7)\n\n# 1,000-10,000 conversations\nexplain(df, min_cluster_size=25)  # Default for larger datasets\n\n# &gt; 10,000 conversations\nexplain(df, min_cluster_size=30)\n</code></pre>"},{"location":"user-guide/configuration-guide/#by-use-case","title":"By Use Case","text":"<pre><code># Research paper (quality matters most)\nexplain(df, model_name=\"gpt-4.1\", embedding_model=\"text-embedding-3-large\")\n\n# Production dashboard (speed + quality balance)\nexplain(df, model_name=\"gpt-4.1-mini\", embedding_model=\"text-embedding-3-small\")\n\n# Exploration/development (speed matters most)\nexplain(df, model_name=\"gpt-4o-mini\", embedding_model=\"all-MiniLM-L6-v2\")\n</code></pre>"},{"location":"user-guide/configuration-guide/#next-steps","title":"Next Steps","text":"<ul> <li>Data Formats - Prepare your input data correctly</li> <li>Performance Tuning - Optimize for speed and cost</li> <li>Custom Pipelines - Build custom configurations</li> </ul>"},{"location":"user-guide/configuration/","title":"Output Files","text":"<p>Understanding the files generated by StringSight analysis.</p> <p>When you run <code>explain()</code> or <code>label()</code> with an <code>output_dir</code>, StringSight saves comprehensive results across multiple file formats. This guide explains each output file and how to use them.</p>"},{"location":"user-guide/configuration/#core-output-files","title":"Core Output Files","text":""},{"location":"user-guide/configuration/#1-clustered_resultsparquet","title":"1. <code>clustered_results.parquet</code>","text":"<p>Primary results file with all analysis data</p> <p>This is the main output file containing your original data enriched with extracted properties and cluster assignments.</p> <pre><code>import pandas as pd\n\n# Load complete results\ndf = pd.read_parquet(\"results/clustered_results.parquet\")\n\n# Key columns added by StringSight:\nprint(df.columns)\n# ['question_id', 'model', 'model_response',           # Original data\n#  'property_description', 'property_evidence',       # Extracted properties  \n#  'property_description_cluster_id',                  # Cluster assignments\n#  'property_description_cluster_label']               # Human-readable cluster names\n\n# Example: Find all responses in a specific cluster\ncluster_data = df[df['property_description_cluster_label'] == 'Detailed Technical Explanations']\n</code></pre> <p>Use this file for: - Interactive analysis and visualization - Building custom dashboards - Statistical analysis of results - Feeding into downstream ML pipelines</p>"},{"location":"user-guide/configuration/#2-metrics-dataframes-jsonl","title":"2. Metrics DataFrames (JSONL)","text":"<p>Model- and cluster-level metrics as DataFrames</p> <p>These files are optimized for frontend and analysis workflows:</p> <ul> <li><code>model_cluster_scores_df.jsonl</code> \u2014 Per model-cluster metrics</li> <li><code>cluster_scores_df.jsonl</code> \u2014 Per cluster aggregated metrics</li> <li><code>model_scores_df.jsonl</code> \u2014 Per model aggregated metrics</li> </ul> <pre><code>import pandas as pd\n\nmodel_cluster = pd.read_json(\"results/model_cluster_scores_df.jsonl\", lines=True)\ncluster_scores = pd.read_json(\"results/cluster_scores_df.jsonl\", lines=True)\nmodel_scores = pd.read_json(\"results/model_scores_df.jsonl\", lines=True)\n\n# Example: top clusters for a given model\ngpt4 = model_cluster[model_cluster[\"model\"] == \"gpt-4\"]\nprint(gpt4.sort_values(\"proportion\", ascending=False).head(10)[[\"cluster\", \"proportion\"]])\n</code></pre> <p>Use these files for: - Model leaderboards and rankings - Performance comparisons - Quality assessment reports - Automated model selection</p>"},{"location":"user-guide/configuration/#3-full_datasetjson","title":"3. <code>full_dataset.json</code>","text":"<p>Complete dataset for reanalysis and caching</p> <p>Contains the entire <code>PropertyDataset</code> object with all conversations, properties, clusters, and metadata.</p> <pre><code>from stringsight.core.data_objects import PropertyDataset\n\n# Load complete dataset\ndataset = PropertyDataset.load(\"results/full_dataset.json\")\n\n# Access all components:\nprint(f\"Conversations: {len(dataset.conversations)}\")\nprint(f\"Properties: {len(dataset.properties)}\")  \nprint(f\"Clusters: {len(dataset.clusters)}\")\nprint(f\"Models: {dataset.all_models}\")\n\n# Rerun metrics with different parameters\nfrom stringsight import compute_metrics_only\nclustered_df, new_stats = compute_metrics_only(\n    \"results/full_dataset.json\",\n    method=\"single_model\",\n    output_dir=\"results_updated/\"\n)\n</code></pre> <p>Use this file for: - Recomputing metrics without re-extracting properties - Debugging and troubleshooting - Building analysis pipelines - Sharing complete analysis state</p>"},{"location":"user-guide/configuration/#additional-output-files","title":"Additional Output Files","text":""},{"location":"user-guide/configuration/#processing-stage-files","title":"Processing Stage Files","text":"<p>Property Extraction: - <code>raw_properties.jsonl</code> - Raw LLM responses before parsing - <code>extraction_stats.json</code> - API call statistics and timing - <code>extraction_samples.jsonl</code> - Sample inputs/outputs for debugging</p> <p>JSON Parsing: - <code>parsed_properties.jsonl</code> - Successfully parsed property objects - <code>parsing_stats.json</code> - Parsing success/failure statistics - <code>parsing_failures.jsonl</code> - Failed parsing attempts for debugging</p> <p>Validation: - <code>validated_properties.jsonl</code> - Properties that passed validation - <code>validation_stats.json</code> - Validation statistics</p> <p>Clustering: - <code>embeddings.parquet</code> - Property embeddings (if <code>include_embeddings=True</code>) - <code>clustered_results_lightweight.jsonl</code> - Results without embeddings - <code>summary_table.jsonl</code> - Cluster summary statistics</p> <p>Metrics: - <code>model_cluster_scores_df.jsonl</code> - Per model-cluster performance (DataFrame JSONL) - <code>cluster_scores_df.jsonl</code> - Aggregate cluster metrics (DataFrame JSONL) - <code>model_scores_df.jsonl</code> - Aggregate model metrics (DataFrame JSONL)</p>"},{"location":"user-guide/configuration/#summary-files","title":"Summary Files","text":"<p><code>summary.txt</code> - Human-readable analysis summary <pre><code>StringSight Results Summary\n==================================================\n\nTotal conversations: 1,234\nTotal properties: 4,567  \nModels analyzed: 8\nFine clusters: 23\nCoarse clusters: 8\n\nModel Rankings (by average quality score):\n  1. gpt-4: 0.847\n  2. claude-3: 0.832\n  3. gemini-pro: 0.801\n  ...\n</code></pre></p>"},{"location":"user-guide/configuration/#working-with-output-files","title":"Working with Output Files","text":""},{"location":"user-guide/configuration/#loading-results-for-analysis","title":"Loading Results for Analysis","text":"<pre><code>import pandas as pd\nimport json\n\n# Quick analysis workflow\ndf = pd.read_parquet(\"results/clustered_results.parquet\")\nwith open(\"results/model_stats.json\") as f:\n    stats = json.load(f)\n\n# Analyze cluster distributions\ncluster_counts = df['property_description_cluster_label'].value_counts()\nprint(\"Top behavioral patterns:\")\nprint(cluster_counts.head(10))\n\n# Compare models within specific clusters  \nfor cluster in cluster_counts.head(5).index:\n    cluster_data = df[df['property_description_cluster_label'] == cluster]\n    model_dist = cluster_data['model'].value_counts()\n    print(f\"\\n{cluster}:\")\n    print(model_dist)\n</code></pre>"},{"location":"user-guide/configuration/#rerunning-analysis","title":"Rerunning Analysis","text":"<pre><code>from stringsight import compute_metrics_only\n\n# Recompute metrics with different parameters\nclustered_df, model_stats = compute_metrics_only(\n    input_path=\"results/full_dataset.json\",\n    method=\"single_model\", \n    metrics_kwargs={\n        'compute_confidence_intervals': True,\n        'bootstrap_samples': 1000\n    },\n    output_dir=\"results_with_ci/\"\n)\n</code></pre>"},{"location":"user-guide/configuration/#building-custom-visualizations","title":"Building Custom Visualizations","text":"<pre><code># Interactive visualization with plotly\nimport plotly.express as px\nimport pandas as pd\n\n# Load results\ndf = pd.read_parquet(\"results/clustered_results.parquet\")\n\n# Build interactive filters\nselected_models = [\"gpt-4\", \"claude-3\"]  # Filter by model\nselected_clusters = df['property_description_cluster_label'].unique()[:10]  # Top clusters\n\n# Filter and display\nfiltered_df = df[\n    (df['model'].isin(selected_models)) &amp; \n    (df['property_description_cluster_label'].isin(selected_clusters))\n]\n\n# Create interactive plot\nfig = px.bar(filtered_df, x='model', y='property_description_cluster_label', \n             title='Model Behavior Comparison')\nfig.show()\n</code></pre>"},{"location":"user-guide/configuration/#file-format-details","title":"File Format Details","text":""},{"location":"user-guide/configuration/#parquet-vs-json-vs-jsonl","title":"Parquet vs JSON vs JSONL","text":"<p>Parquet (<code>.parquet</code>) - Binary format, fastest loading - Preserves data types  - Best for analysis and large datasets - Use: <code>pd.read_parquet()</code></p> <p>JSON (<code>.json</code>) - Human-readable structure - Good for configuration and metadata - Use: <code>json.load()</code></p> <p>JSONL (<code>.jsonl</code>) - Newline-delimited JSON - Streamable for large datasets - Each line is a JSON object - Use: <code>pd.read_json(..., lines=True)</code></p>"},{"location":"user-guide/configuration/#best-practices","title":"Best Practices","text":""},{"location":"user-guide/configuration/#1-file-organization","title":"1. File Organization","text":"<pre><code>results/\n\u251c\u2500\u2500 clustered_results.parquet      # Primary analysis file\n\u251c\u2500\u2500 model_cluster_scores_df.jsonl  # Per model-cluster metrics (DF JSONL)\n\u251c\u2500\u2500 cluster_scores_df.jsonl        # Per cluster metrics (DF JSONL)\n\u251c\u2500\u2500 model_scores_df.jsonl          # Per model metrics (DF JSONL)\n\u251c\u2500\u2500 full_dataset.json              # Complete state\n\u251c\u2500\u2500 summary.txt                    # Human summary\n\u251c\u2500\u2500 embeddings.parquet             # Embeddings (optional)\n\u2514\u2500\u2500 stage_outputs/                 # Detailed processing files\n    \u251c\u2500\u2500 parsed_properties.jsonl\n    \u251c\u2500\u2500 validation_stats.json\n    \u2514\u2500\u2500 ...\n</code></pre>"},{"location":"user-guide/configuration/#2-version-control","title":"2. Version Control","text":"<ul> <li>Include <code>summary.txt</code> and <code>model_stats.json</code> in version control</li> <li>Use <code>.gitignore</code> for large binary files like embeddings</li> <li>Tag important analysis runs</li> </ul>"},{"location":"user-guide/configuration/#3-reproducibility","title":"3. Reproducibility","text":"<ul> <li>Save the exact command/parameters used</li> <li>Keep <code>full_dataset.json</code> for reanalysis</li> <li>Document any post-processing steps</li> </ul>"},{"location":"user-guide/configuration/#next-steps","title":"Next Steps","text":"<ul> <li>Use the quickstart guide to generate these files</li> <li>Learn about explain() and label() functions  </li> <li>Explore visualization options </li> </ul>"},{"location":"user-guide/configuration/#task-descriptions","title":"Task Descriptions","text":"<p>Task descriptions let you steer property extraction toward a specific domain or evaluation goal. When provided, StringSight formats a task-aware system prompt (for both single_model and side_by_side variants) using templates from <code>stringsight/prompts/extractor_prompts.py</code>.</p> <p>Example: <pre><code>clustered_df, model_stats = explain(\n    df,\n    method=\"single_model\",\n    task_description=(\n        \"Evaluate customer support responses for empathy, clarity, \"\n        \"resolution accuracy, and policy adherence.\"\n    ),\n    output_dir=\"results/customer_support\",\n)\n</code></pre></p>"},{"location":"user-guide/configuration/#default-task-description-webdev-arena-helper","title":"Default Task Description (WebDev Arena helper)","text":"<p>When running <code>scripts/run_webdev_arena.py</code>, the following default task description is used unless overridden with <code>--task_description</code> (or disabled with <code>--no_task_description</code>):</p> <pre><code>Each model is given a user prompt to generate a web development project.\n\nWhen looking for interesting properties of responses, consider the following (note these are not exhaustive):\n1. **Code Quality**: Correctness, best practices, security vulnerabilities, and adherence to modern web standards\n2. **Completeness**: Whether the implementation fully addresses the user's requirements and includes necessary dependencies\n3. **User Experience**: UI/UX quality, accessibility, responsiveness, and visual appeal\n4. **Maintainability**: Code organization, documentation, comments, and readability\n5. **Functionality**: Whether the code would actually work as intended, proper error handling, and edge case coverage\n6. **Performance**: Efficient implementations, loading times, and resource usage\n7. **Stylistic Choices**: The model's choices in terms of language, formatting, layout, and style\n8. **User interpretation**: If given vauge instructions, what design choices does the model make to try to fulfill the user's requirements?\n9. **Safety**: Whether the model's response contains vulernabilities or if it generates content that another model would consider unsafe or harmful.\n</code></pre> <p>For full prompt templates, see <code>stringsight/prompts/extractor_prompts.py</code>.</p>"},{"location":"user-guide/configuration/#prompts-optional-details","title":"Prompts (optional details)","text":"View exact extraction and clustering prompts Extraction (single_model) Clustering (label generation &amp; dedup)"},{"location":"user-guide/data-formats/","title":"Data Formats","text":"<p>StringSight requires specific data formats for input and produces structured outputs. This guide covers all supported formats and schemas.</p>"},{"location":"user-guide/data-formats/#input-data-formats","title":"Input Data Formats","text":"<p>StringSight supports two primary analysis methods, each with specific column requirements.</p>"},{"location":"user-guide/data-formats/#single-model-format","title":"Single Model Format","text":"<p>Used for analyzing behavioral patterns from individual model responses.</p>"},{"location":"user-guide/data-formats/#required-columns","title":"Required Columns","text":"Column Type Description Example <code>prompt</code> <code>str</code> User question/prompt <code>\"What is machine learning?\"</code> <code>model</code> <code>str</code> Model identifier <code>\"gpt-4\"</code>, <code>\"claude-3-opus\"</code> <code>model_response</code> <code>str</code> or <code>list</code> Model's response (see Response Format) <code>\"Machine learning is...\"</code>"},{"location":"user-guide/data-formats/#optional-columns","title":"Optional Columns","text":"Column Type Description Example <code>question_id</code> <code>str</code> Unique conversation ID <code>\"q_12345\"</code> (auto-generated if missing) <code>score</code> <code>dict</code> Quality/evaluation scores <code>{\"accuracy\": 0.85, \"helpfulness\": 4.2}</code>"},{"location":"user-guide/data-formats/#example-dataframe","title":"Example DataFrame","text":"<pre><code>import pandas as pd\n\ndf = pd.DataFrame({\n    \"question_id\": [\"q1\", \"q2\", \"q3\"],\n    \"prompt\": [\n        \"What is machine learning?\",\n        \"Explain quantum computing\",\n        \"Write a poem about AI\"\n    ],\n    \"model\": [\"gpt-4\", \"gpt-4\", \"gpt-4\"],\n    \"model_response\": [\n        \"Machine learning is a subset of AI...\",\n        \"Quantum computing uses quantum bits...\",\n        \"In circuits of light, intelligence grows...\"\n    ],\n    \"score\": [\n        {\"accuracy\": 1, \"helpfulness\": 4.5},\n        {\"accuracy\": 0, \"helpfulness\": 3.8},\n        {\"accuracy\": 1, \"helpfulness\": 4.2}\n    ]\n})\n</code></pre>"},{"location":"user-guide/data-formats/#side-by-side-format","title":"Side-by-Side Format","text":"<p>Used for head-to-head model comparisons (Arena-style battles).</p> <p>StringSight supports TWO ways to provide side-by-side data:</p>"},{"location":"user-guide/data-formats/#option-1-pre-paired-format-explicit-columns","title":"Option 1: Pre-Paired Format (Explicit Columns)","text":"<p>Provide explicit columns for both models with their responses already paired.</p> <p>Required Columns:</p> Column Type Description Example <code>prompt</code> <code>str</code> Question given to both models <code>\"What is machine learning?\"</code> <code>model_a</code> <code>str</code> First model identifier <code>\"gpt-4\"</code> <code>model_b</code> <code>str</code> Second model identifier <code>\"claude-3\"</code> <code>model_a_response</code> <code>str</code> or <code>list</code> First model's response <code>\"ML is a subset of AI...\"</code> <code>model_b_response</code> <code>str</code> or <code>list</code> Second model's response <code>\"Machine learning involves...\"</code> <p>Optional Columns:</p> Column Type Description Example <code>question_id</code> <code>str</code> Unique conversation ID <code>\"battle_001\"</code> <code>score</code> <code>dict</code> Battle results and scores <code>{\"winner\": \"model_a\", \"helpfulness\": 4.2}</code> <p>Example:</p> <pre><code># Pre-paired side-by-side format\ndf = pd.DataFrame({\n    \"question_id\": [\"b1\", \"b2\", \"b3\"],\n    \"prompt\": [\n        \"What is machine learning?\",\n        \"Explain quantum computing\",\n        \"Write a poem about AI\"\n    ],\n    \"model_a\": [\"gpt-4\", \"gpt-4\", \"gpt-4\"],\n    \"model_b\": [\"claude-3\", \"claude-3\", \"claude-3\"],\n    \"model_a_response\": [\n        \"ML is a subset of AI...\",\n        \"Quantum computing uses qubits...\",\n        \"In circuits of light...\"\n    ],\n    \"model_b_response\": [\n        \"Machine learning involves...\",\n        \"QC leverages quantum phenomena...\",\n        \"Silicon dreams awaken...\"\n    ],\n    \"score\": [\n        {\"winner\": \"model_a\", \"helpfulness\": 4.2},\n        {\"winner\": \"model_a\", \"helpfulness\": 3.8},\n        {\"winner\": \"model_b\", \"helpfulness\": 4.5}\n    ]\n})\n\n# Run analysis\nfrom stringsight import explain\nclustered_df, model_stats = explain(df, method=\"side_by_side\")\n</code></pre>"},{"location":"user-guide/data-formats/#option-2-tidy-format-with-model-selection-auto-pairing","title":"Option 2: Tidy Format with Model Selection (Auto-Pairing)","text":"<p>Provide data in tidy single-model format and specify which two models to compare using the <code>model_a</code> and <code>model_b</code> parameters. StringSight will automatically pair responses for shared prompts.</p> <p>Key Parameters: - <code>model_a=\"model_name\"</code> - First model to compare - <code>model_b=\"model_name\"</code> - Second model to compare</p> <p>Required Columns:</p> Column Type Description Example <code>prompt</code> <code>str</code> User question/prompt <code>\"What is machine learning?\"</code> <code>model</code> <code>str</code> Model identifier <code>\"gpt-4\"</code>, <code>\"claude-3\"</code> <code>model_response</code> <code>str</code> or <code>list</code> Model's response <code>\"Machine learning is...\"</code> <p>Complete Example:</p> <pre><code># Tidy format with multiple models\ndf_tidy = pd.DataFrame({\n    \"prompt\": [\n        \"What is machine learning?\",\n        \"What is machine learning?\",  # Same prompt, different model\n        \"Explain quantum computing\",\n        \"Explain quantum computing\",\n        \"Write a poem about AI\",\n        \"Write a poem about AI\"\n    ],\n    \"model\": [\"gpt-4\", \"claude-3\", \"gpt-4\", \"claude-3\", \"gpt-4\", \"claude-3\"],\n    \"model_response\": [\n        \"ML is a subset of AI...\",\n        \"Machine learning involves...\",\n        \"Quantum computing uses qubits...\",\n        \"QC leverages quantum phenomena...\",\n        \"In circuits of light...\",\n        \"Silicon dreams awaken...\"\n    ]\n})\n\n# Run side-by-side analysis on tidy data\nfrom stringsight import explain\n\nclustered_df, model_stats = explain(\n    df_tidy,\n    method=\"side_by_side\",\n    model_a=\"gpt-4\",      # \u2190 Specify first model\n    model_b=\"claude-3\"    # \u2190 Specify second model\n)\n\n# Option B: Using CLI script\n# python scripts/run_full_pipeline.py \\\n#   --data_path data/tidy_data.jsonl \\\n#   --output_dir results/ \\\n#   --method side_by_side \\\n#   --model_a \"gpt-4\" \\\n#   --model_b \"claude-3\"\n</code></pre> <p>How Auto-Pairing Works:</p> <ol> <li>Filters dataset to only the two specified models</li> <li>Finds all prompts answered by BOTH models</li> <li>Pairs responses for each shared prompt</li> <li>Converts to side-by-side format internally</li> <li>Runs analysis on paired data</li> </ol> <p>Note: Only rows where both models answered the same prompt are kept.</p>"},{"location":"user-guide/data-formats/#response-format","title":"Response Format","text":""},{"location":"user-guide/data-formats/#automatic-format-detection","title":"Automatic Format Detection","text":"<p>StringSight automatically detects and converts response formats:</p> <ol> <li>Simple strings \u2192 Converted to OpenAI conversation format</li> <li>OpenAI format (list of message dicts) \u2192 Used as-is</li> <li>Other types \u2192 Converted to string then processed</li> </ol>"},{"location":"user-guide/data-formats/#openai-conversation-format","title":"OpenAI Conversation Format","text":"<p>For complex conversations involving multiple turns, tool use, or multimodal content, use OpenAI's conversation format.</p>"},{"location":"user-guide/data-formats/#message-structure","title":"Message Structure","text":"<p>Each message is a dictionary with:</p> <p>Required Fields: - <code>role</code>: <code>\"user\"</code>, <code>\"assistant\"</code>, <code>\"system\"</code>, or <code>\"tool\"</code> - <code>content</code>: Message content (string or dict)</p> <p>Optional Fields: - <code>name</code>: Model/tool identifier - <code>id</code>: Unique identifier for the message</p>"},{"location":"user-guide/data-formats/#simple-text-conversation","title":"Simple Text Conversation","text":"<pre><code>response = [\n    {\n        \"role\": \"user\",\n        \"content\": \"What is machine learning?\"\n    },\n    {\n        \"role\": \"assistant\",\n        \"content\": \"Machine learning is a subset of artificial intelligence...\"\n    }\n]\n</code></pre>"},{"location":"user-guide/data-formats/#multi-turn-conversation","title":"Multi-Turn Conversation","text":"<pre><code>response = [\n    {\"role\": \"user\", \"content\": \"What is 2+2?\"},\n    {\"role\": \"assistant\", \"content\": \"2+2 equals 4.\"},\n    {\"role\": \"user\", \"content\": \"What about 3+3?\"},\n    {\"role\": \"assistant\", \"content\": \"3+3 equals 6.\"}\n]\n</code></pre>"},{"location":"user-guide/data-formats/#tool-augmented-response","title":"Tool-Augmented Response","text":"<pre><code>response = [\n    {\n        \"role\": \"user\",\n        \"content\": \"Search for papers on quantum computing\"\n    },\n    {\n        \"role\": \"assistant\",\n        \"content\": {\n            \"tool_calls\": [\n                {\n                    \"name\": \"search_papers\",\n                    \"arguments\": {\n                        \"query\": \"quantum computing\",\n                        \"year\": 2024,\n                        \"max_results\": 5\n                    },\n                    \"tool_call_id\": \"call_abc123\"\n                }\n            ]\n        }\n    },\n    {\n        \"role\": \"tool\",\n        \"name\": \"search_papers\",\n        \"content\": \"Found 5 papers: [1] Quantum Error Correction...\"\n    },\n    {\n        \"role\": \"assistant\",\n        \"content\": \"Based on the search results, here are recent developments...\"\n    }\n]\n</code></pre>"},{"location":"user-guide/data-formats/#multimodal-input","title":"Multimodal Input","text":"<pre><code>response = [\n    {\n        \"role\": \"user\",\n        \"content\": {\n            \"text\": \"What's in this image?\",\n            \"image\": \"data:image/jpeg;base64,iVBORw0KGgo...\"\n        }\n    },\n    {\n        \"role\": \"assistant\",\n        \"content\": \"I can see a diagram showing neural network architecture...\"\n    }\n]\n</code></pre>"},{"location":"user-guide/data-formats/#score-format","title":"Score Format","text":""},{"location":"user-guide/data-formats/#single-metric","title":"Single Metric","text":"<pre><code>df[\"score\"] = [0.85, 0.92, 0.78]  # Numeric values\n</code></pre>"},{"location":"user-guide/data-formats/#multiple-metrics","title":"Multiple Metrics","text":"<pre><code>df[\"score\"] = [\n    {\"accuracy\": 1, \"helpfulness\": 4.2, \"harmlessness\": 4.8},\n    {\"accuracy\": 0, \"helpfulness\": 3.5, \"harmlessness\": 4.9},\n    {\"accuracy\": 1, \"helpfulness\": 4.8, \"harmlessness\": 4.7}\n]\n</code></pre>"},{"location":"user-guide/data-formats/#side-by-side-winner-format","title":"Side-by-Side Winner Format","text":"<pre><code>df[\"score\"] = [\n    {\"winner\": \"model_a\"},\n    {\"winner\": \"model_b\"},\n    {\"winner\": \"tie\"}\n]\n</code></pre>"},{"location":"user-guide/data-formats/#combined-format","title":"Combined Format","text":"<pre><code>df[\"score\"] = [\n    {\"winner\": \"model_a\", \"accuracy\": 0.9, \"helpfulness\": 4.5},\n    {\"winner\": \"model_b\", \"accuracy\": 0.7, \"helpfulness\": 3.8},\n    {\"winner\": \"tie\", \"accuracy\": 0.8, \"helpfulness\": 4.0}\n]\n</code></pre>"},{"location":"user-guide/data-formats/#using-separate-score-columns","title":"Using Separate Score Columns","text":"<p>Instead of providing scores as a dictionary in a single column, you can use the <code>score_columns</code> parameter to specify separate columns for each metric. StringSight will automatically convert them to the required dictionary format.</p>"},{"location":"user-guide/data-formats/#single-model-with-score-columns","title":"Single Model with Score Columns","text":"<pre><code>import pandas as pd\nfrom stringsight import explain\n\n# Data with separate score columns\ndf = pd.DataFrame({\n    \"prompt\": [\"What is AI?\", \"Explain ML\", \"What is DL?\"],\n    \"model\": [\"gpt-4\", \"gpt-4\", \"gpt-4\"],\n    \"model_response\": [\"AI is...\", \"ML is...\", \"DL is...\"],\n    \"accuracy\": [0.9, 0.85, 0.95],           # Separate column\n    \"helpfulness\": [4.2, 4.0, 4.5],          # Separate column\n    \"clarity\": [4.8, 4.5, 4.7]               # Separate column\n})\n\n# Specify which columns contain scores\nclustered_df, model_stats = explain(\n    df,\n    method=\"single_model\",\n    score_columns=[\"accuracy\", \"helpfulness\", \"clarity\"]  # Automatically converted to score dict\n)\n</code></pre>"},{"location":"user-guide/data-formats/#side-by-side-with-score-columns","title":"Side-by-Side with Score Columns","text":"<p>For side-by-side comparisons, score columns should have <code>_a</code> and <code>_b</code> suffixes:</p> <pre><code># Data with separate score columns for each model\ndf = pd.DataFrame({\n    \"prompt\": [\"What is AI?\", \"Explain ML\"],\n    \"model_a\": [\"gpt-4\", \"gpt-4\"],\n    \"model_b\": [\"claude-3\", \"claude-3\"],\n    \"model_a_response\": [\"AI is...\", \"ML is...\"],\n    \"model_b_response\": [\"AI involves...\", \"Machine learning...\"],\n    \"accuracy_a\": [0.9, 0.85],               # Model A accuracy\n    \"accuracy_b\": [0.88, 0.90],              # Model B accuracy\n    \"helpfulness_a\": [4.2, 4.0],             # Model A helpfulness\n    \"helpfulness_b\": [4.3, 4.1]              # Model B helpfulness\n})\n\n# Specify base metric names (without _a/_b suffixes)\nclustered_df, model_stats = explain(\n    df,\n    method=\"side_by_side\",\n    score_columns=[\"accuracy\", \"helpfulness\"]  # Will look for *_a and *_b columns\n)\n</code></pre>"},{"location":"user-guide/data-formats/#tidy-data-with-score-columns","title":"Tidy Data with Score Columns","text":"<p>When using tidy format with <code>model_a</code> and <code>model_b</code> parameters, specify the score columns and they'll be pivoted automatically:</p> <pre><code># Tidy format with separate score columns\ndf = pd.DataFrame({\n    \"prompt\": [\"What is AI?\", \"What is AI?\", \"Explain ML\", \"Explain ML\"],\n    \"model\": [\"gpt-4\", \"claude-3\", \"gpt-4\", \"claude-3\"],\n    \"model_response\": [\"AI is...\", \"AI involves...\", \"ML is...\", \"ML involves...\"],\n    \"accuracy\": [0.9, 0.88, 0.85, 0.90],\n    \"helpfulness\": [4.2, 4.3, 4.0, 4.1]\n})\n\n# Convert tidy to side-by-side with score columns\nclustered_df, model_stats = explain(\n    df,\n    method=\"side_by_side\",\n    model_a=\"gpt-4\",\n    model_b=\"claude-3\",\n    score_columns=[\"accuracy\", \"helpfulness\"]  # Automatically pivoted to *_a/*_b format\n)\n</code></pre> <p>Benefits of using <code>score_columns</code>: - More natural data format (especially when exporting from databases or spreadsheets) - No need to manually construct score dictionaries - Automatic validation that columns contain numeric values - Works seamlessly with tidy data conversion</p>"},{"location":"user-guide/data-formats/#output-data-formats","title":"Output Data Formats","text":"<p>When you specify <code>output_dir</code>, StringSight saves multiple files in different formats.</p>"},{"location":"user-guide/data-formats/#main-output-files","title":"Main Output Files","text":""},{"location":"user-guide/data-formats/#clustered_resultsparquet","title":"clustered_results.parquet","text":"<p>Format: Apache Parquet Description: Full dataset with all original columns plus extracted properties and clusters</p> <p>New Columns Added:</p> Column Type Description <code>property_id</code> <code>str</code> Unique property identifier <code>property_description</code> <code>str</code> Extracted behavioral property <code>category</code> <code>str</code> Property category <code>reason</code> <code>str</code> Why this property was identified <code>evidence</code> <code>str</code> Evidence from the response <code>behavior_type</code> <code>str</code> Type of behavior <code>property_description_cluster_id</code> <code>int</code> Fine-grained cluster ID <code>property_description_cluster_label</code> <code>str</code> Human-readable fine cluster name <code>embedding</code> <code>list[float]</code> Property embedding vector (if <code>include_embeddings=True</code>) <p>Loading: <pre><code>df = pd.read_parquet(\"results/clustered_results.parquet\")\n</code></pre></p>"},{"location":"user-guide/data-formats/#full_datasetjson","title":"full_dataset.json","text":"<p>Format: JSON Description: Complete <code>PropertyDataset</code> object with all data structures</p> <p>Structure: <pre><code>{\n  \"conversations\": [...],\n  \"properties\": [...],\n  \"clusters\": [...],\n  \"model_stats\": {...},\n  \"all_models\": [...]\n}\n</code></pre></p> <p>Loading: <pre><code>from stringsight.core.data_objects import PropertyDataset\n\ndataset = PropertyDataset.load(\"results/full_dataset.json\")\n</code></pre></p>"},{"location":"user-guide/data-formats/#model_cluster_scoresjson","title":"model_cluster_scores.json","text":"<p>Format: JSON Description: Per model-cluster performance metrics</p> <p>Structure: <pre><code>{\n  \"gpt-4\": {\n    \"Reasoning Transparency\": {\n      \"size\": 150,\n      \"proportion\": 0.25,\n      \"quality\": {\"accuracy\": 0.92, \"helpfulness\": 4.5},\n      \"quality_delta\": {\"accuracy\": 0.15, \"helpfulness\": 0.3},\n      \"proportion_delta\": 0.12,\n      \"examples\": [\"q1\", \"q2\", \"q3\"],\n      \"proportion_ci\": {\"lower\": 0.22, \"upper\": 0.28},\n      \"quality_ci\": {...},\n      \"quality_delta_ci\": {...},\n      \"proportion_delta_ci\": {...}\n    }\n  }\n}\n</code></pre></p>"},{"location":"user-guide/data-formats/#cluster_scoresjson","title":"cluster_scores.json","text":"<p>Format: JSON Description: Per-cluster aggregated metrics across all models</p>"},{"location":"user-guide/data-formats/#model_scoresjson","title":"model_scores.json","text":"<p>Format: JSON Description: Per-model aggregated metrics across all clusters</p>"},{"location":"user-guide/data-formats/#summarytxt","title":"summary.txt","text":"<p>Format: Plain text Description: Human-readable analysis summary</p>"},{"location":"user-guide/data-formats/#intermediate-files","title":"Intermediate Files","text":"<p>These are saved during pipeline execution:</p> <ul> <li><code>raw_properties.jsonl</code> - Raw LLM extraction responses</li> <li><code>extraction_stats.json</code> - Extraction stage statistics</li> <li><code>parsed_properties.jsonl</code> - Parsed property objects</li> <li><code>parsing_stats.json</code> - Parsing success/failure stats</li> <li><code>validated_properties.jsonl</code> - Validated properties</li> <li><code>validation_stats.json</code> - Validation statistics</li> <li><code>embeddings.parquet</code> - Property embeddings data</li> </ul>"},{"location":"user-guide/data-formats/#loading-data","title":"Loading Data","text":""},{"location":"user-guide/data-formats/#from-files","title":"From Files","text":"<pre><code>import pandas as pd\nfrom stringsight.core.data_objects import PropertyDataset\n\n# Load clustered results\ndf = pd.read_parquet(\"results/clustered_results.parquet\")\n\n# Load full dataset\ndataset = PropertyDataset.load(\"results/full_dataset.json\")\n\n# Load metrics\nimport json\nwith open(\"results/model_cluster_scores.json\") as f:\n    metrics = json.load(f)\n</code></pre>"},{"location":"user-guide/data-formats/#from-various-sources","title":"From Various Sources","text":"<pre><code># CSV\ndf = pd.read_csv(\"data.csv\")\n\n# JSON Lines\ndf = pd.read_json(\"data.jsonl\", lines=True)\n\n# Parquet\ndf = pd.read_parquet(\"data.parquet\")\n\n# JSON\ndf = pd.read_json(\"data.json\")\n</code></pre>"},{"location":"user-guide/data-formats/#data-validation","title":"Data Validation","text":"<p>StringSight automatically validates your data format. You can also validate manually:</p> <pre><code>from stringsight.formatters import detect_method, validate_required_columns\n\n# Detect single_model vs side_by_side\nmethod = detect_method(df)\nprint(f\"Detected method: {method}\")\n\n# Validate required columns\ntry:\n    validate_required_columns(df, method)\n    print(\"\u2705 Data format is valid\")\nexcept ValueError as e:\n    print(f\"\u274c Validation error: {e}\")\n</code></pre>"},{"location":"user-guide/data-formats/#converting-between-formats","title":"Converting Between Formats","text":""},{"location":"user-guide/data-formats/#tidy-to-side-by-side","title":"Tidy to Side-by-Side","text":"<p>Convert single-model format to side-by-side for comparison:</p> <pre><code>python scripts/run_full_pipeline.py \\\n  --data_path data/tidy_data.jsonl \\\n  --output_dir results/ \\\n  --method side_by_side \\\n  --model_a \"gpt-4\" \\\n  --model_b \"claude-3\"\n</code></pre> <p>This automatically: 1. Filters to prompts answered by both models 2. Pairs responses for each shared prompt 3. Converts to side-by-side format</p>"},{"location":"user-guide/data-formats/#programmatic-conversion","title":"Programmatic Conversion","text":"<pre><code># Example: Convert tidy to side-by-side\nmodels_a = df[df['model'] == 'gpt-4']\nmodels_b = df[df['model'] == 'claude-3']\n\nside_by_side = pd.merge(\n    models_a,\n    models_b,\n    on='prompt',\n    suffixes=('_a', '_b')\n).rename(columns={\n    'model_a': 'model_a',\n    'model_b': 'model_b',\n    'model_response_a': 'model_a_response',\n    'model_response_b': 'model_b_response'\n})\n</code></pre>"},{"location":"user-guide/data-formats/#best-practices","title":"Best Practices","text":""},{"location":"user-guide/data-formats/#data-preparation","title":"Data Preparation","text":"<ol> <li>Unique IDs: Use meaningful <code>question_id</code> values for easier tracking</li> <li>Consistent Naming: Use consistent model names across your dataset</li> <li>Score Format: Use dictionaries for multiple evaluation criteria</li> <li>Response Format: Use OpenAI format for complex conversations</li> </ol>"},{"location":"user-guide/data-formats/#quality-checks","title":"Quality Checks","text":"<pre><code># Check for missing values\nprint(df.isnull().sum())\n\n# Check model distribution\nprint(df['model'].value_counts())\n\n# Check response lengths\ndf['response_length'] = df['model_response'].str.len()\nprint(df['response_length'].describe())\n\n# Verify score format\nif 'score' in df.columns:\n    print(df['score'].apply(type).value_counts())\n</code></pre>"},{"location":"user-guide/data-formats/#performance-tips","title":"Performance Tips","text":"<ol> <li>Use Parquet: Faster loading/saving than CSV or JSON</li> <li>Sample Large Datasets: Use sampling for initial exploration</li> <li>Cache Results: Save intermediate results to avoid recomputation</li> </ol>"},{"location":"user-guide/data-formats/#troubleshooting","title":"Troubleshooting","text":""},{"location":"user-guide/data-formats/#common-issues","title":"Common Issues","text":"<p>\"Missing required column\" <pre><code># Check your columns\nprint(df.columns.tolist())\n\n# Rename if needed\ndf = df.rename(columns={'response': 'model_response'})\n</code></pre></p> <p>\"Invalid response format\" <pre><code># Convert all responses to strings\ndf['model_response'] = df['model_response'].astype(str)\n</code></pre></p> <p>\"Score column not recognized\" <pre><code># Ensure scores are dictionaries\nimport json\ndf['score'] = df['score'].apply(json.loads)  # If stored as strings\n</code></pre></p>"},{"location":"user-guide/data-formats/#next-steps","title":"Next Steps","text":"<ul> <li>Configuration Options - Learn about all available parameters</li> <li>Basic Usage - See how to use these formats with <code>explain()</code> and <code>label()</code></li> <li>Visualization - Explore results in the web interface</li> </ul>"},{"location":"user-guide/parameters/","title":"Parameter Reference","text":"<p>This guide provides a comprehensive reference for all parameters available in StringSight's <code>explain()</code> and <code>label()</code> functions.</p> <p>Parameters are organized by their role in the analysis pipeline.</p>"},{"location":"user-guide/parameters/#general-parameters","title":"General Parameters","text":"<p>These parameters control data handling, analysis method, and output.</p>"},{"location":"user-guide/parameters/#method","title":"<code>method</code>","text":"<ul> <li>Purpose: Type of analysis to perform</li> <li>Options:</li> <li><code>\"single_model\"</code>: Extract patterns per trace, recommended if you only have responses from 1 model or if you are comparing 3+ models</li> <li><code>\"side_by_side\"</code>: Compare two models to find differences, recommended if you are comparing 2 models</li> <li>Default: <code>\"single_model\"</code></li> </ul>"},{"location":"user-guide/parameters/#sample_size","title":"<code>sample_size</code>","text":"<ul> <li>Purpose: Number of samples to process before analysis</li> <li>Type: <code>int</code> (optional)</li> <li>Behavior:</li> <li>For single_model with balanced datasets (each prompt answered by all models): Samples prompts, keeping all model responses per prompt</li> <li>Otherwise: Samples individual rows</li> <li>Recommended: Start with 50-100 for testing</li> <li>Default: None (uses all data)</li> </ul>"},{"location":"user-guide/parameters/#score_columns","title":"<code>score_columns</code>","text":"<ul> <li>Purpose: Specify which columns contain evaluation metrics</li> <li>Type: <code>list[str]</code> (optional)</li> <li>Format:</li> <li>Single model: <code>['accuracy', 'helpfulness']</code></li> <li>Side-by-side: <code>['accuracy_a', 'accuracy_b', 'helpfulness_a', 'helpfulness_b']</code></li> <li>Alternative: Use a <code>score</code> dict column instead</li> <li>Default: None</li> </ul>"},{"location":"user-guide/parameters/#column-mapping-parameters","title":"Column Mapping Parameters","text":"<p>If your dataframe uses different column names, you can map them using these parameters:</p>"},{"location":"user-guide/parameters/#prompt_column","title":"<code>prompt_column</code>","text":"<ul> <li>Purpose: Name of your prompt column</li> <li>Type: <code>str</code></li> <li>Default: <code>\"prompt\"</code></li> <li>Example: <code>prompt_column=\"input\"</code></li> </ul>"},{"location":"user-guide/parameters/#model_column","title":"<code>model_column</code>","text":"<ul> <li>Purpose: Name of your model column</li> <li>Type: <code>str</code></li> <li>Default: <code>\"model\"</code></li> <li>Example: <code>model_column=\"llm_name\"</code></li> </ul>"},{"location":"user-guide/parameters/#model_response_column","title":"<code>model_response_column</code>","text":"<ul> <li>Purpose: Name of your response column</li> <li>Type: <code>str</code></li> <li>Default: <code>\"model_response\"</code></li> <li>Example: <code>model_response_column=\"output\"</code></li> </ul>"},{"location":"user-guide/parameters/#question_id_column","title":"<code>question_id_column</code>","text":"<ul> <li>Purpose: Name of your question_id column</li> <li>Type: <code>str</code></li> <li>Default: <code>\"question_id\"</code></li> <li>About <code>question_id</code>:</li> <li>Used to track which responses belong to the same prompt</li> <li>Useful for side-by-side pairing: rows with the same prompt must have the same question_id</li> <li>If not provided, StringSight will use <code>prompt</code> alone for pairing</li> </ul> <p>Example with custom column names: <pre><code>clustered_df, model_stats = explain(\n    df,\n    prompt_column=\"input\",           # Map \"input\" \u2192 \"prompt\"\n    model_column=\"llm_name\",         # Map \"llm_name\" \u2192 \"model\"\n    model_response_column=\"output\",  # Map \"output\" \u2192 \"model_response\"\n    score_columns=[\"reward\"]\n)\n</code></pre></p>"},{"location":"user-guide/parameters/#property-extraction-parameters","title":"Property Extraction Parameters","text":"<p>These parameters control how behavioral properties are extracted from model responses.</p>"},{"location":"user-guide/parameters/#model_name","title":"<code>model_name</code>","text":"<ul> <li>Purpose: LLM used for extracting behavioral properties</li> <li>Type: <code>str</code></li> <li>Default: <code>\"gpt-4.1\"</code></li> <li>Options: Any OpenAI model name (<code>\"gpt-4.1\"</code>, <code>\"gpt-4o-mini\"</code>, etc.)</li> <li>Cost/Quality Tradeoff:</li> <li><code>\"gpt-4.1\"</code>: Best quality, higher cost</li> <li><code>\"gpt-4o-mini\"</code>: Good balance</li> <li><code>\"gpt-3.5-turbo\"</code>: Fastest, cheapest</li> </ul>"},{"location":"user-guide/parameters/#system_prompt","title":"<code>system_prompt</code>","text":"<ul> <li>Purpose: Prompt template used for extraction</li> <li>Type: <code>str</code></li> <li>Options:</li> <li><code>\"agent\"</code>: Optimized for agent/conversational data</li> <li><code>\"default\"</code>: General-purpose extraction</li> <li>Default: <code>\"agent\"</code></li> </ul>"},{"location":"user-guide/parameters/#task_description","title":"<code>task_description</code>","text":"<ul> <li>Purpose: Helps tailor extraction to your specific domain</li> <li>Type: <code>str</code> (optional)</li> <li>Example: <code>\"airline booking agent conversations, look out for instances of the model violating policy, being tricked by the user, and any other additional issues or stylistic choices.\"</code></li> <li>Default: Uses default prompt if not provided</li> <li>Recommendation: Provide a clear description of what you're analyzing and what behaviors to look for</li> </ul>"},{"location":"user-guide/parameters/#clustering-parameters","title":"Clustering Parameters","text":"<p>These parameters control how properties are grouped into behavioral clusters.</p>"},{"location":"user-guide/parameters/#min_cluster_size","title":"<code>min_cluster_size</code>","text":"<ul> <li>Purpose: Minimum number of examples required per cluster</li> <li>Type: <code>int</code></li> <li>Default: <code>5</code></li> <li>Effect:</li> <li>Higher values = fewer, more general clusters</li> <li>Lower values = more, fine-grained clusters</li> <li>Recommendation:</li> <li>Start with 5 for most analyses</li> <li>Use 3-5 for side-by-side comparisons (smaller datasets)</li> <li>Increase to 10+ for very large datasets</li> </ul>"},{"location":"user-guide/parameters/#embedding_model","title":"<code>embedding_model</code>","text":"<ul> <li>Purpose: Model used for embedding properties during clustering</li> <li>Type: <code>str</code></li> <li>Default: <code>\"text-embedding-3-small\"</code></li> <li>Options:</li> <li><code>\"text-embedding-3-small\"</code>: Fast, cost-effective</li> <li><code>\"text-embedding-3-large\"</code>: Higher quality embeddings</li> <li>Any OpenAI embedding model</li> </ul>"},{"location":"user-guide/parameters/#summary_model","title":"<code>summary_model</code>","text":"<ul> <li>Purpose: LLM used for generating cluster summaries</li> <li>Type: <code>str</code></li> <li>Default: <code>\"gpt-4.1\"</code></li> <li>Recommendation: Use a strong model for best summary quality</li> </ul>"},{"location":"user-guide/parameters/#cluster_assignment_model","title":"<code>cluster_assignment_model</code>","text":"<ul> <li>Purpose: LLM used to match outliers to clusters</li> <li>Type: <code>str</code></li> <li>Default: <code>\"gpt-4.1-mini\"</code></li> <li>Note: This makes many calls, so using a cheaper model is recommended</li> <li>Recommendation: <code>\"gpt-4o-mini\"</code> or <code>\"gpt-3.5-turbo\"</code> for cost efficiency</li> </ul>"},{"location":"user-guide/parameters/#side-by-side-specific-parameters","title":"Side-by-Side Specific Parameters","text":"<p>For side-by-side comparison using tidy format (auto-pairing):</p>"},{"location":"user-guide/parameters/#model_a","title":"<code>model_a</code>","text":"<ul> <li>Purpose: Name of first model to compare</li> <li>Type: <code>str</code></li> <li>Required: Only when using <code>method=\"side_by_side\"</code> with tidy format</li> <li>Example: <code>model_a=\"gpt-4o\"</code></li> </ul>"},{"location":"user-guide/parameters/#model_b","title":"<code>model_b</code>","text":"<ul> <li>Purpose: Name of second model to compare</li> <li>Type: <code>str</code></li> <li>Required: Only when using <code>method=\"side_by_side\"</code> with tidy format</li> <li>Example: <code>model_b=\"claude-sonnet-35\"</code></li> </ul>"},{"location":"user-guide/parameters/#output-parameters","title":"Output Parameters","text":""},{"location":"user-guide/parameters/#output_dir","title":"<code>output_dir</code>","text":"<ul> <li>Purpose: Directory to save results</li> <li>Type: <code>str</code> (optional)</li> <li>Saves:</li> <li><code>clustered_results.parquet</code>: Main dataframe with clusters</li> <li><code>full_dataset.json</code>: Complete PropertyDataset (JSON)</li> <li><code>full_dataset.parquet</code>: Complete PropertyDataset (Parquet)</li> <li><code>model_stats.json</code>: Model statistics</li> <li><code>summary.txt</code>: Human-readable summary</li> <li>Default: None (results not saved to disk)</li> </ul>"},{"location":"user-guide/parameters/#verbose","title":"<code>verbose</code>","text":"<ul> <li>Purpose: Whether to print progress messages</li> <li>Type: <code>bool</code></li> <li>Default: <code>True</code></li> </ul>"},{"location":"user-guide/parameters/#use_wandb","title":"<code>use_wandb</code>","text":"<ul> <li>Purpose: Whether to log to Weights &amp; Biases</li> <li>Type: <code>bool</code></li> <li>Default: <code>True</code></li> <li>Disable: Set to <code>False</code> or set environment variable <code>WANDB_DISABLED=true</code></li> </ul>"},{"location":"user-guide/parameters/#wandb_project","title":"<code>wandb_project</code>","text":"<ul> <li>Purpose: W&amp;B project name for logging</li> <li>Type: <code>str</code> (optional)</li> <li>Default: <code>\"stringsight\"</code></li> </ul>"},{"location":"user-guide/parameters/#performance-parameters","title":"Performance Parameters","text":""},{"location":"user-guide/parameters/#max_workers","title":"<code>max_workers</code>","text":"<ul> <li>Purpose: Number of parallel workers for API calls</li> <li>Type: <code>int</code></li> <li>Default: <code>10</code></li> <li>Recommendation: Adjust based on your API rate limits</li> </ul>"},{"location":"user-guide/parameters/#extraction_cache_dir","title":"<code>extraction_cache_dir</code>","text":"<ul> <li>Purpose: Directory to cache extraction results to avoid re-running expensive API calls</li> <li>Type: <code>str</code> (optional)</li> <li>Default: None (no caching)</li> <li>Recommendation: Enable for iterative development</li> </ul>"},{"location":"user-guide/parameters/#label-specific-parameters","title":"Label-Specific Parameters","text":"<p>For the <code>label()</code> function:</p>"},{"location":"user-guide/parameters/#taxonomy","title":"<code>taxonomy</code>","text":"<ul> <li>Purpose: Predefined behavioral categories to detect</li> <li>Type: <code>dict[str, str]</code></li> <li>Required: Yes for <code>label()</code></li> <li>Format: <code>{\"behavior_name\": \"description of what to detect\"}</code></li> <li>Example: <pre><code>taxonomy = {\n    \"tricked by the user\": \"Does the model behave unsafely due to user manipulation?\",\n    \"reward hacking\": \"Does the model game the evaluation system?\",\n    \"refusal\": \"Does the model refuse to follow the user's request due to policy constraints?\",\n    \"tool calling\": \"Does the model call tools?\"\n}\n</code></pre></li> </ul>"},{"location":"user-guide/parameters/#cost-estimation","title":"Cost Estimation","text":"<p>To estimate the number of LLM calls for a given analysis:</p> <p>For <code>sample_size=100</code> with <code>min_cluster_size=3</code>: - 100 calls for property extraction (usually get 3-5 properties per trace with gpt-4.1) - ~300-500 embedding calls for each property - ~(300-500) / min_cluster_size LLM calls to generate cluster summaries - ~50-100 outlier matching calls (hence why we recommend using a smaller model)</p> <p>Note: The larger your <code>min_cluster_size</code>, the more outliers you will likely have.</p>"},{"location":"user-guide/parameters/#best-practices","title":"Best Practices","text":""},{"location":"user-guide/parameters/#starting-out","title":"Starting Out","text":"<ol> <li>Start with <code>sample_size=50-100</code> for initial exploration</li> <li>Use cheaper models first: <code>model_name=\"gpt-4o-mini\"</code>, <code>cluster_assignment_model=\"gpt-3.5-turbo\"</code></li> <li>Iterate on <code>min_cluster_size</code> to find the right granularity</li> </ol>"},{"location":"user-guide/parameters/#data-preparation","title":"Data Preparation","text":"<ol> <li>Include <code>question_id</code> for side-by-side analysis</li> <li>Clean your data: remove duplicates, handle missing values</li> <li>Format responses: ensure model responses are readable</li> <li>Include <code>score_columns</code> if you have metrics for richer analysis</li> </ol>"},{"location":"user-guide/parameters/#optimization","title":"Optimization","text":"<ol> <li>Enable caching with <code>extraction_cache_dir</code> to avoid re-running expensive API calls</li> <li>Adjust <code>max_workers</code> based on your API rate limits</li> <li>For single_model with multiple models per prompt, <code>sample_size</code> samples prompts not rows</li> </ol>"},{"location":"user-guide/parameters/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>Too many clusters: Increase <code>min_cluster_size</code></li> <li>Too few clusters: Decrease <code>min_cluster_size</code> or increase <code>sample_size</code></li> <li>API errors: Check rate limits, reduce <code>max_workers</code></li> <li>Poor cluster quality: Try a different <code>embedding_model</code> or increase <code>sample_size</code></li> </ul>"},{"location":"user-guide/parameters/#example-configurations","title":"Example Configurations","text":""},{"location":"user-guide/parameters/#cost-effective-analysis","title":"Cost-Effective Analysis","text":"<pre><code>clustered_df, model_stats = explain(\n    df,\n    sample_size=50,\n    model_name=\"gpt-4o-mini\",\n    embedding_model=\"text-embedding-3-small\",\n    cluster_assignment_model=\"gpt-3.5-turbo\",\n    min_cluster_size=5,\n    use_wandb=False\n)\n</code></pre>"},{"location":"user-guide/parameters/#high-quality-analysis","title":"High-Quality Analysis","text":"<pre><code>clustered_df, model_stats = explain(\n    df,\n    model_name=\"gpt-4.1\",\n    embedding_model=\"text-embedding-3-large\",\n    summary_model=\"gpt-4.1\",\n    min_cluster_size=10,\n    use_wandb=True,\n    wandb_project=\"production-analysis\"\n)\n</code></pre>"},{"location":"user-guide/parameters/#task-specific-analysis","title":"Task-Specific Analysis","text":"<pre><code>task_description = \"\"\"\nEvaluate customer support responses for:\n- Empathy and emotional intelligence\n- Clarity of communication\n- Adherence to company policies\n- Problem-solving effectiveness\n\"\"\"\n\nclustered_df, model_stats = explain(\n    df,\n    method=\"single_model\",\n    task_description=task_description,\n    model_name=\"gpt-4.1\",\n    min_cluster_size=5,\n    output_dir=\"results/customer_support\"\n)\n</code></pre>"},{"location":"user-guide/visualization/","title":"Visualization","text":"<p>StringSight provides various ways to visualize and explore analysis results.</p>"},{"location":"user-guide/visualization/#visualization-best-practices","title":"Visualization Best Practices","text":""},{"location":"user-guide/visualization/#plot-interpretation","title":"Plot Interpretation","text":""},{"location":"user-guide/visualization/#frequency-plots","title":"Frequency Plots","text":"<p>Shows what proportion of each model's responses fall into each behavioral cluster.</p> <ul> <li>Y-axis: Cluster names</li> <li>X-axis: Proportion (0-1) or percentage</li> <li>Bars: One per model</li> <li>Interpretation:</li> <li>Longer bars = model exhibits this behavior more frequently</li> <li>Compare bar lengths to see behavioral differences</li> </ul>"},{"location":"user-guide/visualization/#quality-plots","title":"Quality Plots","text":"<p>Shows how well models perform within specific behavioral clusters.</p> <ul> <li>Y-axis: Cluster names</li> <li>X-axis: Quality score (varies by metric)</li> <li>Bars: One per model</li> <li>Interpretation:</li> <li>Higher values = better performance in this behavior cluster</li> <li>Compare across models to see strengths/weaknesses</li> </ul>"},{"location":"user-guide/visualization/#delta-plots","title":"Delta Plots","text":"<p>Shows relative differences from baseline/median performance.</p> <ul> <li>Y-axis: Cluster names</li> <li>X-axis: Delta value (can be positive/negative)</li> <li>Zero line: Baseline performance</li> <li>Interpretation:</li> <li>Positive = over-represented / better than average</li> <li>Negative = under-represented / worse than average</li> </ul>"},{"location":"user-guide/visualization/#filtering-strategies","title":"Filtering Strategies","text":"<p>For Initial Exploration: 1. Start with all clusters visible 2. Sort by Frequency (Descending) to see most common behaviors 3. Review Top 10 clusters</p> <p>For Finding Differences: 1. Enable Significance filter 2. Sort by Relative Frequency \u0394 3. Look for clusters where models diverge most</p> <p>For Quality Analysis: 1. Switch to Quality plot type 2. Select quality metric (e.g., \"accuracy\", \"helpfulness\") 3. Sort by Quality Delta \u0394 4. Identify where each model excels</p>"},{"location":"user-guide/visualization/#performance-optimization","title":"Performance Optimization","text":"<p>For Large Datasets: - Use sampling during initial exploration - Filter by significant clusters only - Limit to Top N clusters (e.g., 10-20) - Disable confidence intervals for faster rendering</p> <p>For Sharing: - Export filtered data to CSV - Save plots as PNG/PDF</p>"},{"location":"user-guide/visualization/#customizing-visualizations","title":"Customizing Visualizations","text":"<pre><code>import plotly.express as px\n\n# Custom plot function\ndef create_custom_plot(data, metric=\"proportion\"):\n    fig = px.bar(\n        data,\n        x=metric,\n        y=\"cluster_label\",\n        color=\"model\",\n        orientation=\"h\",\n        title=\"Custom Behavioral Analysis\"\n    )\n    return fig\n</code></pre>"},{"location":"user-guide/visualization/#react-customization","title":"React Customization","text":"<p>Modify chart configurations in <code>frontend/src/components/</code>:</p> <pre><code>// In Plot.tsx\nconst chartConfig = {\n  type: 'bar',\n  layout: {\n    title: 'Custom Title',\n    xaxis: { title: 'Custom X Label' },\n    yaxis: { title: 'Custom Y Label' }\n  }\n};\n</code></pre>"},{"location":"user-guide/visualization/#exporting-results","title":"Exporting Results","text":""},{"location":"user-guide/visualization/#from-react","title":"From React","text":"<ol> <li>Click \"Export\" button in data table</li> <li>Use browser's print function for plots</li> <li>Copy data from tables directly</li> </ol>"},{"location":"user-guide/visualization/#programmatically","title":"Programmatically","text":"<pre><code>import pandas as pd\nimport json\n\n# Load results\ndf = pd.read_parquet(\"results/clustered_results.parquet\")\n\n# Export filtered data\nfiltered = df[df['property_description_cluster_label'] == 'Reasoning Transparency']\nfiltered.to_csv(\"reasoning_examples.csv\", index=False)\n\n# Export metrics (prefer JSONL DataFrame exports when available)\ntry:\n    metrics_df = pd.read_json(\"results/model_cluster_scores_df.jsonl\", lines=True)\n    gpt4_metrics = metrics_df[metrics_df[\"model\"] == \"gpt-4\"]\n    top_clusters = (\n        gpt4_metrics.sort_values(\"proportion\", ascending=False)\n                   .head(10)[[\"cluster\", \"proportion\"]]\n    )\nexcept FileNotFoundError:\n    with open(\"results/model_cluster_scores.json\") as f:\n        metrics = json.load(f)\n    gpt4_metrics = metrics.get(\"gpt-4\", {})\n    top_clusters = sorted(\n        gpt4_metrics.items(),\n        key=lambda x: x[1].get(\"proportion\", 0),\n        reverse=True\n    )[:10]\n</code></pre>"},{"location":"user-guide/visualization/#troubleshooting","title":"Troubleshooting","text":""},{"location":"user-guide/visualization/#frontend-issues","title":"Frontend Issues","text":"<p>Port already in use: <pre><code># Kill process on port 8000 or 5173\nlsof -ti:8000 | xargs kill -9\nlsof -ti:5173 | xargs kill -9\n\n# Or use different ports\npython -m uvicorn stringsight.api:app --port 8001\n# Update frontend/src/config.ts accordingly\n</code></pre></p> <p>CORS errors: <pre><code># In stringsight/api.py, add allowed origins:\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"http://localhost:5173\", \"http://127.0.0.1:5173\"],\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n</code></pre></p>"},{"location":"user-guide/visualization/#visualization-issues","title":"Visualization Issues","text":"<p>Plots not rendering: <pre><code># Reinstall plotly\npip install --upgrade plotly\n</code></pre></p> <p>Data not loading: <pre><code># Check file paths are absolute\nimport os\ndata_path = os.path.abspath(\"results/clustered_results.parquet\")\n</code></pre></p> <p>Slow loading: - Reduce dataset size - Use parquet instead of JSON - Filter data before loading</p>"},{"location":"user-guide/visualization/#next-steps","title":"Next Steps","text":"<ul> <li>Configuration Options - Customize analysis parameters</li> <li>Data Formats - Understand input/output formats</li> <li>Advanced Usage - Build custom visualizations</li> </ul>"}]}