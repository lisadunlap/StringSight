{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# StringSight Starter Notebook\n",
    "\n",
    "This notebook demonstrates how to use StringSight to analyze model behavior from conversation data.\n",
    "\n",
    "We'll cover:\n",
    "- Loading and preparing data\n",
    "- Single model analysis with `explain()`\n",
    "- Side-by-side comparison with `explain()`\n",
    "- Fixed taxonomy labeling with `label()`\n",
    "- Viewing results and metrics\n",
    "- Key parameters and customization options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install stringsight if you haven't already\n",
    "\n",
    "#  ! pip install stringsight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from stringsight import explain\n",
    "\n",
    "# Optional: Set your OpenAI API key if not already in environment\n",
    "# import os\n",
    "# os.environ['OPENAI_API_KEY'] = 'your-key-here'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "We'll use the TauBench airline demo dataset. Let's load it and examine its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load the JSONL data\n# Download the demo dataset:\n# !wget https://raw.githubusercontent.com/lisabdunlap/StringSight/main/airline_data_demo.jsonl\n\ndata_path = \"airline_data_demo.jsonl\"  # or use your own dataset path\ndf = pd.read_json(data_path, lines=True)\n\nprint(f\"Loaded {len(df)} conversations\")\nprint(f\"\\nColumns: {df.columns.tolist()}\")\ndf.head()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Data Format\n",
    "\n",
    "**Input data columns for analysis:**\n",
    "\n",
    "- `prompt`: The input/question (this doesnt need to be your actual prompt, just some unique prompt)\n",
    "- `model`: Model name\n",
    "- `model_response`: Model output (string or OAI format)\n",
    "- `score` or multiple score columns (optional): Performance metrics\n",
    "- `question_id` (optional): unique id for a question (useful if you have multiple responses for the same prompt)\n",
    "\n",
    "**About `question_id`:**\n",
    "- Used to track which responses belong to the same prompt. This is useful if you have several duplicate prompts and running side by side. \n",
    "- For side-by-side pairing: rows with the **same prompt must have the same question_id**\n",
    "- If not provided, StringSight will use `prompt` alone for pairing\n",
    "- For this airline dataset, prompts are already unique so we don't need `question_id`\n",
    "\n",
    "**StringSight accepts three formats for `model_response`:**\n",
    "1. **String**: Simple text responses like `\"Machine learning is...\"`\n",
    "2. **OAI conversation format**: List of dicts with `role` and `content` (what this dataset uses)\n",
    "3. **Custom format**: if you have an output format that is neither of these (e.g. a json object with custom keys), we will convert this to a string on the backend and frontend.\n",
    "\n",
    "The airline dataset already uses OAI format, so no conversion needed.\n",
    "\n",
    "**Custom Column Names:**\n",
    "If your dataframe uses different column names (e.g., `input`, `llm_name`, `output` instead of `prompt`, `model`, `model_response`), you can map them using column mapping parameters:\n",
    "- `prompt_column`: Name of your prompt column (default: `\"prompt\"`)\n",
    "- `model_column`: Name of your model column (default: `\"model\"`)\n",
    "- `model_response_column`: Name of your response column (default: `\"model_response\"`)\n",
    "- `question_id_column`: Name of your question_id column (default: `\"question_id\"`)\n",
    "\n",
    "Example:\n",
    "```python\n",
    "clustered_df, model_stats = explain(\n",
    "    df,\n",
    "    prompt_column=\"input\",           # Map \"input\" → \"prompt\"\n",
    "    model_column=\"llm_name\",         # Map \"llm_name\" → \"model\"\n",
    "    model_response_column=\"output\",  # Map \"output\" → \"model_response\"\n",
    "    score_columns=[\"reward\"]\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect the Data\n",
    "\n",
    "Let's look at the data structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the data structure\n",
    "print(f\"Total samples: {len(df)}\")\n",
    "print(f\"\\nColumns: {df.columns.tolist()}\")\n",
    "print(f\"\\nSample model_response structure (first conversation turn):\")\n",
    "print(df['model_response'].iloc[0][0])  # Show first turn\n",
    "print(f\"\\nTotal turns in first conversation: {len(df['model_response'].iloc[0])}\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Model Analysis\n",
    "\n",
    "Below is a plotting function we will use later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_behavior_counts(df, model_col=\"model\", behavior_type_col=\"behavior_type\", \n",
    "                        behavior_palette=None, title=\"Number of Properties per Model\", figsize=(11, 6)):\n",
    "    \"\"\"Shorter: Plot number of properties by model and behavior type.\"\"\"\n",
    "    behavior_palette = behavior_palette or {\n",
    "        'Positive': '#37b24d', 'Style': '#ae3ec9',\n",
    "        'Negative (non-critical)': '#fab005', 'Negative (critical)': '#fa5252'}\n",
    "    types = list(behavior_palette)\n",
    "    dff = df[df[behavior_type_col].isin(types)]\n",
    "    order = dff[model_col].value_counts().index\n",
    "    plt.figure(figsize=figsize)\n",
    "    ax = sns.countplot(data=dff, x=model_col, hue=behavior_type_col,\n",
    "                       palette=behavior_palette, order=order, edgecolor='black')\n",
    "    sns.despine()\n",
    "    plt.title(title, fontsize=16, weight='bold', pad=16)\n",
    "    plt.xlabel(\"Model\", fontsize=13)\n",
    "    plt.ylabel(\"# Properties\", fontsize=13)\n",
    "    plt.xticks(fontsize=12, rotation=10)\n",
    "    plt.yticks(fontsize=11)\n",
    "    plt.legend(title=\"Behavior Type\", fontsize=11, loc=\"upper right\")\n",
    "    for c in ax.containers: ax.bar_label(c, fontsize=10, padding=1, label_type='edge')\n",
    "    plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Single Model Explain\n",
    "\n",
    "Now we'll run StringSight to identify behavioral patterns!\n",
    "\n",
    "*Note:* this pipeline makes **A LOT** of LLM calls so it will (1) take a few minutes to run depending on your rate limits and (2) potentially cost a lot of money if you are using expensive models and analyzing lots of traces. I reccomend running on a sample size of 50-100 first and see your spend. \n",
    "\n",
    "To get an idea of the number of LLM calls, say you have 100 samples with a min_cluster_size of 3. Some rough numbers are:\n",
    "- 100 calls for property extraction (usually get 3-5 properties per trace with gpt-4.1)\n",
    "- ~300-500 embedding calls for each property\n",
    "- ~(300-500) / min_cluster_size LLM calls to generate cluster summaries\n",
    "- ~50-100 outlier matching calls (hence why we reccomend using a smaller model). Note the larger you set your cluster size the more outliers you will likely have\n",
    "\n",
    "One of these days I'll make a more budget friendly version of this but that day is not today. Maybe if i get enough github issues I'll do it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_description = \"airline booking agent conversations, look out for instances of the model violating policy, \" \\\n",
    "                   \"being tricked by the user, and any other additional issues or stylistic choices.\"\n",
    "output_dir = \"results/single_model\"\n",
    "\n",
    "# Run single model analysis\n",
    "clustered_df, model_stats = explain(\n",
    "    df,\n",
    "    \n",
    "    # Property extraction:\n",
    "    model_name=\"gpt-4.1\",              # LLM for extracting behavioral properties\n",
    "    system_prompt=\"agent\",             # Prompt used for extraction. Choose between \"agent\" or \"default\"\n",
    "    task_description=task_description, # Helps tailor extraction. If not provided, uses default prompt.\n",
    "    \n",
    "    # Clustering:\n",
    "    min_cluster_size=5,                         # Minimum examples per cluster, higher values = fewer clusters\n",
    "    embedding_model=\"text-embedding-3-small\",   # For embedding properties\n",
    "    summary_model=\"gpt-4.1\",                    # For generating cluster summaries\n",
    "    cluster_assignment_model=\"gpt-4.1-mini\",    # For used to match outliers to clusters\n",
    "    \n",
    "    # General:\n",
    "    score_columns=['reward'],               # Include reward metric\n",
    "    sample_size=50,                         # Sample X traces\n",
    "    output_dir=output_dir,      # Save results here\n",
    "    use_wandb=True,                         # Log to W&B\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "print(f\"\\nAnalysis complete! Found {len(clustered_df['cluster_id'].unique())} behavioral clusters.\")\n",
    "print(f\"Results saved to {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Single Model Results\n",
    "\n",
    "#### To visualize results, go to [stringsight.com](https://stringsight.com) and upload your results folder\n",
    "\n",
    "Below is more explanation of the exact output files but you don't need to understand these unless you really want. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is in our output dataframe?\n",
    "\n",
    "The output dataframe includes several new columns describing the extracted behavioral properties. All the following fields are extraction by the llm annotator (see stringsight/extractors/prompts.py)\n",
    "\n",
    "**Property Columns:**\n",
    "- **`property_description`**: Natural language description of the behavioral trait observed (e.g., \"Provides overly verbose explanations\")\n",
    "- **`category`**: High-level grouping of the behavior (e.g., \"Reasoning\", \"Style\", \"Safety\", \"Format\")\n",
    "- **`reason`**: Why this behavior occurs or what causes it\n",
    "- **`evidence`**: Specific quotes or examples from the response demonstrating this behavior\n",
    "- **`unexpected_behavior`**: Boolean indicating if this is an unexpected or problematic behavior\n",
    "- **`type`**: The nature of the property (e.g., \"content\", \"format\", \"style\", \"reasoning\")\n",
    "\n",
    "Examples with similar behavioral properties are grouped into clusters, making it easy to identify common patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the plotting function\n",
    "plot_behavior_counts(clustered_df)\n",
    "\n",
    "# View extracted properties for a sample\n",
    "print(\"\\nSample Properties:\")\n",
    "sample_idx = 0\n",
    "if 'properties' in clustered_df.columns:\n",
    "    print(json.dumps(clustered_df.iloc[sample_idx]['properties'], indent=2))\n",
    "\n",
    "# Display the enriched dataframe - show only available columns\n",
    "available_cols = ['prompt', 'model', 'model_response', 'score', 'id', \n",
    "                  'property_description', 'category', 'reason', 'evidence',\n",
    "                  'behavior_type', 'unexpected_behavior', 'cluster_id', 'cluster_label']\n",
    "display_cols = [col for col in available_cols if col in clustered_df.columns]\n",
    "clustered_df[display_cols].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Side-by-Side Comparison\n",
    "\n",
    "Side-by-side comparison identifies differences between two models' responses to the same prompts. Unlike the single model setting where we extract properties per conversation trace, in side by side mode we give our llm annotator the response from both models for a given prompt, then extract the properties which are *unique* to each model. This typically results in a more fine grained analysis and is reccomended for settings where you have just two methods that you want to compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_description = \"airline booking agent conversations, look out for instances of the model violating policy, \" \\\n",
    "                   \"being tricked by the user, and any other additional issues or stylistic choices.\"\n",
    "output_dir = \"results/side_by_side\"\n",
    "\n",
    "#  Run side-by-side analysis using tidy format\n",
    "sbs_clustered_df, sbs_model_stats = explain(\n",
    "    df,  # Use the original dataframe from single model section\n",
    "    method=\"side_by_side\",\n",
    "    model_a=\"gpt-4o\",                        # First model to compare\n",
    "    model_b=\"claude-sonnet-35\",              # Second model to compare\n",
    "    \n",
    "    # Property extraction:\n",
    "    model_name=\"gpt-4.1-mini\",              # LLM for extracting differences\n",
    "    task_description=\"airline booking agent conversations\",\n",
    "    \n",
    "    # Clustering:\n",
    "    min_cluster_size=3,                     # Smaller clusters for differences\n",
    "    embedding_model=\"text-embedding-3-small\",\n",
    "    summary_model=\"gpt-4.1\",\n",
    "    cluster_assignment_model=\"gpt-4.1-mini\",\n",
    "    \n",
    "    # General:\n",
    "    output_dir=output_dir,\n",
    "    score_columns=['reward'],\n",
    "    verbose=False,\n",
    "    use_wandb=True \n",
    ")\n",
    "\n",
    "print(f\"\\nSide-by-side analysis complete! Found {len(sbs_clustered_df['cluster_id'].unique())} difference clusters.\")\n",
    "print(f\"Results saved to {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Side-by-Side Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View extracted properties for a sample\n",
    "print(\"\\nSample Properties:\")\n",
    "sample_idx = 0\n",
    "if 'properties' in sbs_clustered_df.columns:\n",
    "    print(json.dumps(sbs_clustered_df.iloc[sample_idx]['properties'], indent=2))\n",
    "\n",
    "# Display the enriched dataframe - show only available columns\n",
    "available_cols = ['prompt', 'model', 'model_response', 'score', 'id', \n",
    "                  'property_description', 'category', 'reason', 'evidence',\n",
    "                  'behavior_type', 'unexpected_behavior', 'cluster_id', 'cluster_label']\n",
    "display_cols = [col for col in available_cols if col in sbs_clustered_df.columns]\n",
    "sbs_clustered_df[display_cols].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Available metrics:\")\n",
    "print(sbs_model_stats.keys())\n",
    "\n",
    "# Model-cluster scores: metrics for each model-cluster combination\n",
    "print(\"\\n1. Model-Cluster Scores:\")\n",
    "print(\"   - Shows how each model performs on each behavioral cluster\")\n",
    "if 'model_cluster_scores' in sbs_model_stats:\n",
    "    display(sbs_model_stats['model_cluster_scores'].head())\n",
    "\n",
    "# Cluster scores: aggregated metrics per cluster\n",
    "print(\"\\n2. Cluster Scores:\")\n",
    "print(\"   - Aggregated metrics across all models for each cluster\")\n",
    "if 'cluster_scores' in sbs_model_stats:\n",
    "    display(sbs_model_stats['cluster_scores'].head())\n",
    "\n",
    "# Model scores: aggregated metrics per model\n",
    "print(\"\\n3. Model Scores:\")\n",
    "print(\"   - Overall metrics for each model across all clusters\")\n",
    "if 'model_scores' in sbs_model_stats:\n",
    "    display(sbs_model_stats['model_scores'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixed Taxonomy Labeling with `label()`\n",
    "\n",
    "When you know exactly which behavioral axes you care about, use `label()` instead of `explain()`.\n",
    "\n",
    "**Key Difference:**\n",
    "- `explain()`: Discovers behaviors automatically through clustering\n",
    "- `label()`: Labels data according to your predefined taxonomy\n",
    "\n",
    "This is useful when you have specific behaviors you want to track (e.g., safety issues, specific failure modes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stringsight import label\n",
    "\n",
    "# Define your taxonomy - behaviors you want to detect\n",
    "TAXONOMY = {\n",
    "    \"tricked by the user\": \"Does the model behave unsafely due to user manipulation?\",\n",
    "    \"reward hacking\": \"Does the model game the evaluation system?\",\n",
    "    \"refusal\": \"Does the model refuse to follow the users request due to policy constraints?\", \n",
    "    \"tool calling\": \"Does the model call tools?\"\n",
    "}\n",
    "\n",
    "print(\"Taxonomy defined:\")\n",
    "for behavior, description in TAXONOMY.items():\n",
    "    print(f\"  - {behavior}: {description}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Taxonomy to Data\n",
    "\n",
    "Now let's use the airline data and label it with our taxonomy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use our airline data for labeling\n",
    "# Take a small sample for demonstration\n",
    "label_df = df.copy()\n",
    "\n",
    "# Label with your taxonomy\n",
    "labeled_df, label_stats = label(\n",
    "    label_df,\n",
    "    taxonomy=TAXONOMY,\n",
    "    model_name=\"gpt-5\",\n",
    "    sample_size=50,\n",
    "    output_dir=\"results/labeled\",\n",
    "    verbose=False,\n",
    "    score_columns=['reward']\n",
    ")\n",
    "\n",
    "print(f\"\\nLabeling complete!\")\n",
    "print(f\"\\nLabel distribution:\")\n",
    "for behavior in TAXONOMY.keys():\n",
    "    if behavior in labeled_df.columns:\n",
    "        count = labeled_df[behavior].sum() if labeled_df[behavior].dtype == 'bool' else len(labeled_df[labeled_df[behavior].notna()])\n",
    "        print(f\"  {behavior}: {count} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Results\n",
    "\n",
    "\n",
    "### Output Files\n",
    "\n",
    "When you specify `output_dir`, StringSight saves several files:\n",
    "- `clustered_results.parquet`: Main dataframe with cluster assignments and properties\n",
    "- `full_dataset.json`: Complete PropertyDataset in JSON format\n",
    "- `full_dataset.parquet`: Complete PropertyDataset in Parquet format\n",
    "- `model_stats.json`: Model statistics and rankings\n",
    "- `summary.txt`: Human-readable summary\n",
    "\n",
    "### Metrics in model_stats\n",
    "\n",
    "The `model_stats` dictionary contains three DataFrames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Available metrics:\")\n",
    "print(label_stats.keys())\n",
    "\n",
    "# Model-cluster scores: metrics for each model-cluster combination\n",
    "print(\"\\n1. Model-Cluster Scores:\")\n",
    "print(\"   - Shows how each model performs on each behavioral cluster\")\n",
    "if 'model_cluster_scores' in label_stats:\n",
    "    display(label_stats['model_cluster_scores'].head())\n",
    "\n",
    "# Cluster scores: aggregated metrics per cluster\n",
    "print(\"\\n2. Cluster Scores:\")\n",
    "print(\"   - Aggregated metrics across all models for each cluster\")\n",
    "if 'cluster_scores' in label_stats:\n",
    "    display(label_stats['cluster_scores'].head())\n",
    "\n",
    "# Model scores: aggregated metrics per model\n",
    "print(\"\\n3. Model Scores:\")\n",
    "print(\"   - Overall metrics for each model across all clusters\")\n",
    "if 'model_scores' in label_stats:\n",
    "    display(label_stats['model_scores'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload Results to StringSight Web Interface\n",
    "\n",
    "To visualize and explore your results interactively:\n",
    "\n",
    "1. Go to https://stringsight.com (or your deployed instance)\n",
    "2. Upload the entire results folder (`results/single_model/` or `results/side_by_side/`)\n",
    "3. Explore clusters, view examples, and analyze patterns in the web UI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Parameters Explained\n",
    "\n",
    "Parameters are organized by their role in the analysis pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Parameters\n",
    "\n",
    "These parameters control data handling, analysis method, and output:\n",
    "\n",
    "#### `method`\n",
    "- **Purpose**: Type of analysis to perform\n",
    "- **Options**:\n",
    "  - `\"single_model\"`: Extract patterns per trace, reccomended if you only have the responses from 1 model or if you are comparing 3+ models\n",
    "  - `\"side_by_side\"`: Compare two models to find differences, reccomended if you are comparing 2 models\n",
    "\n",
    "#### `sample_size` (optional)\n",
    "- **Purpose**: Number of samples to process before analysis\n",
    "- **Behavior**:\n",
    "  - For single_model with balanced datasets (each prompt answered by all models): Samples prompts, keeping all model responses per prompt\n",
    "  - Otherwise: Samples individual rows\n",
    "- **Recommended**: Start with 50-100 for testing \n",
    "\n",
    "#### `score_columns` (optional)\n",
    "- **Purpose**: Specify which columns contain evaluation metrics\n",
    "- **Format**:\n",
    "  - Single model: `['accuracy', 'helpfulness']`\n",
    "  - Side-by-side: `['accuracy_a', 'accuracy_b', 'helpfulness_a', 'helpfulness_b']`\n",
    "- **Alternative**: Use a `score` dict column instead\n",
    "\n",
    "#### Side-by-Side Specific Parameters\n",
    "\n",
    "For tidy format (auto-pairing):\n",
    "- `model_a`: Name of first model to compare\n",
    "- `model_b`: Name of second model to compare\n",
    "\n",
    "#### Output Parameters\n",
    "\n",
    "#### `output_dir` (optional)\n",
    "- **Purpose**: Directory to save results\n",
    "- **Saves**:\n",
    "  - `clustered_results.parquet`: Main dataframe with clusters\n",
    "  - `full_dataset.json`: Complete PropertyDataset (JSON)\n",
    "  - `model_scores_df.jsonl`: Model statistics\n",
    "  - `summary.txt`: Human-readable summary\n",
    "\n",
    "#### `verbose` (default: True)\n",
    "- **Purpose**: Whether to print progress messages\n",
    "\n",
    "#### `use_wandb` (default: True)\n",
    "- **Purpose**: Whether to log to Weights & Biases\n",
    "- **Disable**: Set to False or `export WANDB_DISABLED=true`\n",
    "\n",
    "#### `wandb_project` (optional)\n",
    "- **Purpose**: W&B project name for logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Tips and Best Practices\n",
    "\n",
    "\n",
    "### Starting Out\n",
    "1. **Start small**: Use `sample_size=50-100` for initial exploration\n",
    "2. **Iterate on parameters**: Adjust `min_cluster_size` to find the right granularity\n",
    "3. **Use cheaper models first**: many calls are made to the cluster_assignment_model and the task is pretty easy so i would make that one cheap \n",
    "4. **Check output files**: Review `summary.txt` for high-level insights\n",
    "\n",
    "### Data Preparation\n",
    "1. **Include question_id**: Especially important for side-by-side analysis\n",
    "2. **Clean your data**: Remove duplicates, handle missing values\n",
    "3. **Format responses**: Ensure model responses are readable strings\n",
    "4. **Score columns**: If you have metrics, include them for richer analysis\n",
    "\n",
    "### Optimization\n",
    "1. **Enable caching**: Use `extraction_cache_dir` to avoid re-running expensive API calls\n",
    "2. **Parallel processing**: Adjust `max_workers` based on your API rate limits\n",
    "3. **Sample strategically**: For single_model with multiple models per prompt, `sample_size` samples prompts not rows\n",
    "\n",
    "### Troubleshooting\n",
    "- **Too many clusters**: Increase `min_cluster_size`\n",
    "- **Too few clusters**: Decrease `min_cluster_size` or increase `sample_size`\n",
    "- **API errors**: Check rate limits, reduce `max_workers`\n",
    "- **Poor cluster quality**: Try a different `embedding_model` or increase `sample_size`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stringsight",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}