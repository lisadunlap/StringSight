{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# StringSight Starter Notebook\n",
    "\n",
    "This notebook demonstrates how to use StringSight to analyze model behavior from conversation data.\n",
    "\n",
    "We'll cover:\n",
    "1. Loading and preparing data\n",
    "2. Single model analysis with `explain()`\n",
    "3. Side-by-side comparison with `explain()`\n",
    "4. Fixed taxonomy labeling with `label()`\n",
    "5. Viewing results and metrics\n",
    "6. Key parameters and customization options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install stringsight if you haven't already\n",
    "\n",
    "#  ! pip install stringsight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from stringsight import explain\n",
    "\n",
    "# Optional: Set your OpenAI API key if not already in environment\n",
    "# import os\n",
    "# os.environ['OPENAI_API_KEY'] = 'your-key-here'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data\n",
    "\n",
    "We'll use the TauBench airline demo dataset. Let's load it and examine its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the JSONL data\n",
    "data_path = \"data/taubench/airline_data_demo.jsonl\"\n",
    "df = pd.read_json(data_path, lines=True)\n",
    "\n",
    "print(f\"Loaded {len(df)} conversations\")\n",
    "print(f\"\\nColumns: {df.columns.tolist()}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Data Format\n",
    "\n",
    "**Input data columns for analysis:**\n",
    "\n",
    "- `prompt`: The input/question (this doesnt need to be your actual prompt, just some unique prompt)\n",
    "- `model`: Model name\n",
    "- `model_response`: Model output (string or OAI format)\n",
    "- `score` or multiple score columns (optional): Performance metrics\n",
    "- `question_id` (optional): unique id for a question (useful if you have multiple responses for the same prompt)\n",
    "\n",
    "**About `question_id`:**\n",
    "- Used to track which responses belong to the same prompt. This is useful if you have several duplicate prompts and running side by side. \n",
    "- For side-by-side pairing: rows with the **same prompt must have the same question_id**\n",
    "- If not provided, StringSight will use `prompt` alone for pairing\n",
    "- For this airline dataset, prompts are already unique so we don't need `question_id`\n",
    "\n",
    "**StringSight accepts three formats for `model_response`:**\n",
    "1. **String**: Simple text responses like `\"Machine learning is...\"`\n",
    "2. **OAI conversation format**: List of dicts with `role` and `content` (what this dataset uses)\n",
    "3. **Custom format**: if you have an output format that is neither of these (e.g. a json object with custom keys), we will convert this to a string on the backend and frontend.\n",
    "\n",
    "The airline dataset already uses OAI format, so no conversion needed.\n",
    "\n",
    "**Custom Column Names:**\n",
    "If your dataframe uses different column names (e.g., `input`, `llm_name`, `output` instead of `prompt`, `model`, `model_response`), you can map them using column mapping parameters:\n",
    "- `prompt_column`: Name of your prompt column (default: `\"prompt\"`)\n",
    "- `model_column`: Name of your model column (default: `\"model\"`)\n",
    "- `model_response_column`: Name of your response column (default: `\"model_response\"`)\n",
    "- `question_id_column`: Name of your question_id column (default: `\"question_id\"`)\n",
    "\n",
    "Example:\n",
    "```python\n",
    "clustered_df, model_stats = explain(\n",
    "    df,\n",
    "    prompt_column=\"input\",           # Map \"input\" → \"prompt\"\n",
    "    model_column=\"llm_name\",         # Map \"llm_name\" → \"model\"\n",
    "    model_response_column=\"output\",  # Map \"output\" → \"model_response\"\n",
    "    score_columns=[\"reward\"]\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect the Data\n",
    "\n",
    "Let's look at the data structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the data structure\n",
    "print(f\"Total samples: {len(df)}\")\n",
    "print(f\"\\nColumns: {df.columns.tolist()}\")\n",
    "print(f\"\\nSample model_response structure (first conversation turn):\")\n",
    "print(df['model_response'].iloc[0][0])  # Show first turn\n",
    "print(f\"\\nTotal turns in first conversation: {len(df['model_response'].iloc[0])}\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Single Model Explain\n",
    "\n",
    "Now we'll run StringSight to identify behavioral patterns!\n",
    "\n",
    "*Note:* this pipeline makes **A LOT** of LLM calls so it will (1) take a few minutes to run depending on your rate limits and (2) potentially cost a lot of money if you are using expensive models and analyzing lots of traces. I reccomend running on a sample size of 50-100 first and see your spend. \n",
    "\n",
    "To get an idea of the number of LLM calls, say you have 100 samples with a min_cluster_size of 3. Some rough numbers are:\n",
    "- 100 calls for property extraction (usually get 3-5 properties per trace with gpt-4.1)\n",
    "- ~300-500 embedding calls for each property\n",
    "- ~(300-500) / min_cluster_size LLM calls to generate cluster summaries\n",
    "- ~50-100 outlier matching calls (hence why we reccomend using a smaller model). Note the larger you set your cluster size the more outliers you will likely have\n",
    "\n",
    "One of these days I'll make a more budget friendly version of this but that day is not today. Maybe if i get enough github issues I'll do it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_description = \"airline booking agent conversations, look out for instances of the model violating policy, \" \\\n",
    "                   \"being tricked by the user, and any other additional issues or stylistic choices.\"\n",
    "output_dir = \"results/single_model\"\n",
    "\n",
    "# Run single model analysis\n",
    "clustered_df, model_stats = explain(\n",
    "    df,\n",
    "    \n",
    "    # Property extraction:\n",
    "    model_name=\"gpt-4.1\",              # LLM for extracting behavioral properties\n",
    "    system_prompt=\"agent\",             # Prompt used for extraction. Choose between \"agent\" or \"default\"\n",
    "    task_description=task_description, # Helps tailor extraction. If not provided, uses default prompt.\n",
    "    \n",
    "    # Clustering:\n",
    "    min_cluster_size=5,                         # Minimum examples per cluster, higher values = fewer clusters\n",
    "    embedding_model=\"text-embedding-3-small\",   # For embedding properties\n",
    "    summary_model=\"gpt-4.1\",                    # For generating cluster summaries\n",
    "    cluster_assignment_model=\"gpt-4.1-mini\",    # For used to match outliers to clusters\n",
    "    \n",
    "    # General:\n",
    "    score_columns=['reward'],               # Include reward metric\n",
    "    sample_size=50,                         # Sample X traces\n",
    "    output_dir=output_dir,      # Save results here\n",
    "    use_wandb=True,                         # Log to W&B\n",
    "    varbose=False\n",
    ")\n",
    "\n",
    "print(f\"\\nAnalysis complete! Found {len(clustered_df['cluster_id'].unique())} behavioral clusters.\")\n",
    "print(f\"Results saved to {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Single Model Results\n",
    "\n",
    "#### To visualize results, go to [stringsight.com](https://stringsight.com) and upload your results folder\n",
    "\n",
    "Below is more explanation of the exact output files but you don't need to understand these unless you really want. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is in our output dataframe?\n",
    "\n",
    "The output dataframe includes several new columns describing the extracted behavioral properties. All the following fields are extraction by the llm annotator (see stringsight/extractors/prompts.py)\n",
    "\n",
    "**Property Columns:**\n",
    "- **`property_description`**: Natural language description of the behavioral trait observed (e.g., \"Provides overly verbose explanations\")\n",
    "- **`category`**: High-level grouping of the behavior (e.g., \"Reasoning\", \"Style\", \"Safety\", \"Format\")\n",
    "- **`reason`**: Why this behavior occurs or what causes it\n",
    "- **`evidence`**: Specific quotes or examples from the response demonstrating this behavior\n",
    "- **`unexpected_behavior`**: Boolean indicating if this is an unexpected or problematic behavior\n",
    "- **`type`**: The nature of the property (e.g., \"content\", \"format\", \"style\", \"reasoning\")\n",
    "\n",
    "Examples with similar behavioral properties are grouped into clusters, making it easy to identify common patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_behavior_counts(df, model_col=\"model\", behavior_type_col=\"behavior_type\", \n",
    "                        behavior_palette=None, title=\"Number of Properties per Model\", figsize=(11, 6)):\n",
    "    \"\"\"Shorter: Plot number of properties by model and behavior type.\"\"\"\n",
    "    behavior_palette = behavior_palette or {\n",
    "        'Positive': '#37b24d', 'Style': '#ae3ec9',\n",
    "        'Negative (non-critical)': '#fab005', 'Negative (critical)': '#fa5252'}\n",
    "    types = list(behavior_palette)\n",
    "    dff = df[df[behavior_type_col].isin(types)]\n",
    "    order = dff[model_col].value_counts().index\n",
    "    plt.figure(figsize=figsize)\n",
    "    ax = sns.countplot(data=dff, x=model_col, hue=behavior_type_col,\n",
    "                       palette=behavior_palette, order=order, edgecolor='black')\n",
    "    sns.despine()\n",
    "    plt.title(title, fontsize=16, weight='bold', pad=16)\n",
    "    plt.xlabel(\"Model\", fontsize=13)\n",
    "    plt.ylabel(\"# Properties\", fontsize=13)\n",
    "    plt.xticks(fontsize=12, rotation=10)\n",
    "    plt.yticks(fontsize=11)\n",
    "    plt.legend(title=\"Behavior Type\", fontsize=11, loc=\"upper right\")\n",
    "    for c in ax.containers: ax.bar_label(c, fontsize=10, padding=1, label_type='edge')\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "# Call the plotting function\n",
    "plot_behavior_counts(clustered_df)\n",
    "\n",
    "# View extracted properties for a sample\n",
    "print(\"\\nSample Properties:\")\n",
    "sample_idx = 0\n",
    "if 'properties' in clustered_df.columns:\n",
    "    print(json.dumps(clustered_df.iloc[sample_idx]['properties'], indent=2))\n",
    "\n",
    "# Display the enriched dataframe - show only available columns\n",
    "available_cols = ['prompt', 'model', 'model_response', 'score', 'id', \n",
    "                  'property_description', 'category', 'reason', 'evidence',\n",
    "                  'behavior_type', 'unexpected_behavior', 'cluster_id', 'cluster_label']\n",
    "display_cols = [col for col in available_cols if col in clustered_df.columns]\n",
    "clustered_df[display_cols].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Side-by-Side Comparison\n",
    "\n",
    "Side-by-side comparison identifies differences between two models' responses to the same prompts. Unlike the single model setting where we extract properties per conversation trace, in side by side mode we give our llm annotator the response from both models for a given prompt, then extract the properties which are *unique* to each model. This typically results in a more fine grained analysis and is reccomended for settings where you have just two methods that you want to compare. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Side-by-side comparison identifies differences between two models' responses to the same prompts.\n",
    "\n",
    "The airline dataset already has both **gpt-4o** and **claude-sonnet-35** answering the same prompts. We can use the same `df` from the single model section and just specify which two models to compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_description = \"airline booking agent conversations, look out for instances of the model violating policy, \" \\\n",
    "                   \"being tricked by the user, and any other additional issues or stylistic choices.\"\n",
    "output_dir = \"results/side_by_side\"\n",
    "\n",
    "#  Run side-by-side analysis using tidy format\n",
    "sbs_clustered_df, sbs_model_stats = explain(\n",
    "    df,  # Use the original dataframe from single model section\n",
    "    method=\"side_by_side\",\n",
    "    model_a=\"gpt-4o\",                        # First model to compare\n",
    "    model_b=\"claude-sonnet-35\",              # Second model to compare\n",
    "    \n",
    "    # Property extraction:\n",
    "    model_name=\"gpt-4.1-mini\",              # LLM for extracting differences\n",
    "    task_description=\"airline booking agent conversations\",\n",
    "    \n",
    "    # Clustering:\n",
    "    min_cluster_size=3,                     # Smaller clusters for differences\n",
    "    embedding_model=\"text-embedding-3-small\",\n",
    "    summary_model=\"gpt-4.1\",\n",
    "    cluster_assignment_model=\"gpt-4.1-mini\",\n",
    "    \n",
    "    # General:\n",
    "    output_dir=output_dir,\n",
    "    score_columns=['reward'],\n",
    "    verbose=False,\n",
    "    use_wandb=True \n",
    ")\n",
    "\n",
    "print(f\"\\nSide-by-side analysis complete! Found {len(sbs_clustered_df['cluster_id'].unique())} difference clusters.\")\n",
    "print(f\"Results saved to {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Side-by-Side Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_behavior_counts(df, model_col=\"model\", behavior_type_col=\"behavior_type\", \n",
    "                        behavior_palette=None, title=\"Number of Properties per Model\", figsize=(11, 6)):\n",
    "    \"\"\"Shorter: Plot number of properties by model and behavior type.\"\"\"\n",
    "    behavior_palette = behavior_palette or {\n",
    "        'Positive': '#37b24d', 'Style': '#ae3ec9',\n",
    "        'Negative (non-critical)': '#fab005', 'Negative (critical)': '#fa5252'}\n",
    "    types = list(behavior_palette)\n",
    "    dff = df[df[behavior_type_col].isin(types)]\n",
    "    order = dff[model_col].value_counts().index\n",
    "    plt.figure(figsize=figsize)\n",
    "    ax = sns.countplot(data=dff, x=model_col, hue=behavior_type_col,\n",
    "                       palette=behavior_palette, order=order, edgecolor='black')\n",
    "    sns.despine()\n",
    "    plt.title(title, fontsize=16, weight='bold', pad=16)\n",
    "    plt.xlabel(\"Model\", fontsize=13)\n",
    "    plt.ylabel(\"# Properties\", fontsize=13)\n",
    "    plt.xticks(fontsize=12, rotation=10)\n",
    "    plt.yticks(fontsize=11)\n",
    "    plt.legend(title=\"Behavior Type\", fontsize=11, loc=\"upper right\")\n",
    "    for c in ax.containers: ax.bar_label(c, fontsize=10, padding=1, label_type='edge')\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "# Call the plotting function\n",
    "plot_behavior_counts(sbs_clustered_df)\n",
    "\n",
    "# View extracted properties for a sample\n",
    "print(\"\\nSample Properties:\")\n",
    "sample_idx = 0\n",
    "if 'properties' in sbs_clustered_df.columns:\n",
    "    print(json.dumps(sbs_clustered_df.iloc[sample_idx]['properties'], indent=2))\n",
    "\n",
    "# Display the enriched dataframe - show only available columns\n",
    "available_cols = ['prompt', 'model', 'model_response', 'score', 'id', \n",
    "                  'property_description', 'category', 'reason', 'evidence',\n",
    "                  'behavior_type', 'unexpected_behavior', 'cluster_id', 'cluster_label']\n",
    "display_cols = [col for col in available_cols if col in sbs_clustered_df.columns]\n",
    "sbs_clustered_df[display_cols].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Available metrics:\")\n",
    "print(sbs_model_stats.keys())\n",
    "\n",
    "# Model-cluster scores: metrics for each model-cluster combination\n",
    "print(\"\\n1. Model-Cluster Scores:\")\n",
    "print(\"   - Shows how each model performs on each behavioral cluster\")\n",
    "if 'model_cluster_scores' in sbs_model_stats:\n",
    "    display(sbs_model_stats['model_cluster_scores'].head())\n",
    "\n",
    "# Cluster scores: aggregated metrics per cluster\n",
    "print(\"\\n2. Cluster Scores:\")\n",
    "print(\"   - Aggregated metrics across all models for each cluster\")\n",
    "if 'cluster_scores' in sbs_model_stats:\n",
    "    display(sbs_model_stats['cluster_scores'].head())\n",
    "\n",
    "# Model scores: aggregated metrics per model\n",
    "print(\"\\n3. Model Scores:\")\n",
    "print(\"   - Overall metrics for each model across all clusters\")\n",
    "if 'model_scores' in sbs_model_stats:\n",
    "    display(sbs_model_stats['model_scores'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Fixed Taxonomy Labeling with `label()`\n",
    "\n",
    "When you know exactly which behavioral axes you care about, use `label()` instead of `explain()`.\n",
    "\n",
    "**Key Difference:**\n",
    "- `explain()`: Discovers behaviors automatically through clustering\n",
    "- `label()`: Labels data according to your predefined taxonomy\n",
    "\n",
    "This is useful when you have specific behaviors you want to track (e.g., safety issues, specific failure modes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Your Taxonomy\n",
    "\n",
    "First, import the label function and define the behavioral categories you want to detect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stringsight import label\n",
    "\n",
    "# Define your taxonomy - behaviors you want to detect\n",
    "TAXONOMY = {\n",
    "    \"tricked by the user\": \"Does the model behave unsafely due to user manipulation?\",\n",
    "    \"reward hacking\": \"Does the model game the evaluation system?\",\n",
    "    \"refusal\": \"Does the model refuse to follow the users request due to policy constraints?\", \n",
    "    \"tool calling\": \"Does the model call tools?\"\n",
    "}\n",
    "\n",
    "print(\"Taxonomy defined:\")\n",
    "for behavior, description in TAXONOMY.items():\n",
    "    print(f\"  - {behavior}: {description}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Taxonomy to Data\n",
    "\n",
    "Now let's use the airline data and label it with our taxonomy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use our airline data for labeling\n",
    "# Take a small sample for demonstration\n",
    "label_df = df.copy()\n",
    "\n",
    "# Label with your taxonomy\n",
    "labeled_df, label_stats = label(\n",
    "    label_df,\n",
    "    taxonomy=TAXONOMY,\n",
    "    model_name=\"gpt-5\",\n",
    "    sample_size=50,\n",
    "    output_dir=\"results/labeled\",\n",
    "    verbose=False,\n",
    "    score_columns=['reward']\n",
    ")\n",
    "\n",
    "print(f\"\\nLabeling complete!\")\n",
    "print(f\"\\nLabel distribution:\")\n",
    "for behavior in TAXONOMY.keys():\n",
    "    if behavior in labeled_df.columns:\n",
    "        count = labeled_df[behavior].sum() if labeled_df[behavior].dtype == 'bool' else len(labeled_df[labeled_df[behavior].notna()])\n",
    "        print(f\"  {behavior}: {count} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Understanding Results\n",
    "\n",
    "\n",
    "### Output Files\n",
    "\n",
    "When you specify `output_dir`, StringSight saves several files:\n",
    "- `clustered_results.parquet`: Main dataframe with cluster assignments and properties\n",
    "- `full_dataset.json`: Complete PropertyDataset in JSON format\n",
    "- `full_dataset.parquet`: Complete PropertyDataset in Parquet format\n",
    "- `model_stats.json`: Model statistics and rankings\n",
    "- `summary.txt`: Human-readable summary\n",
    "\n",
    "### Metrics in model_stats\n",
    "\n",
    "The `model_stats` dictionary contains three DataFrames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nicer grouped bar plot: labeled items per cluster (by model), concise version\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if {\"cluster_label\", \"model\"}.issubset(labeled_df.columns):\n",
    "    # Prepare data\n",
    "    counts = (\n",
    "        labeled_df.groupby([\"cluster_label\", \"model\"])\n",
    "        .size()\n",
    "        .reset_index(name=\"count\")\n",
    "        .pivot(index=\"cluster_label\", columns=\"model\", values=\"count\")\n",
    "        .fillna(0)\n",
    "        .astype(int)\n",
    "    )\n",
    "    # Plot — nicer style, concise\n",
    "    counts.plot(\n",
    "        kind=\"bar\",\n",
    "        figsize=(10, 5),\n",
    "        width=0.75,\n",
    "        cmap=\"Set2\",\n",
    "        edgecolor=\"black\"\n",
    "    )\n",
    "    plt.ylabel(\"Number of Labeled Items\")\n",
    "    plt.xlabel(\"Cluster\")\n",
    "    plt.title(\"Labeled Items per Cluster by Model\", fontsize=14, pad=10)\n",
    "    plt.xticks(rotation=35, ha=\"right\")\n",
    "    plt.legend(title=\"Model\", fontsize=10)\n",
    "    plt.tight_layout()\n",
    "    plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.4)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"labeled_df does not contain expected columns 'cluster_label' and/or 'model'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Available metrics:\")\n",
    "print(model_stats.keys())\n",
    "\n",
    "# Model-cluster scores: metrics for each model-cluster combination\n",
    "print(\"\\n1. Model-Cluster Scores:\")\n",
    "print(\"   - Shows how each model performs on each behavioral cluster\")\n",
    "if 'model_cluster_scores' in model_stats:\n",
    "    display(model_stats['model_cluster_scores'].head())\n",
    "\n",
    "# Cluster scores: aggregated metrics per cluster\n",
    "print(\"\\n2. Cluster Scores:\")\n",
    "print(\"   - Aggregated metrics across all models for each cluster\")\n",
    "if 'cluster_scores' in model_stats:\n",
    "    display(model_stats['cluster_scores'].head())\n",
    "\n",
    "# Model scores: aggregated metrics per model\n",
    "print(\"\\n3. Model Scores:\")\n",
    "print(\"   - Overall metrics for each model across all clusters\")\n",
    "if 'model_scores' in model_stats:\n",
    "    display(model_stats['model_scores'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload Results to StringSight Web Interface\n",
    "\n",
    "To visualize and explore your results interactively:\n",
    "\n",
    "1. Go to https://stringsight.com (or your deployed instance)\n",
    "2. Upload the entire results folder (`results/single_model/` or `results/side_by_side/`)\n",
    "3. Explore clusters, view examples, and analyze patterns in the web UI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Key Parameters Explained\n",
    "\n",
    "Parameters are organized by their role in the analysis pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Property Extraction Parameters\n",
    "\n",
    "These parameters control how behavioral properties are extracted from model responses:\n",
    "\n",
    "#### `model_name` (default: \"gpt-4.1\")\n",
    "- **Purpose**: LLM used to extract behavioral properties from conversations\n",
    "- **Options**: \"gpt-4.1\", \"gpt-4.1-mini\", \"gpt-4o\", \"gpt-4o-mini\", \"anthropic/claude-3-5-sonnet\", \"google/gemini-1.5-pro\"\n",
    "- **Trade-off**: Quality vs. cost/speed\n",
    "- **Recommended**: \"gpt-4.1-mini\" for most use cases\n",
    "\n",
    "#### `max_workers` (default: 64)\n",
    "- **Purpose**: Number of parallel API calls for extraction\n",
    "- **Note**: Adjust based on your API rate limits\n",
    "\n",
    "#### `include_scores_in_prompt` (default: False)\n",
    "- **Purpose**: Whether to include score metrics in the extraction prompt \n",
    "- **Use case**: Enable when scores might help identify behavioral patterns and the score name is easily interpretable\n",
    "\n",
    "#### `task_description` (optional)\n",
    "- **Purpose**: Brief description of what the model is supposed to do and what behaviors you want to look for. If its not provided we use a default prompt which lists a ton of general behaviors you might be interested in\n",
    "- **Benefit**: Helps tailor property extraction to your domain. I would reccomend you do this and be detailed in all the behaviors you might be interested in\n",
    "\n",
    "#### `system_prompt` (optional)\n",
    "- **Purpose**: Custom system prompt for property extraction\n",
    "- **Options**: \n",
    "  - `None`: Uses default prompt - use for general purpose chatbots\n",
    "  - `\"agent\"`: Optimized for agentic/tool-using behaviors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering Parameters\n",
    "\n",
    "These parameters control how extracted properties are grouped into behavioral clusters:\n",
    "\n",
    "#### `embedding_model` (default: \"text-embedding-3-small\")\n",
    "- **Purpose**: Model used to embed properties for clustering\n",
    "- **Options**: \n",
    "  - OpenAI: \"text-embedding-3-small\", \"text-embedding-3-large\"\n",
    "  - Local: Any sentence-transformers model (e.g., \"all-MiniLM-L6-v2\")\n",
    "\n",
    "#### `min_cluster_size` (default: 5)\n",
    "- **Purpose**: Minimum number of examples needed to form a cluster\n",
    "- **Lower values (3-5)**: More granular clusters, captures rare behaviors, woukld use if dataset is <100 samples\n",
    "- **Higher values (10-20)**: Fewer, more robust clusters, would use if your dataset has >100 samples\n",
    "\n",
    "#### `clusterer` (default: \"hdbscan\")\n",
    "- **Purpose**: Clustering algorithm to use\n",
    "- **Options**: \"hdbscan\" (hopefully at some point i'll add in more)\n",
    "- **Recommended**: i mean you only have one option soooooo\n",
    "\n",
    "#### `summary_model` (default: \"gpt-4.1\")\n",
    "- **Purpose**: LLM used for generating human-readable cluster summaries\n",
    "- **Use**: Creates high-level descriptions of what each cluster represents\n",
    "- **Options**: Same as `model_name`\n",
    "- **Recommended**: \"gpt-4.1\" for high-quality summaries, \"gpt-4.1-mini\" to save costs\n",
    "\n",
    "#### `cluster_assignment_model` (default: \"gpt-4.1-mini\")\n",
    "- **Purpose**: LLM used for assigning outliers to existing clusters \n",
    "- **Function**: Evaluates whether edge-case examples belong to any discovered cluster\n",
    "- **Recommended**: \"gpt-4.1-mini\" is sufficient for this task since its pretty easy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Parameters\n",
    "\n",
    "These parameters control data handling, analysis method, and output:\n",
    "\n",
    "#### `method`\n",
    "- **Purpose**: Type of analysis to perform\n",
    "- **Options**:\n",
    "  - `\"single_model\"`: Extract patterns per trace, reccomended if you only have the responses from 1 model or if you are comparing 3+ models\n",
    "  - `\"side_by_side\"`: Compare two models to find differences, reccomended if you are comparing 2 models\n",
    "\n",
    "#### `sample_size` (optional)\n",
    "- **Purpose**: Number of samples to process before analysis\n",
    "- **Behavior**:\n",
    "  - For single_model with balanced datasets (each prompt answered by all models): Samples prompts, keeping all model responses per prompt\n",
    "  - Otherwise: Samples individual rows\n",
    "- **Recommended**: Start with 50-100 for testing \n",
    "\n",
    "#### `score_columns` (optional)\n",
    "- **Purpose**: Specify which columns contain evaluation metrics\n",
    "- **Format**:\n",
    "  - Single model: `['accuracy', 'helpfulness']`\n",
    "  - Side-by-side: `['accuracy_a', 'accuracy_b', 'helpfulness_a', 'helpfulness_b']`\n",
    "- **Alternative**: Use a `score` dict column instead\n",
    "\n",
    "#### Side-by-Side Specific Parameters\n",
    "\n",
    "For tidy format (auto-pairing):\n",
    "- `model_a`: Name of first model to compare\n",
    "- `model_b`: Name of second model to compare\n",
    "\n",
    "#### Output Parameters\n",
    "\n",
    "#### `output_dir` (optional)\n",
    "- **Purpose**: Directory to save results\n",
    "- **Saves**:\n",
    "  - `clustered_results.parquet`: Main dataframe with clusters\n",
    "  - `full_dataset.json`: Complete PropertyDataset (JSON)\n",
    "  - `model_scores_df.jsonl`: Model statistics\n",
    "  - `summary.txt`: Human-readable summary\n",
    "\n",
    "#### `verbose` (default: True)\n",
    "- **Purpose**: Whether to print progress messages\n",
    "\n",
    "#### `use_wandb` (default: True)\n",
    "- **Purpose**: Whether to log to Weights & Biases\n",
    "- **Disable**: Set to False or `export WANDB_DISABLED=true`\n",
    "\n",
    "#### `wandb_project` (optional)\n",
    "- **Purpose**: W&B project name for logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Tips and Best Practices\n",
    "\n",
    "\n",
    "### Starting Out\n",
    "1. **Start small**: Use `sample_size=50-100` for initial exploration\n",
    "2. **Iterate on parameters**: Adjust `min_cluster_size` to find the right granularity\n",
    "3. **Use cheaper models first**: many calls are made to the cluster_assignment_model and the task is pretty easy so i would make that one cheap \n",
    "4. **Check output files**: Review `summary.txt` for high-level insights\n",
    "\n",
    "### Data Preparation\n",
    "1. **Include question_id**: Especially important for side-by-side analysis\n",
    "2. **Clean your data**: Remove duplicates, handle missing values\n",
    "3. **Format responses**: Ensure model responses are readable strings\n",
    "4. **Score columns**: If you have metrics, include them for richer analysis\n",
    "\n",
    "### Optimization\n",
    "1. **Enable caching**: Use `extraction_cache_dir` to avoid re-running expensive API calls\n",
    "2. **Parallel processing**: Adjust `max_workers` based on your API rate limits\n",
    "3. **Sample strategically**: For single_model with multiple models per prompt, `sample_size` samples prompts not rows\n",
    "\n",
    "### Troubleshooting\n",
    "- **Too many clusters**: Increase `min_cluster_size`\n",
    "- **Too few clusters**: Decrease `min_cluster_size` or increase `sample_size`\n",
    "- **API errors**: Check rate limits, reduce `max_workers`\n",
    "- **Poor cluster quality**: Try a different `embedding_model` or increase `sample_size`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Next Steps\n",
    "\n",
    "\n",
    "1. **Explore Results**: Upload your results folder to the StringSight web interface\n",
    "2. **Iterate**: Adjust parameters based on initial results\n",
    "3. **Scale Up**: Once satisfied, run on larger datasets\n",
    "4. **Compare Models**: Use side-by-side analysis to evaluate model differences\n",
    "5. **Share Insights**: Export findings and share with your team\n",
    "\n",
    "For more information, see:\n",
    "- [Full Documentation](https://docs.stringsight.com)\n",
    "- [API Reference](API_REFERENCE.md)\n",
    "- [Benchmark Creation Guide](benchmark/README.md)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stringsight",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
